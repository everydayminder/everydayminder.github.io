var store = [{
        "title": "20080709 Install Ubuntu 7.10 & VMware-tools",
        "excerpt":"[C] Installiation of Ubuntu 7.10     flow   downloaded the image file        installation complete (without security packages)       comments   installed Ubuntu 8.04 (the latest version)   failed to install other development packages on it   version 8.04 seems NOT stable as 7.xx -&gt; let’s install version 7.10 instead   it takes too long!! (about an hour)     -&gt; backing up the vmware image file after installation will be desirable   [C] Installation of vmware-tools     choose ‘Install VMware-tools’ from the VMWare menu   mount VMware tools from the desktop   copy VMware tools tar.gz file to anywhere that i want   extract the archive file, and change the permission to root user by ‘sudo -i’   execute vmware-install.pl   ","categories": ["system"],
        "tags": ["tomcat"],
        "url": "http://localhost:4000/system/20080709-install-ubuntu-710-vmware-tools/",
        "teaser":null},{
        "title": "Ubuntu에서 VIM이 재설치가 안될때",
        "excerpt":"Ubuntu 7.10에서 기본적으로 설치되어 있는 Vim은 Tiny 버전이다. 기타 필요한 VIM의 기능을 쓸 수가 없으므로, 재설치는 필수이다.   sudo apt-get install vim   그러나, 이 재설치가 제대로 안되는 경우가 있다. 이럴 때, Ubuntu 소스의 repository를 제대로 업데이트 해줘야 sudo apt-get update도 된다.   다음의 내용을 보완하자.   /etc/apt/sources.list 파일에 아래의 내용을 추가(혹은 기존 내용은 모두 삭제 or 코멘트 처리)하면 된다.   Daum Mirror Repository in Korea  deb http://ftp.daum.net/ubuntu gutsy main multiverse restricted universe deb-src http://ftp.daum.net/ubuntu gutsy main multiverse restricted universe deb http://ftp.daum.net/ubuntu gutsy-backports main multiverse restricted universe deb-src http://ftp.daum.net/ubuntu gutsy-backports main multiverse restricted universe deb http://ftp.daum.net/ubuntu gutsy-proposed main multiverse restricted universe deb-src http://ftp.daum.net/ubuntu gutsy-proposed main multiverse restricted universe deb http://ftp.daum.net/ubuntu gutsy-security main multiverse restricted universe deb-src http://ftp.daum.net/ubuntu gutsy-security main multiverse restricted universe  deb http://ftp.daum.net/ubuntu gutsy-updates main multiverse restricted universe deb-src http://ftp.daum.net/ubuntu gutsy-updates main multiverse restricted universe   이 내용을 적용한 후,   sudo apt-get update sudo apt-get install vim   하면 정상으로 설치가 된다.  ","categories": ["tips"],
        "tags": ["vim"],
        "url": "http://localhost:4000/tips/install_vim_under_ubuntu/",
        "teaser":null},{
        "title": "HBase Installation (Standalone mode/local file system)",
        "excerpt":"Requirements     Java 1.5.x, preferably from Sun.   Hadoop 0.16.x. This version of HBase will only run on Hadoop 0.16.x..   ssh must be installed and sshd must be running to use Hadoop’s scripts     to manage remote Hadoop daemons.   HBase currently is a file handle hog. The usual default of 1024 on *nix systems     is insufficient if you are loading any significant amount of data into regionservers.    See the FAQ: Why do I see “java.io.IOException…(Too many open files)” in my logs?  for how to up the limit.   [C] Installation of HBase     Java 1.5.x            installed java 1.6.x       Not sure whether 1.6.x will work           Unzipped the Hadoop archive file (ver 1.3) at ~/hbase   ssh &amp; sshd configuration (execution)            sudo apt-get install openssh-server       sudo /etc/init.d/ssh start (stop)           edit the limit number of file handlers            checked /etc/security/limits.conf but there is no option to edit for this           [C] HBase Configuration     ${HBASE_HOME} : the real location of hbase (/home/luran/hbase)   modify JAVA_HOME information on ${HBASE_HOME}/conf/hbase-env.sh   start HBase by ${HBASE_HOME}/bin/start_hbase.sh   stop HBase by ${HBASE_HOME}/bin/stop-hbase.sh   ","categories": ["system"],
        "tags": ["hadoop"],
        "url": "http://localhost:4000/system/hbase-installation-standalone-modelocal-file-system/",
        "teaser":null},{
        "title": "Hadoop Installation on Ubuntu Linux 7.10",
        "excerpt":"Related articles by Michael G. Noll          Running Hadoop On Ubuntu Linux (Single Node Cluster) http://wiki.apache.org/hadoop/Running_Hadoop_On_Ubuntu_Linux_%28Single-Node_Cluster%29            Running Hadoop On Ubuntu Linux (Multi Node Cluster) http://www.michael-noll.com/wiki/Running_Hadoop_On_Ubuntu_Linux_%28Multi-Node_Cluster%29       [Comments]     I created a VMWare Team project file and added two instances of existing virtual machines.   Each VMWare instance has a single node cluster.      IP alias name should be defined before loop back alias name (localhost) in ‘/etc/hosts’ file.   ","categories": ["system"],
        "tags": ["hadoop"],
        "url": "http://localhost:4000/system/hadoop-installation-on-ubuntu-linux-710/",
        "teaser":null},{
        "title": "불필요 ethernet interface 정보 정리하기",
        "excerpt":"VMWare를 쓰면서, 만들어둔 이미지를 여러번 복사하면서 쓰다보니 불필요한 ethernet interface 정보가 생성됐다.   eth0, eth1만 필요한데, 막상 인터페이스 정보를 보면, eth4, eth5로 생성되어 있는 이 현상을 정리하려면 다음과 같이 하면 된다.   $ cd /etc/udev/rules.d $ vi 70-persistent-net.rules   파일을 열어서, 내용을 다 지워주고, 재부팅한다. 재부팅 후, 네트워크 인터페이스를 조사하면, 다시 eth0, eth1 등으로  재 설정된 것을 확인할 수 있다.   이제 IP를 원하는 대로 설정하고 깨끗하게 쓰자.   ","categories": ["linux","vm"],
        "tags": ["linux"],
        "url": "http://localhost:4000/linux/vm/Cleansing_Ehternet_Info/",
        "teaser":null},{
        "title": "퍼옴) 명지대 김정운 교수 강연내용 발췌 (2008. 7. 10)",
        "excerpt":"김정운 명지대 교수 강연내용(2008.7.10, DM총괄 Professional Forum)   ㅇ 본인이 행복을 강연하고 다니니 어설픈 사람으로 인식되는 경우가 있으나     나름대로 공부를 많이 했음. 독일에서 13년간 유학했고 본인이 가르친     제자 중 3명이 교수가 되었음. 최근 1명이 하버드대학 교수가 되었는데,     본인이 가르친 것이 계기가 되어 관련 분야를 연구하여 결과적으로     하버드대 교수가 될 수 있었다고 고맙다는 편지를 받았음.   ㅇ 한국사회의 근본적인 문제는 무엇인가?     - ‘정치’라고 말하지만 사실은 그렇지 않음. 미국의 경우 2%가 나머지 98%를       돌리는 구조임. 지금 정도면 우리나라의 민도에 맞게 잘하는 편임.     - ‘경제’라고 말하지만 이것도 틀렸음. 한강의 기적과 라인강의 기적을        비교하는 것은 기분 나쁨. 독일은 망했었지만 잠수함 만들던 나라임.        풀먹고 무기 빌리던 한심했던 한국과 기술력을 갖고 있던 독일을        비교한다는 것은 말이 안됨. 불과 50년만에 빠른 발전을 이룩했음.     그렇다면, 무엇이 문제인가? OECD국가 중 노동시간이 최장이라는 점임.     본인은 ‘아침형 인간’은 가라고 외치고 다님. 이제 근면, 성실은 미친 짓임.     사는게 재미있어야 함. 아침형 인간하면 떠오르는 것이 남산 약수터     다니는 사람임. 솔직히 그 중 절반은 환자임.   ㅇ 본인은 삼성 노트북을 쓰지 않고 있음. 폼이 안남. 어쩌다 한번 삼성 것    쓰려고 해도 5년간 거의 안바뀌었음. 소니 VAIO의 경우 6개월에 한번씩     바뀜. 유사기능에 소니 노트북과 삼성노트북이 40만원 정도 차이가     나는데 순전히 디자인값의 차이임. 한마디로 삼성은 예쁜 것을 못만드는     것임. 휴대폰 빼고. 왜 예쁜 것을 못만들까. 좋은게 무언지 모르기 때문에     못만드는 것임. 본인은 소니 것 쓰다가 최근 맥북을 구입하였는데, 정말     폼이 남. 결과적으로 삼성의 문제는 좋은 것, 행복한 것이 무언지 모르는     것임.   ㅇ 개미 콤플렉스라는 것이 있음. 재미있으면 불안하고 행복하면 찝찝한 것임.     그러나 21세기 버전으로 생각하면 개미는 죽어라 일만 하다가 허리디스크에     걸려 겨울을 나는데 비해 베짱이는 여름내 놀다가 겨울에는 개미 모아놓고     노래하면서 토크쇼한다고 함. 베짱이는 남을 행복하게 만드는 재주가 있음.     이것이 바로 디자인인 것임. 삼성은 휴대폰만 디자인에 성공하고 있음.   ㅇ 열심히 일하면 뭐하나. 최근 광고에 “열심히 일하는 당신 떠나라.”라는     카피가 유행한 적이 있음. 그래서 떠나려고 하는데 “뭘하지?”라고 생각함.     그러다 여행을 가면 여행지에서 좀 놀다가 “놀면 뭐하냐?”라고 하면서     화투를 치게 됨. 한마디로 제대로 놀줄을 모르는 것임. 이것은 전문용어로     Learned helplessness(학습된 무기력)이라고 함. 개를 묵어놓고     전기고문을 가하면 고통을 느낌. 이것을 여러번 반복한 후, 풀어놓으면    스스로 자유롭게 자율적으로 행동하지 않음. 학습된 무기력인 것임.     동일한 증상을 어른 남자에게서 발견할 수 있음.   ㅇ 여기도 보면 참고 인내한 표정을 갖고 있는 분이 여러분 계심. CEO모임에     나가보면 CEO들이 모두 그러함. 유능한 사람이 저런 병에 걸릴 가능성이     높다는 것임. Peter’s Principle이라고 함. 주체적으로 살아본 적이 없는     것임.   ㅇ 삼성은 관리의 삼성이라고 함. 그런데 최근 삼성에서 창조경영을 이야기함.    삼성은 창조경영이 무언지 모름. MP3 iriver의 경우 2년만에 사라짐.     iPod 때문에 한 방에 갔음. iPod가 더 비싸지만 듣는 것 하나는 죽임.     소니도 MP3시장에서 나감. iPhone도 표면 감각이 죽임!     결과적으로 행복과 재미가 있어야 예쁜 물건을 만들어 낼 수 있는 것임.     쇠고기 데모 파동을 보면 우리나라 사람들이 불행하다는 것을 느낄 수     있음. 거기 모이는 절반이 놀러 오는 것임. 다시말해서 불행하여 놀     건수를 찾았던 것임   ㅇ 독수리5형제 증후군이라는 것이 있음. 우리나라 사람들은 술 한잔 먹으면    독수리5형제가 됨. 정치, 경제, 개혁, 온난화에 대해 거품을 물고 이야기함.     길가다가 우연히 자빠져도 대통령을 탓하는 것이 우리나라임. 이렇게     의기가 넘치는 우리나라 사람들이 자신의 행복을 챙기는 데에는 비겁함.     여기 계신 분 중에 우아한 레스토랑에 혼자 가서 좋아하는 스테이크를     혼자 먹으실 수 있는 분이 있으신가? 아니면 좋아하는 음악회를 혼자     가서 들을 수 있는가? 그렇게하면 또라이라는 소리를 들음. 또라이라는     소리를 들으면 어떤가. 용기를 갖고 행복을 추구할 수 있어야함.     독수리 5형제라고 하는데 사실은 조류5남매임.   ㅇ 주5일 근무제 도입할때 행복해질 것으로 착각했었음. 4인가족이 2박3일     놀러가면 얼마드는가? 먹고자는 데에만 40만원이 듬. 그래서 연간 1~2회     정도밖에 못하는 것임. 그러니까 주말이면 퍼 자게 되고 싸움만 일어남.   ㅇ 휴일이 늘면서 이혼도 늘어나고 있음. 독일에서의 연구에 따르면 휴일이     늘자 이혼율이 75%나 증가하였음. 이유를 조사해 보니 평소에 바쁘다고     문제를 덮고 살다가 시간이 나니 말싸움하다가 문제가 떠올라 실컷     싸우고 이혼하게 되는 것임. 지난 3일간 정서적 표현을 써먹은 경우를     떠 올려보시기 바람. 대화라고 해봐야 “여보, 돈 부쳤나?”정도 아닌지..     아이들하고도 생전 대화를 하지 않다가 대화가 부족했음을 느끼고 막상     대화하려고 하면 “야 몇등했냐?”로 시작해 갈등을 초래함. 정서적 표현은     갈등을 해소하는 힘이 있음. 이것이 없으니 싸움이 많아지는 것임. 이렇게     되면 함께 있는 시간이 고통스러워지는 것임.    ㅇ 일본말로 “누레오치바”라는 것이 있음. 젖은 낙엽이라는 뜻임. 일본 아줌마    들이 남편을 일컫는 말임. 일본 남성들은 일만하는 것으로 유명함. 그래서     년간 섹스수가 일본인 평균 36회에 불과함. 참고로 그리스는 120회에     달한다고 함. 그렇게 일만 하다가 은퇴후 부인의 치마를 붙잡고 늘어지는데    젖은 낙엽처럼 떨어지지 않는다고 해서 붙인 말이라고 함. 우리나라에도     ‘곰국’ ‘애견’ 등 남성의 문제점을 비웃는 유사한 말이 도는 실정임.   ㅇ 그러면, 창의성은 어떻게 키울 수 있는가? 관리의 삼성에서는 절대 창의성이     나올 수 없음. 창의성이란 새로운 것을 만드는 능력을 말함. 그런데     새로운 것이 생전 보지도 듣지도 못한 것을 생각해 내는 것은 아님.     우선은 어디서 본적이 있는 것을 Representation해 내야 함. 다만 낯설게     하는 것에서 창의성이 생겨남.     (시상식 사진을 하나 보여줌. 그런데 아주 난감한 시상식이라고 하고 보면      그냥 시상식이라고 했을 때 보이지 않던 부분이 보임)   ㅇ 독일어로 낯설게 하기는 Verfremdung임. 재미가 있어야 낯설게 하기가     가능해짐. 조직내에 재미가 살아있어야 창의가 가능해짐. 13년전 독일에     갔을때 한국산은 싸구려만 조금 볼 수 있었고, 본인도 베트남 사람으로     여겨졌음. 독일 사람들은 한국을 모름. 그런데 최근에는 최고 매장에서     최고의 모니터가 삼성 제품임. 많이 달라짐. 그런데 삼성에 재미가 빠져있는     것이 문제임.   ㅇ 빗자루 사진을 보여주면서 무슨 생각을 하는가 물음. 대부분은 쓸 생각만    했을 것임. 그런데 옆에 해리포터의 빗자루 타고가는 장면을 보여줌.     아이들은 빗자루를 가지고 날아가는 장면을 떠올림. 아이들은 재미만     생각하기 때문에 가능함. 다른 맥락을 끌고 들어온 것임. 해리포터는     현대자동차의 연간 수익의 3~4배 이익을 올린다고 함. 이런 것을     전문용어로 맥락적 사고라고 함. 맥락적 사고가 되어야 낯설게하기가     가능해지는 것임. 교육이란 아이들을 사회화시키는 것임. 이런 과정을     지나면 재미가 사라짐.   ㅇ ‘동물고스톱’이라는 것이 있음. 화투장에서 동물의 숫자대로 돈을 내는 것임.    예를 들어 팔 십짜리는 3마리, 비는 1마리가 이해되는데, 팔광을 놓고     동물이 있다는 것임. 보름달이 그려져 있으니토끼가 있다고 인정해 준다는    것임. 팔광에서 토끼를 보는 능력은 재미에서 유발된 것임.     “닌자거북이”를 5번 외쳐 보세요. 크게. 그리고 나서 “세종대왕이 만든     배는 뭡니까?”라고 묻자 절반의 청중이 “거북선”이라고 대답함.     닌자거북이 5번 외치기가 근면 성실의 사례임.   ㅇ 재미가 없으면 창의성이 없음. 돈, 권력으로 남을 움직일 수 없음. 남의     마음을 움직여야 하는 것임. 이렇게 하려면 정서공유가 되어야 함. 논리로     설득해서는 남을 움직일 수 없음. 마음이 움직이면 자기도 모르게     하이파이브를 하게 됨. 정서 공유가 매우 중요함. 노대통령은 토론을     좋아함. 듣는 사람이 논리적으로 굴복하여도 실제로는 마음이 움직이지     않음. 정서공유가 되지 않은채 논리로만 해서는 안되는 것을 모른 것임.   ㅇ 정서공유의 대표적 장면으로 광고장면 하나를 보여줌. 최민식이 어려운     사람을 위로하는 장면인데 어깨잡고 묘한 표정을 짓는데 정서공유가     일어남. 최민식은 정서공유로 상대를 끌고 있는 것임. 본인이 강의하면서     계속 ‘응!’ ‘응!’하고 상대의 긍정을 유도하는 것은 정서적 반응을 이끌어    내려는 본인만의 기술인 것임.   ㅇ 조직문화도 정서공유가 되어야 함. 이것을 지식경영에서는 Implecite     Knowledge라고함. 우리나라에서 이것이 잘되는 모임이 3개임.     해병대전우회, 고대교유회, 호남향우회가 그것임.   ㅇ 예쁜 여자들은 통상 아무리 재미있게 이야기를 해도 정서적 반응을 보이지    않음. 이것을 “꽃이 향기가 없다”고 표현함. 본인도 예쁜 여자를 좋아하는데    미팅나가서 예쁜 여자한테 재미있으려고 용을 썼는데 반응이 없고     그 옆에 안 예쁜여자가 반응해 30분쯤 지나 이 여자에게 재미있게     이야기하다 지금 결혼해 살고 있음. 웃는 여자는 무조건 예쁘다는 말도 있음.     그런데 미우면서도  웃지도 않는 여자가 있음. 환경오염임.    ㅇ 21세기 리더십은 정서공유가 핵심임. 정서 공유가 없으면 남의 마음을     움직일 수 없음. 정서의 수도관을 먼저 뚫어 놓고 말을 해야 논리가     흘러들어갈 수 있는 것임.    ㅇ 21세기의 시대정신은 재미임. 사회주의가 망하면서 나온 것인 웰빙,     엔터테인먼트 등 행복추구임. 본인이 글을 쓸 때에도 재미있게 쓰면 좀     모자란 듯 쓰여도 독자가 귀신같이 알아보고 전화함. 그런데 재미없는     상태에서 그럴듯하게 정제해 써 놓아도 감동이 없음.   ㅇ 생각지 않는 1천만원이 생기면 무얼하시겠습니까?라고 하면 심한 사람은     저축한다고 함. 대부분 여행한다고 하는데, 본인이 독일에 있을때 한국사    람이 여행와 차를 빌려서 여행안내를 한 적이 있음. 그런데 차를 반납하면    독일 사람이 고개를 갸우뚱함. 2주만에 5000km를 뛴 것을 보고 이해할     수 없다고 함. 무조건 달리기만 했던 것임. 여행을 좋아한다고 하지만     좋아한 것이 분명치 않았던 것임. 우리의 목적이 불분명하면 돈버는 것도     불가능한 것임.   ㅇ 휴테크의 원리로 마무리하고자 함.  ㅇ 1 . 사소한 것을 즐겨라. 다양하게 즐길    수 있어야 함. 선진국이란 재미의 조건이 다양한 나라임. 새소리, 별보기와     같은 일상의 사소한 것에서 재미를 느낄 줄 알아야 함. 우리는 재미에     대한 환상을 가지고 있음. 8시 뉴스, 9시 뉴스, 11시 뉴스, 마감뉴스     보면서 세상이 뒤집어지기를 내심 바람. 이게 안되니 폭탄주 먹고 자신의     장을 뒤집는 것임. 월드컵때 재미있었음. 그런데 본인은 월드컵이 문제라    생각함. 어지간 한 것에 재미를 못느끼게 만들었기 때문임.  노벨상 경제학상을     받았던 카네만이라는 학자는 행복을 간단히 정의했음.     “내 하루의 삶 속에서 기분좋은 시간이 길어야 행복한 것이다”라고.     재미에 대한 환상은 버려야 함.   ㅇ 2. Mania가 되라. 매니아는 정신병 용어로 조증이라고 함. 최근에 성공한     사람들의 공통점은 일종의 Hypermania들임. 20세기에는 근면 성실이     재미 행복보다 더 중요했었음. 그러나 21세기에는 근면 성실보다     재미행복이 더 중요함. 21세기의 천재는 사는게 재미있는 사람임.     정체성(Identity)이 매우 중요함. 자신의 존재가 확인되지 않으면 불행한     것임. 사회적 지위로 정체성을 찾던 시기는 20세기였음. 거기에 무엇에     재미있어하는지가 덧붙여져야 함. 재미로 그 사람의 정체성을 확인할 수     있는 것임. 예를 들어, ‘삼성전자 부장’이라고만 소개되지 말고, ‘삼성전자     부장인데 어떤 것을 재미로 합니다’로 소개될 수 있어야 함. BNG가     무언지 아시는가? 뻥앤구라임. 본인의 별명임. 이런 황당한 자부심을     가지고 살고 있음.   ㅇ 3. 감동하라. ‘왜사냐?’하고 물으면 죽지못해 산다고 하는 경우가 있음.     행복하면 감탄사가 절로 나오게 됨. 본인은 인간문명의 기원을 연구하여     박사학위를 받았음. 인간만 미숙아로 태어남. 인간 이외의 동물은 태어나    자 마자 자신이 스스로를 돌 볼 수 있음. 인간도 이런 수준이 되려면     18개월후에 태어나야 함. 그런데 9개월만에 태어남. 나머지 9개월간     어머니가 아기를 보면서 감탄하는 시간임. 본인은 이런 어머니 비디오를     보면서 3년간 연구했음. 미칠 것 같았음. 그런데 어머니들은 계속해서     감탄을 함. 여기서 아기는 엄마의 감탄을 먹고 자란다는 것을 알게 되었음.     엄마는 가능성을 알아내는데 귀신임. “애가 걸었어!” “애가 ‘아빠’라고     불렀어”라고 해서 보면 전혀 그렇게 하지 못함. 그런데 1주일 후면 정확히    그렇게 했음. 어머니는 아이의 잠재력을 화~악 끌어내는 것임.     우리가 음악, 그림, 산, 여행을 즐기는 이유는 감탄때문임. 그래서 산에     오르면 “야~호!” 소리를 지르는 것임. 감탄이 삶의 동력임. 인간은 감탄의     욕구를 가지고 살고 있는 것임. 여기 계신 분들의 표정이 굳은 것은     감탄을 받지 못해서임. Wonderful!, 독일어 Wunderbar! 일본어 소고이!     스바라시!라는 감탄사가 발전되어 있는데 한국어에는 없음. Wonderful!을    번역하면 “오! 놀라워라!”정도인데 ??임. 한국어에 유사한 것으로는     “죽인다!!” 정도가 있음.     우리나라에서 여자가 남자보다 평균 7.2년 더 산다고 함. 여자들은 무척     감동을 잘함. 별것도 아닌 것에 계속 감탄함. 그래서 오래 사는 것임.     우리는 잘 노는 사람들의 부드러운 사회를 건설해야 함.     “하루에 몇 번 감동했는가?” 되돌아 보시기 바람.     “내 삶의 주인이 될 것인가 시키는 일만 할 것인가?”  ","categories": ["reading"],
        "tags": [],
        "url": "http://localhost:4000/reading/Happiness_by_JWKim/",
        "teaser":null},{
        "title": "Ubuntu에 Eclipse 설치하기",
        "excerpt":"   먼저, java를 설치한다.       sudo apt-get install sun-java6-jdk           시스템에서 기본적으로 동작할 java 를 선택해준다.      sudo update-alternatives --config java          원하는 java를 선택한다.       eclipse를 설치한다.      sudo apt-get install eclipse           원하는 패키지를 추가로 설치한다.      sudo synaptic          예) PyDev : Python 개발 플러그인       ","categories": ["system"],
        "tags": [],
        "url": "http://localhost:4000/system/ubuntu-eclipse/",
        "teaser":null},{
        "title": "Ubuntu에서 휠마우스 사용하기",
        "excerpt":"sudo vi /etc/X11/xorg.conf 를 실행하여 해당 파일을 편집한다.   Section “InputDevice”     Identifier “Configured Mouse”     Driver “mouse”     Option “CorePointer”     Option “Device” “/dev/input/mice”     Option “Protocol” “ImPS/2\"     Option “Buttons” “5”     Option “ZAxisMapping” “4 5” EndSection   마우스 디바이스 정의 부분 중, Protocol 부분을 위와 같이 바꿔주고 재시작하면, 휠 마우스의 휠이 동작한다.   ","categories": ["system"],
        "tags": ["tips"],
        "url": "http://localhost:4000/system/ubuntu-mouse-wheel-config/",
        "teaser":null},{
        "title": "암호없이 SSH 키로 인증하기",
        "excerpt":"1. Localhost에서 인증하기  루트의 권한을 가진 ID로 로그인하여 다양하게 스크립트를 실행할 경우, SSH 로그인을 필요로 할 수 있고, 이 때, 매번 password를 입력해야할 경우 번거로울 수 있다.   SSH는 암호 뿐만 아니라, public key, private key 기반의 인증을 제공하고  어렵지 않게 설정할 수 있다.   공개키는 암호화를 위해 사용되고, 공개키에 매핑되는 개인키로 복호화를 할 수 있다. 암호없이 SSH 로그인을 하려면, 클라이언트 시스템에서 키를 생성해야 한다.   ssh-keygen 명령어를 사용하여, 키를 생성할 수 있고,  -t 옵션을 통해 생성되는 키의 type(RSA/DSA)을 지정할 수 있다.   $ ssh-keygen -t rsa -P \"\"   와 같이 하면, .ssh 디렉토리에 id_rsa와 id_rsa.pub라는 키가 생성된다. ir_rsa는 개인키, id_rsa.pub는 공개키이다.   로컬 머신에서 이 키를 자동으로 사용하기 위해,  $cat $HOME/.ssh/id_rsa.pub &amp;gt;&amp;gt; $HOME/.ssh/authorized_keys  로 복사한다.   그 후에,   $ ssh localhost  를 수행하여 확인해보면, localhost로 암호없이 키만으로 ssh 인증이 되는 것을 확인할 수 있다.   2. Remote host에서 인증하기  리모트호스트에서 암호없이 키만으로 SSH 인증하려면, 위와 같이 생성된 내 시스템의 키 값이 내가 접속할 시스템의 authorized_keys에 포함되어 있으면 된다.   생성된 공개키를 서버에 복사하기 위해, copy &amp; paste를 하거나, FTP 등을 사용하여 복사하고 append 해도 된다. SSH가 설치되어 있으므로, SCP를 통해 복사하려면 다음과 같이 한다.   $ scp .ssh/id_rsa.pub target_system_ip:/tmp/tmp_id_rsa.pub   이후, 리모트 시스템에 접속하여, 조금전에 복사한 /tmp/tmp_id_rsa.pub의 내용을 ./ssh/authorized_keys에 append 해주자.           복사한 tmp_id_rsa.pub가 authorized_keys에 정확히 append 되었다면, 꼭 지워줄 것.            내 시스템에서 remote system으로 ssh target_system_ip 하여 확인해보자. 성공적으로 설정되었다면, 암호 인증없이 연결되는 것을 확인할 수 있다.      ","categories": ["system"],
        "tags": ["linux","ssh"],
        "url": "http://localhost:4000/system/ssh-without-password/",
        "teaser":null},{
        "title": "Ubuntu console mode로 로그인하기",
        "excerpt":"Ubuntu-desktop은 GUI모드로 부팅된다. redhat 계열 리눅스와는 달리 runlevel을 수정하는 것만으로는 동작하지 않는다.  $ sudo mv /etc/rc3.d/S30gdm /etc/rc3.d/K30gdm   (내 경우에는 rc2.d에서 해줘야 먹혔음) 참고로, Ubuntu에서 X윈도우를 터미널창에서 끄고 시작하려면,  $ sudo /etc/init.d/gdm start(stop, restart)   Ubuntu에서 X윈도우를 자동으로 실행하지 않고 터미널로 시작되도록 설정하는 명령어  $ sudo update-rc.d -f gdm remove   다시, X윈도우를 자동으로 띄우려면,  $ sudo update-rc.d gdm defaults  ","categories": ["system"],
        "tags": [],
        "url": "http://localhost:4000/system/ubuntu-console-mode-login/",
        "teaser":null},{
        "title": "HDFS 설정하기",
        "excerpt":"기본적으로는, 아래 posting과 같이 Michael G. Noll씨가 설명한 바와 같이 따라하면, 하나의 machine에 single cluster를 이상없이 설치할 수 있다.   Michael이 권고하기를, 여러 개로 multi cluster를 구축하기에 앞서, 단일 cluster로 정상 작동하는지 확인 후, 여러 개를 붙여가는 방법이 보다 효과적이라고 한다.   두 개의 cluster를 master-slave로 구성하고자 한다면, 앞서 마친 단일 cluster 2개 중 하나를 master로 다른 하나를 slave로 설정한다.      master와 slave의 conf/hadoop-site.xml에서 localhost 부분을 master로 바꿔준다.   master의 conf/masters 파일에 master를 넣는다. (slave에서는 안해줘도 됨)   master의 conf/slaves에 master, slave를 넣는다. (slave에서는 안해줘도 됨)   앞서 single cluster 설정시에 사용했던 file system directory를 지워준다.   (… /dir/dfs)   master에서 namenode를 포맷한다. (bin/hadoop/ namenode -format)   master에서 bin/start-dfs.sh를 실행시켜 프로세스의 정상 가동 여부를 확인한다.  1) master에서 뜨는 process : Namenode, DataNode, SecondaryNamenode  2) slave에서 뜨는 process : DataNode   master 및 slave에서 각각 로그내 이상유무를 확인한다. (ERROR 유무)   master에서 MapReduce 를 실행 시킨다.(bin/start-mapred.sh)   jps로 프로세스를 확인한다.   1) master에 추가되는 process : TaskTracker, JobTracker  2) slave에 추가되는 process : TaskTracker   master 및 slave에서 각각 로그내 이상유무를 확인한다. (ERROR 유무)   정상적으로 동작한다면, start-all.sh stop-all.sh 을 실행시킴으로써 dfs 및 mapreducer를 일괄적으로 시작/종료시킬 수 있다.   요약하면, master/slave의 dfs가 정상적으로 연동되고 있다면 외형적으로는 다음 두 조건을 만족시켜야 한다.      master 및 slave에서 각각 떠야할 프로세스들이 jps로 확인된다.   master 및 slave의 각 로그파일에 에러가 없다. (출처 : http://hadoop.apache.org/core/docs/current/hdfs_design.html)   ","categories": ["system"],
        "tags": [],
        "url": "http://localhost:4000/system/hdfs-setup/",
        "teaser":null},{
        "title": "HBase 설치/설정하기",
        "excerpt":"Michael 씨가 자세하게 설명했던 HDFS 설정과는 달리, HBase 설정은 wiki.apache.org/hadoop/HBase에 나와있는 설명이 전부이다. 물론, 웹에서 많은 사람들의 노력의 흔적은 곳곳에 있긴 하지만, 역시 어렵다. 기본적인 수준의 설명이나, 링크의 소개, 해봤더니 잘 되더라는 소감이 대다수이니까.           내 시스템 구성  1) 앞서 설정한 HDFS(master, slave) -&gt; 2대  2) HBase -&gt; 1대            미리 준비할 것  1) hbase1과 master, slave 간 ssh가 미리 설정되어 있어야 함. (비밀번호 없이 SSH 인증하기 참조)  2) hbase1, master, slave는 서로 reachable 해야 함.   (hbase1는 master랑만 통신해도 될 줄 알았는데.. 아니었음)       환경 설정  1) .bashrc에 HBASE_HOME을 설정  2) /conf/hbase-env.sh에 JAVA_HOME 설정  3) /conf/hbase-defaul.xml을 /conf/hbase-site.xml로 복사  4) /conf/hbase-site.xml중, hbase,master 항목과 hbase.rootdir 항목 변경            hbase.master -&gt; example.org:60000을 hbase1:60000 으로 설정       hbase.rootdir -&gt; hdfs://example.org:9000/hbase를 master:54310/hbase로  설정  (hbase가 뜨는 곳은 hbase1이고, hbase가 사용할 hdfs는 master로 시작됨)  5) /conf/regionservers는 ${HADOOP_HOME}/conf/slaves와 동일하게 작성함  6) Hbase가 HDFS를 제대로 쓰도록 하기 위해, hadoop-site.xml이 필요하므로, ${HBASE_HOME}/conf에 ${HADOOP_HOME}/conf/hadoop-site.xml을 복사함.           실행  1) 띄우기 : ${HBASE_HOME}/bin/start-hbase.sh            성공적으로 뜨면, 로그에 에러가 안 남음       jps 결과 : HMaster, HRegionServer가 추가됨  2) 종료하기 : ${HBASE_HOME}/bin/stop-hbase.sh       jps 실행시, HMaster, HRegionServer가 사라짐              문제점   내가 실험한 결과, start-hbase.sh 가 정상 작동하지 않는다. HMaster는   뜨지만, HRegionServer가 정상적으로 뜨지 않아서 쓸 수 없는 상태가 됨.   path가 너무 길거나 복잡하면 에러가 생기는 듯 한데, shell을 고쳐서 직접   실행시키면 뜨기는 한다. 당분간은 이렇게라도 실행시켜봐야할 듯.   정상적으로 동작하면, jps에도 뜨지만, start-base/stop-base 수행시간도 짧다.   오작동시, 시간이 길어지며 최악의 경우, kill도 감수해야 한다. (vulnerablity)   ","categories": ["system"],
        "tags": ["hadoop"],
        "url": "http://localhost:4000/system/hbase-Installation/",
        "teaser":null},{
        "title": "HBase에서 HQL 사용하기",
        "excerpt":"HBase에서 HQL 사용하기    (wiki.apache.org내용을 근거로)   설치된 HBase를 바탕으로, HQL을 수행하였다. (bin/hbase shell) 해당 정보의 참고 링크는 아래와 같다. http://wiki.apache.org/hadoop/Hbase/HbaseShell?action=print      Create Table     hql&gt; create table movielog_table ( -&gt; year, length, inColor, studioName, vote, producer, actor);           Insert Data     hql&gt; insert into movielog_table (year:, length:, inColor:, studioName:, 'vote:user name', producer:, 'actor:hero') -&gt; valules ('1977', '124', 'true', 'Fox', '5', 'George Lucas', 'Mark Hamill') -&gt; where row = 'Star Wars';              내가 해 본 시도)   column 명을 주지 않고, 곧바로 values들만 넣어봄 : 에러        where 부분에서 row key 를 지정하지 않음 : 당연히 에러!       row key가 따로 명시되지만, 사실 일반 PK 필드와 동일함.   사용시 where 이후에 기록을 꼭해줘야 함. (활용폭이 적음, 맵 구조인지라  키를 중심으로 데이터를 처리함)   관계형 DB에 비해 where 조건에 넣을 수 있는 실질적인 비교구문이 없음      Select Data     hql&gt; select count(studioName:FOX) from movielog_table;           Truncate     hql&gt; truncate table 테이블명           Drop Table     hql&gt; drop table 테이블명              truncate나 drop table을 실행하기 전에 반드시 disable 테이블명을 해줘야 실행된다.   ","categories": ["system"],
        "tags": ["hadoop","hbase"],
        "url": "http://localhost:4000/system/hbase-hql/",
        "teaser":null},{
        "title": "The Hadoop Distributed File System : Architecture and Design 요약",
        "excerpt":"본 내용은 해당 링크내의 원본을 읽고, 키워드 등의 관점에서 축약하였음   원본 링크 :  http://hadoop.apache.org/core/docs/current/hdfs_design.html   본 내용은 해당 링크내의 원본을 읽고, 키워드 등의 관점에서 축약하였음   ■ Introduction     HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.   HDFS provided high throughtput access to application data and is suitable for applications that have large data sets.   ■ Assumptions and Goals     Hadrware Failure : detection of faults and quick, automatic recovery   Streaming Data Access : It is designed more for batch processing rather than interactive use by users   Large Data Sets : It provides high aggregate data bandwidth and scale to hundreds of nodes in a single cluster.   Simple Coherency Model : write-once-read-many access model for files   “Moving Computation is Cheaper than Moving Data” : It is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.   Portability Across Heterogeneous Hardware and Software Platforms : HDFS is easily portable from one platform to another.   ","categories": ["system"],
        "tags": [],
        "url": "http://localhost:4000/system/the-hadoop-distributed-file-system-architecture-and-design/",
        "teaser":null},{
        "title": "chm Help File 만들기",
        "excerpt":"chm 파일을 만들기 위해, 다음의 툴들중 하나만 있어도 손쉽게 만들 수 있다.      HTML Help Workshop   WinCHM   jd2chm (커맨드라인 방식)   EasyCHM 등   대부분의 툴들은 직관적인 메뉴를 제공하여 몇번의 클릭만으로도 손쉽게 파일을 만들 수 있다.   어떤 툴을 사용하건 간에 기억할 사항은, “root는 실제 index.html등이 존재하는 최상의 디렉토리일 것.” 이라는 것이다.   그 밖의 조건은 사용자의 기호에 맞게 메뉴 또는 내용을 추가/삭제 하여 만들면 될 뿐이다.   실행을 위한 조건.     네트워크 드라이브상의 파일은 실행이 안된다.   외부에서 다운로드 받은 파일은 차단되어 있을 가능성이 있다.    (마우스오른클릭-&gt;속성-&gt;차단해제 후 열람 가능)   꼭 모든 manual만을 chm으로 만들지 않고, 로컬 디스크의 자료 관리나 자신이 자주 애용하는 사이트들의 bookmark를 chm 형태로 만들어서 적극 활용하면 매우 좋을 것 같다.  시간내서 한 번 고민해 볼 일이다.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/chm-help-file/",
        "teaser":null},{
        "title": "How do you call him?",
        "excerpt":"질문 의도는 “그 사람을 뭐라 부르면 되나요?” 지만,   대답으로 일반전화 번호나 휴대폰 전화를 알게 될 것이다. 질문이 “그 사람에게 어떻게 전화하나요?” 이었기 때문이다.   그 사람을 뭐라 부르면 되나요?에 대한 대답을 들으려면, How do I address him? 이라고 하자.   &lt;/span&gt;   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/how-do-you-call-him/",
        "teaser":null},{
        "title": "Twisted Matrix 로 TCP Server 만들기",
        "excerpt":"Twisted Matrix 패키지를 다운로드 받아 설치하는데는 별다른 복잡한 과정이 필요하지 않다. 그냥 다운받아서 실행하면 끝. Blocking IO 방식의 통신 모듈만 썼었는데, 이번 기회에 Non-blocking IO를 파이썬으로 시도하게 되었다.   파이썬의 기본 모듈만 써서 비동기 통신을 구현할 수도 있으나, 편의성을 제공하는 유명한 framework이 존재하여, 이를 써보게 되었다.   나 말고, 다른 사람들은 이미 Twisted를 쓰고 있기 때문이기도 하다. 공식 사이트의 reference에 나와 있는 예제를 통해, TCP 서버를 순식간에 만들어 낼 수 있다. Factory 패턴과 Reactor 패턴을 써서, 하라는 대로만 하면 순식간에 간단한 서버를 만들어 준다. 세션 유지를 위한 Alive Packet을 별도로 작성할 필요도 없이 말이다.   이 서버에 장기간 동안 연결을 붙여 놓고, 자리를 비웠지만 끊김없이 TCP connection이 유지되었다.   ```python from twisted.internet.protocol import Protocol, Factory from twisted.internet import reactor   class QOTD(Protocol):   def connectionMade(self): \tself.transport.write(“An apple a day keeps the doctor awayrn”) \tself.transport.loseConnection()   factory = Factory() factory.protocol = QOTD reactor.listenTCP(8077, factory)   reactor.run()   이렇게 작성하는 것이 끝이다. 이후에는, 프로토콜과 애플리케이션을 분리하여 프로토콜을 완성하면 된다.  ","categories": ["development"],
        "tags": ["python","twisted"],
        "url": "http://localhost:4000/development/twisted-matrix-tcp-server-setup/",
        "teaser":null},{
        "title": "Twisted Matrix로 TCP Client 만들기",
        "excerpt":"앞서 간단히 만든 서버에 접속할 클라이언트를 만들어 봤다.  프로토콜  class TestClient(LineReceiver): \tdef connectionMade(self): \t\tself.sendLine(\"A new connection has been made!\") \t\tself.factory.clientReady(self) \t \tdef lineReeived(self, line): \t    print \"$ got msg [%s]rn\" % line      def connectionLost(self, reason): \t    reactor.stop()  팩토리  class TestFactory(ClientFactory):     protocol = TestClient          def __init__(self):         self.startFactory()          def clientConnectionFailed(self, connector, reason):         reactor.stop()              def clientConnectionLost(self, connector, reason):         reactor.stop()          def startFactory(self):         print '-- startFactory() called'         self.messageQueue = []         self.clientInstance = None      def clientReady(self, instance):         print '-- clientReady() called'         self.clientInstance = instance         for msg in self.messageQueue:             self.sendMessge(msg)      def sendMessage(self, msg):         if self.clientInstance is not None:             print '$ client instance is not null'             self.clientInstance.sendLine(msg)         else:             print '$ client instance is null'             self.messageQueue.append(msg)   참고로, 창의적으로 고안해서 만든게 아니라 googling 해서 뜨는 걸 보고  작성했다. 아마도 메일링리스트 였던 듯. (나중에 출처를 찾게 되면 꼭 수정해야겠다)   이와 같이, 클라이언트용 프로토콜과 팩토리를 만들었으면 서버에 접속하도록 해보자.   if __name__ == '__main__':     f = TestFactory()     reactor.connectTCP('127.0.0.1', 8001, f)     reactor.run()   이렇게 하면, 앞서 띄운 서버에 연결을 하게 되고, 웬만하면 연결이 끊기지 않는다. (기특하다) 물론, 몇몇 필요한 클래스는  앞서 import 해줘야 한다.   예)  from twisted.protocols.basic import LineReceiver  from twisted.internet.protocol import ClientFactory, ServerFactory from twisted.internet import reactor  ","categories": ["development"],
        "tags": ["twisted","tcp"],
        "url": "http://localhost:4000/development/twisted-matrix-tcp-client/",
        "teaser":null},{
        "title": "어떤 구문의 소요시간을 milliseconds로 확인하려면",
        "excerpt":"java에서의 System.currentTimeMillis()에 해당 하는 것이 Python에서는  어떤 것이 있을까 궁금했다.   현재, 확인한 바로는 time 패키지의 time.clock()이 유용할 듯 하다. 이것의 특징은 애플리케이션이 실행된 시점으로부터 계산되는 count라는 점이다. 즉, 상대적인 stop watch라는 점.      begin = time.clock() ... do something ... end = time.clock() elapsed = end - begin      과 같이, 소요된 시간을 구할 수 있다. 기본 단위가 seconds로 리턴이 되기 때문에 milliseconds는 0.xxx로 표현되는 값으로부터 얻을 수 있다.   반면, datetime.now()을 사용하게 되면, HH:MM:SS:ss의 형태로 값을 얻게 되는데, difference를 구하더라도 마찬가지의 형태로 연산 결과를 산출한다.      begin = datetime.now() .. do something .. end = datetime.now() elapsed = end - begin  elapsed.seconds elapsed.microseconds      결과를 second와 microseconds 속성으로 얻을 수도 있는데, microseconds라고 호출한다고 해서, seconds 이상의 값을 microseconds (milliseconds) 단위로 자동 환산해주지는 않는다. 따라서, milliseconds 범위내에서의 시간 소요를 측정할 경우는 유용하나 1초를 초과하는 범위의 측정에서는 적절치 않을 수도 있다.   ","categories": ["development"],
        "tags": ["python"],
        "url": "http://localhost:4000/development/currenttimemillis_in_python/",
        "teaser":null},{
        "title": "Twisted를 사용하여 TCP Server와 Client를 한번에",
        "excerpt":"Twisted에서 제공하는 TAC를 사용하면, 여러개의 서비스를 동시에 묶어 application service로 등록 사용할 수 있다. 앞서 만들었던 TestClient에 추가적으로 Server의 요소를 넣고자 한다. 예를 들면, Administrator의 목적으로.   이를 위해, 몇몇 패키지를 import 한다.   from twisted.internet.protocol import ServerFactory from twisted.application import internet, service&gt;   추가적으로 서버의 프로토콜과 팩토리를 정의한다.   class AdminProtocol(LineReceiver):     delimiter = 'n';      def lineReceived(self, line):         print line      def connectionMade(self):         print '$NEW CLIENT : %d clients are connected' % self.factory.numProtocols          if self.factory.numProtocols &amp;gt; 100:             self.transport.write(\"Too many connections, try later\")             self.transport.loseConnection()      def connectionLost(self, reason):         self.factory.numProtocols = self.factory.numProtocols - 1                print '$CONNECTION LOST : %d clients are remained' % self.factory.numProtocols      def startedConnecting(self, connector):         print '$started connecting..'  class AdminFactory(ServerFactory):     protocol = AdminProtocol     numProtocols = 0     ID = 0      def __init__(self):         print 'initialized'   Service로 등록하기 위해, service.Service를 상속받아 각각 ServerFactory와 ClientFactory를 리턴하도록 구현한다.   class AdminService(service.Service):     def getTestFactory(self):         f = TestFactory()         f.protocol = TestClient         return f      def getAdminFactory(self):         f = AdminFactory()         f.protocol = AdminProtocol         return f&lt;/td&gt;  애플리케이션 서버로 묶어 주기 위해, 다음과 같이 등록한다.  ```python application = service.Application('test', uid=1, gid=1) f = AdminService()  serviceCollection = service.IServiceCollection(application) internet.TCPServer(8002, f.getAdminFactory()).setServiceParent(serviceCollection) internet.TCPClient('localhost', 8001, f.getTestFactory()).setServiceParent(serviceCollection)   이와 같이 작성한 파이썬 파일을 tac 파일로 변경하면, twisted application으로 실행할 수 있게 되는데, 이대 twistd 명령어로 실행시켜야 한다. ``` twistd -ny finger.tac   #just like before twisted -y finger.tac   # daemonize, keep pid in twistd.pid  ","categories": ["development"],
        "tags": ["twisted","python"],
        "url": "http://localhost:4000/development/twisted-tcp-server-client/",
        "teaser":null},{
        "title": "Recommending places [should vs. had better]",
        "excerpt":"뭔가 추천할 때, 흔히 쓸 수 있는 had better. 그러나, had better가 더 stronger meaning을 갖고 있으므로,   had better 대신 should를 쓰자.   ex) I think you should go to XXX.   조언을 해줄 때는, had better를 쓰지 말자~  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/recommending-places-should-vs-had-better/",
        "teaser":null},{
        "title": "OptionParser를 활용한 commandline parsing 하기",
        "excerpt":"Optik (aka optarse) 를 사용하면, python 애플리케이션 개발시 커맨드라인 옵션/ 파라미터를 파싱하기 쉬워진다.   예를 들어서,  “실행파일명 –config 파일명” 으로 구성되는 형태의 옵션을 만들고 싶다면,   다음과 같이 간단하게 구성할 수 있다.  from optik import OptionParser  def main():     usage = \"usage: %prog [options] arg\"     parser = OptionParser(usage)     parser.add_option(\"-c\", \"--config\", action=\"store\", type=\"string\", dest=\"filename\" )     (options, args) = parser.parse_args()          if options.filename:         print \"cfg : %s\" % options.filename   add_option() 내에 사용된 옵션에 대해 첨언하자면, ㅁ -c : 한개의 대쉬 문자 뒤에 지정되는 문자는 1 캐릭터여야 한다 ㅁ –config : 풀로 표기하는 옵션은 두개의 대쉬 문자뒤에 표기한다.    (두 개의 위치는 바뀌어도 상관없다. 표기만 제대로 한다면) ㅁ action=”store” : c/config  옵션 뒤에 오는  argument를 활용하기 위해 지정함. ㅁ type=”string” : 문자열을 받도록 해줌 ㅁ dest=”filename” : 나중에 c/config 등등 다양한 형태로 지정된 옵션을 통합하여,   호출할 때 사용됨. 즉, c/config 옵션은 “filename”이라는 옵션으로 통합하여    인지한다.   action = “store” 로 저장한 파일이름은 options.filename의 속성값으로 활용한다.   기본적으로 -h/ –help 옵션은 deploy된다. 즉 테스트파일에 –help 옵션을 줘서 실행하면,   Usage: OptionTester.py [options] arg  Options:   -h, --help            show this help message and exit   -c FILENAME, --config=FILENAME   와 같이 결과가 나온다. 여기에서 참고할 만한 내용은, 대쉬의 개수에 따라 동작이 다르게 될 수도 있다는 점.   동일한 결과를 내려면.  -c FILENAME --config=FILENAME --config FILENAME  의 형태로 활용할 수 있다. 보다 자세한 설명 및 활용 방법은  http://optik.sourceforge.net/ 에서 확인할 수 있다.  ","categories": ["python"],
        "tags": [],
        "url": "http://localhost:4000/python/optionparser-commandline-parsing/",
        "teaser":null},{
        "title": "그 책은 히트쳤어~",
        "excerpt":"그 책은 히트쳤어~ 라는 말은 사실 같은 말을 두 번 사용한 잘못된 표현이다. “히트 = 치다” 라는 뜻이기 때문이다.  따라서, 그냥 그 책은 히트였다. 라고 해야 맞는 표현일 것이다.   That book was a hit,  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/that-book-was-a-hit/",
        "teaser":null},{
        "title": "Gastroenteritis?",
        "excerpt":"의학 용어가 나오니 문장이 완전 어려워 보인다. Gastroenteritis is a mild inflammation of the intestines.   (위염은 위/장에 생기는 약한 염증이다)  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/gastroenteritis/",
        "teaser":null},{
        "title": "examples of symptoms",
        "excerpt":"   sore throat&lt;/span&gt;&lt;/span&gt;   &lt;/font&gt;&lt;/strong&gt;a painful or sensitive condition of the throat exaggerated by swallowing or talking, usually caused by bacteria or viruses.   nasal congestion    &lt;/font&gt;&lt;/strong&gt;stuffy nose   runny nose   &lt;/font&gt;&lt;/strong&gt;watery mucus discharge from the nose as in the common cold   chill   a sensation of coldness, often accompanied by shivering and pallor(paleness)   of the skin.   migraine   &lt;/font&gt;&lt;/strong&gt;a severe recurring headache, usually affecting only one side of the head, characterized by sharp pain and often accompanied by nausea, vomiting, and visual disturbances   high blood pressure   elevation of the blood pressure or a condition reseulting from it; its medical term is ‘hypertension’. abbreviation - HBP   cf. low blood pressure - hypotension   stiff neck   &lt;/font&gt;&lt;/strong&gt;a condition of the neck such that the head cannot be moved without difficulty and pain.   insect bite   mosquito bite or any kinds of wound caused by the thrust of an insect’s stinger into skin   bee sting   a sting from a bee   bruise  an injury turning the skin a dark color    blister  caused by burning or irritation    swell  to increase abnormally in size, as by inflation   e.g. Her ankles swelled from standing.    sprained ankle  happens when the ligaments have been damaged by the joint being twisted.    My back is sore  sore - physically painful or sensitive, as a wound, hurt, or diseased partt: a sore arm    sneeze  to emit air or breath suddenly and forcibly through the nose and mouth by involuntary action.   I feel like throwing up [ vomit]   I have trouble with asthma.   wheeze  breathing with difficulty and with a whistling sound  : Asthma caused him to wheeze.         ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/examples-of-symptoms/",
        "teaser":null},{
        "title": "Doctors",
        "excerpt":"Orthopedist   a doctor who treats bone problems   Pediatrician   &lt;/strong&gt;a doctor who treats children only   General Practitioner   &lt;/strong&gt;a doctor who treats the entire family and who does not specialize in only one area   Ophthalmologist   &lt;/strong&gt;a doctor who treats eye problems   Internist   a doctor who specializes in internal medicine   Obstetrician   &lt;/strong&gt;a doctor who cares for pregnant women   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/doctors/",
        "teaser":null},{
        "title": "테이블 내용을 파일로 출력하기",
        "excerpt":"mysql &gt; select * into outfile '파일명' fields terminated by '구분자' from 테이블명   ","categories": ["mysql"],
        "tags": ["tips"],
        "url": "http://localhost:4000/mysql/export_mysql_query_to_file/",
        "teaser":null},{
        "title": "be allergic to, contagious",
        "excerpt":"be allergic to  commonly used when you have some kind of allergies   contagious  a disease is capable of being trasmitted by infection or easily spreads as   from one person to another  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/be-allergic-to-contagious/",
        "teaser":null},{
        "title": "It's on the house",
        "excerpt":"It's on the house.  : 가게에서 지불하므로, (그것에 대해서) 너는 돈을 낼 필요가 없다.   It's on me(my company).   &lt;/font&gt;&lt;/strong&gt;   It's my treat.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/its-on-the-house/",
        "teaser":null},{
        "title": "historic vs. historical",
        "excerpt":"historic   1. well-known or important in history: a historic building; historic occasions   2. notable, renowned, famous, famed, memorable   historical      1. of, pertaining to, treating, or characteristic of history or past events: historical records;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp; historical research.        2. based on or reconstructed from an event, custom, style, etc., in the past: a historical reenactment of the battle of Gettysburg.        3. having once existed or lived in the real world, as opposed to being part of legend or fiction or as distinguished from religious belief: to doubt that a historical Camelot ever existed; a theologian's study of the historical Jesus.        4. narrated or mentioned in history; belonging to the past.        5. noting or pertaining to analysis based on a comparison among several periods of development of a phenomenon, as in language or economics.        쉽게 생각해서,   historic은 ‘역사적인, 역사적으로 의미있는, 중요한’,   historical은 ‘과거의, 옛날의, past’의 뜻으로 보면 됨.&lt;/font&gt;&lt;/strong&gt;&lt;/u&gt;   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/historic-historical/",
        "teaser":null},{
        "title": "의견 말하기의 어조",
        "excerpt":"Strong I feel very strongly that ~ I’m certain that ~ i’m sure that ~   Neutral I think ~ In my opinion ~ From my point of view, ~   Tentative It seems to me that ~ I would say that ~  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/speech-tone/",
        "teaser":null},{
        "title": "공공요금?",
        "excerpt":"   택시요금 : taxi fare   버스요금 : bus fare   지하철 요금 : subway fare   수도요금 : water rate   전기세 : electric charge   헷갈릴 때는 charge만 써도 된다~~~   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/charge/",
        "teaser":null},{
        "title": "내 VIM이 설치된 곳은 어디?",
        "excerpt":"linux에서 vi를 쓰다보면, 어디에 설치되었는지 모르는 웃지 못할 상황이 생기곤 한다. type/ which 명령어를 써도 명확히 알기 어렵다.   이럴 때!! vi를 실행시킨 상태에서 확인이 가능하다는 사실!   :!echo $VIMRUNTIME  ","categories": ["system"],
        "tags": ["vim"],
        "url": "http://localhost:4000/system/vim-location/",
        "teaser":null},{
        "title": "첫 눈을 기다리고 있어요",
        "excerpt":"I am anxious to see the first snowfall. ‘첫 눈’을 the first snow라고만 하기 쉬운데.. snowfall이라고 하자.   자꾸 까먹는다.. anxious를 쓸 때는 조심하자.   I am anxious about the future.  라고 하면,   ‘미래가 기대된다’ 라는 의미보다는 ‘미래가 걱정된다’라는 의미가 더 강하다니깐..  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/first-snowfall/",
        "teaser":null},{
        "title": "건강검진을 영어로?",
        "excerpt":"have a medical check-up  이라고 한단다..   Did you have a medical check-up this year?   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/medical-check-up/",
        "teaser":null},{
        "title": "rely on vs. depend on",
        "excerpt":"모두 다, ‘~에게 의지하다’, ‘~을 믿다’라고 알려져 있지만, 두 개의 표현에는 미묘한 뜻의 차이가 있다고 하는데,       rely on : (심리적으로, 정신적으로) ~에(을) 의지하다, 믿다 depend on : (경제적으로) ~에 의지하다, 믿다      의 뜻이라고 한다. 구분해서 사용하자.   '나는 너를 믿어'라는 뜻으로  I depend on you. 라고 했다가 (나는 경제적으로 너를 믿어. 나 너에게 빌 붙는다~)고   오해를 살 수도 있다는 얘기..   이럴 때는, I rely on you.라고 하자.  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/rely-on-vs-depend-on/",
        "teaser":null},{
        "title": "거지가 찬밥 더운밥 가리랴?",
        "excerpt":"말 그대로 표현이 있다.  Beggars can't be choosers.   뭔가 아쉬운 사람이 꼭 있기 마련이다…   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/beggars-cant-be-choosers/",
        "teaser":null},{
        "title": "mySQL drbd primary 설정적용",
        "excerpt":"1. 10.0.0.1에 접속    2. cat /proc/drbd    -&gt; primary인지 secondary인지 정보가 출력됨   3. drbdadm primary mysql           -&gt; primary로 전환   4. mount /dev/drbd0 /drbd/    5. service mysql start   ","categories": ["system"],
        "tags": ["drbd","mysql"],
        "url": "http://localhost:4000/system/mysql_drbd_primary/",
        "teaser":null},{
        "title": "someone's hands are tight",
        "excerpt":"손이 tight 하다 = 바쁘다~   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/someones_hands_are_tight/",
        "teaser":null},{
        "title": "기회비용?",
        "excerpt":"예전에는 A를 선택함으로써 포기하게 되는 B의 비용이라고만 막연히 알고 있었다. 짜장을 선택했기 때문에 포기해야하는 짬뽕의 가치랄까 -_-   두 개의 선택 중, 하나를 택하는 상황만 생각해 봤는데, 여러 개의 경우는 포기한 가치 중 가장 큰 가치를 뜻하는 것이라는 걸 알게 되었다. 기회비용의 정의가, ‘여러 선택방안 중에서 한 개를 선택했을 때 포기한 대안 가운데 가장 좋은 한 가지의 가치’ 라고 한다.   주어진 10만원에 대해, A - 7만원짜리 옷을 산다. B - 8만원짜리 MP3를 산다. C - 9만원짜리 신발을 산다.   사고 싶은 것을 사는 걸 좋은 가치라고 생각한다면, B를 택했을 때의 기회비용은 9만원이 되는 것 같은데.. 돈 절약 관점에서는 돈을 덜 쓰는게 더 좋은 가치가 되는 건 아닐까?   그럼, 7만원이 기회비용이라고 우겨볼 수도 있을 것 같은데? -_-  ","categories": ["life"],
        "tags": [],
        "url": "http://localhost:4000/life/opportunity-cost/",
        "teaser":null},{
        "title": "베블렌 효과 (Veblen effect)",
        "excerpt":"“상층계급의 두드러진 소비는 사회적 지위를 과시하기 위하여 자각없이 행해진다.”고  Thorstein B. Veblen이라는 사람이 &lt;유한계급론&gt;이라는 저서를 통해 소개한 개념이다.   가격이 오르는데도 일부 계층의 과시욕이나 허영심 등으로 인해 수요가 줄어들지 않는 현상을 일컫는다.   BusinessDictionary.com에 인용된, Veblen effect의 정의를 소개하자면,   “Abnormal market behavior where consumers purchase the higher-priced goods whereas similar low-priced (but not identical) substitutes are available. It is caused either by the belief that higher price means higher quality, or by the desire for conspicuous consumption (to be seen as buying an expensive, prestige item). Named after its discoverer, the US social-critic Thorstein Bunde Veblen (1857-1929).”   언젠가 고향 친구 녀석이 했던 말이 생각이 난다. 학원에서 하는 과외 조차도, 값이 어느 정도 비싸줘야 학원 수강생이 는다고. 싸기만 하면, 품질을 의심한다나…?   사회의 어느 구석이든 사회적으로 허용되는 ‘허영심의 임계치’가 있는 것인지.. 품질을 보장해 줄 수 있는 보이지 않는 가격선이 존재하는 것인지 알 수 없다.   ","categories": ["life"],
        "tags": [],
        "url": "http://localhost:4000/life/veblen-effect/",
        "teaser":null},{
        "title": "너무 좋아서 사실이라고 믿기지 않아",
        "excerpt":"This is too good to be true.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/too-good-to-be-true/",
        "teaser":null},{
        "title": "현재 클래스의 이름 얻기 + 현재 클래스의 메소드 얻기",
        "excerpt":"현재 클래스의 메소드 이름 얻기  class Base 에 대해, Base.dict 라고 하면, Base에 선언된 정보를 얻을 수 있으나,   이중, 메소드 이름만 추출하고 싶다면,  from types import *  def getMethodNames():   result = []   for attr, val in Base.__dict__.items():     if type(val) == FunctionType:        result.append(attr)   return result   과 같이 함으로써, method 이름을 추출할 수 있다. 그러나,  이미 특정 class로부터 instance를 만든 경우에는 위의 방법을 그대로 사용할 수 없다. 즉, Base.getMethodNames()라고 하면 결과를 얻을 수 있지만,   aa = Base() aa.getMethodNames()   라고 하면, 데이터를 얻을 수 없다. 따라서, aa 로부터 instance하기 전의 클래스가 무엇인지 알아내야 한다.   self.__class  라고 하면, 현재의 클래스를 리턴한다. 따라서, 위의 for 문에서 Base.__dict__items() 대신   self.__class__.__dict__items()  라고 쓰면 된다. 이때, 클래스의 이름을 self.class.name 으로 하여 확인할 수도 있는데, self.class.dict.itesm() 와 eval(self.class.name).dict.items()는   동일한 결과물을 뽑아준다.   즉, class와 eval(class의 이름)이 동일하게 취급된다는 것.   ","categories": ["development"],
        "tags": ["python"],
        "url": "http://localhost:4000/development/get-current-methodname-current-class/",
        "teaser":null},{
        "title": "queue의 push/pop 속도 비교",
        "excerpt":"ㅁ Queue.Queue    1. push(‘a’) : 6.39s    2. push(‘a’) + pop() : 11.37s   ㅁ collections.dequeue    1. append(‘a’) : 0.13s    2. append(‘a’) + popleft() : 0.27s   ㅁ list   1. append(‘a’) : 0.15s   2. append(‘a’) + pop() : 0.40s   Queue를 쓰려면, dequeue를 활용하는 것이 좋을 것 같다.   ","categories": ["development"],
        "tags": ["python","performance"],
        "url": "http://localhost:4000/development/queue-pushpop-comparison/",
        "teaser":null},{
        "title": "How to catch 'ExpatError exception' (handling)",
        "excerpt":"ElementTree와 같은 패키지를 사용하여 XML를 파싱하는 경우, XML 엘리먼트의 짝이 안맞는 등, 유효하지 않은 XML 구성이 탐지되면 ExpatError가 뜨는데,   try:  &nbsp; # XML 연산  catch ExpatError, e:  &nbsp; # do something   하면,   NameError: global name ‘ExpatError’ is not defined라는 에러가 뜬다.   이를 해결하려면,   ExpatError를 catch하는 py 파일의 앞에, scope을 맞춰서   from xml.parsers.expat&nbsp; import ExpatError    라고 넣어주자. 그러면, 문제 해결!!   ’’   While you are using XML packages such as ElementTree, you might want to handle   ExpatError which might be caused by invalid-format or missing tags and so on.   However, you will see another error message saying “NameError: global name ‘ExpatError’   is not defined”.  What you only have to solve this problem is just add a line on the top of   your code.   That is,   from xml.parsers.expat import ExpatError   It’ll work, then.   ","categories": ["development"],
        "tags": ["python"],
        "url": "http://localhost:4000/development/how-to-catch-expaterror-exception-handling/",
        "teaser":null},{
        "title": "현재 함수의 이름 얻기",
        "excerpt":"현재 함수의 이름 얻기  def whoami():  import sys   return sys._getframe(1).f_code.co_name   현재 함수의 caller 이름 얻기  def callersname():  import sys   return sys._getframe(2).f_code.co_name&lt;/font&gt;&lt;/font&gt;   출처 : Python Recipe 66062: Determining Current Function Name   ","categories": ["development"],
        "tags": ["python","method"],
        "url": "http://localhost:4000/development/python-current-method-name/",
        "teaser":null},{
        "title": "현재 클래스 이름, 메소드 이름, 라인 넘버 얻기",
        "excerpt":"현재 실행하는 클래스의 이름과 메소드 이름, 라인 넘버를 얻어보자. 클래스의 인스턴스에서 호출한다는 가정하에,   def trace(obj, toList = False): \timport sys \tclassName = obj.__class__.__name__  \tmethodName = callersname() \tlineNumber = sys.exc_info()[2].tb_lineno  \tif toList: \t\treturn className, methodName, lineNumber \telse: \t\treturn \"%s.%s[%d]\" % (className, methodName, lineNumber)  단, callersname()은   def callersname(): \treturn sys._getframe(2).f_code.co_name   그러나, 호출하는 클래스가 특정 클래스를 상속한 경우, 부모에 정의된 함수를 자식 클래스에서 그대로 사용하면서 trace()를 호출하였다면, 부모에 정의된 메소드를 실행하고 있더라도, 자식 클래스의 이름을  현재 클래스의 이름으로 리턴한다.   즉, A에 a()가 정의되어 있고, b = B(A)이나, b.a()를 실행도중 trace()를 호출하면,  현재 클래스의 이름이 A로 출력되는 대신 B로 출력된다.   ","categories": ["development"],
        "tags": ["python"],
        "url": "http://localhost:4000/development/current-method-name-in-python/",
        "teaser":null},{
        "title": "Books that I want to read sometime",
        "excerpt":"from :   http://agile.dzone.com/news/top-50-new-software   1 Dreaming in Code: Two Dozen Programmers, Three Years, 4,732 Bugs, and One Quest for Transcendent Software/Scott Rosenberg/ 26-1-2007   2 Clean Code: A Handbook of Agile Software Craftsmanship/Robert C. Martin/ 11-8-2008   3 Pragmatic Thinking and Learning: Refactor Your Wetware/Andy Hunt/ 15-8-2008   4 Managing Humans: Biting and Humorous Tales of a Software Engineering Manager/Michael Lopp/ 12-6-2007   5 Beautiful Code: Leading Programmers Explain How They Think/Andy Oram, Greg Wilson/ 26-6-2007   6 SOA Principles of Service Design/Thomas/ Erl 28-7-2007   7 The Productive Programmer/Neal Ford/ 3-7-2008   8 Smart and Gets Things Done: Joel Spolsky’s Concise Guide to Finding the Best Technical Talent/Joel Spolsky/ 31-5-2007   9 Making Things Happen: Mastering Project Management/Scott Berkun/ 25-3-2008   10 Release It!: Design and Deploy Production-Ready Software/Michael Nygard/ 30-3-2007   11 The Art of Agile Development/James Shore, Shane Warden/ 26-10-2007   12 Service-Oriented Modeling: Service Analysis, Design, and Architecture/Michael Bell/ 25-2-2008   13 Scaling Software Agility: Best Practices for Large Enterprises/Dean Leffingwell/ 8-3-2007   14 The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine/Charles Petzold/ 16-6-2008   15 Sketching User Experiences: Getting the Design Right and the Right Design/Bill Buxton/ 11-4-2007   16 Continuous Integration: Improving Software Quality and Reducing Risk/Paul Duvall, Steve Matyas, Andrew Glover/ 9-7-2007   17 SOA Design Patterns/Thomas Erl/ 23-10-2008   18 The Developer’s Guide to Debugging/Thorsten Grotker, Ulrich Holtmann, Holger Keding, Markus Wloka/ 11-8-2008   19 Agile Adoption Patterns: A Roadmap to Organizational Success/Amr Elssamadisy/ 7-7-2008   20 Manage It!: Your Guide to Modern, Pragmatic Project Management/Johanna Rothman/ 7-6-2007   21 The Principles of Project Management/Meri Williams/ 13-3-2008   22 Introduction to Information Retrieval/Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze/ 7-7-2008   23 Head First Software Development/Dan Pilone, Russ Miles/ 11-1-2007   24 Web Service Contract Design and Versioning for SOA/Thomas Erl, Anish Karmarkar, Priscilla Walmsley/ 21-9-2008   25 The Art of Multiprocessor Programming/Maurice Herlihy, Nir Shavit/ 29-2-2008   26 Scaling Lean &amp; Agile Development: Thinking and Organizational Tools for Large-Scale Scrum/Craig Larman, Bas Vodde/ 22-12-2008   27 SOA in Practice: The Art of Distributed System Design/Nicolai M. Josuttis/ 24-8-2007   28 Agile Testing: A Practical Guide for Testers and Agile Teams/Lisa Crispin, Janet Gregory/ 5-1-2009   29 The Business Analyst’s Handbook/Howard Podeswa/ 4-11-2008   30 Scrum and XP from the Trenches/Henrik Kniberg/ 4-10-2007   31 xUnit Test Patterns: Refactoring Test Code/Gerard Meszaros/ 31-5-2007   32 Applied SOA: Service-Oriented Architecture and Design Strategies/Michael Rosen, Boris Lublinsky, Kevin T. Smith, Marc J. Balcer/ 13-6-2008   33 97 Things Every Software Architect Should Know /Richard Monson-Haefel/ 13-2-2009 34 Perfect Software: And Other Illusions about Testing/Gerald M. Weinberg/ 29-8-2008   35 Expert Product Management: Advanced Techniques, Tips and Strategies for Product Management &amp; Product Marketing/Brian Lawley/ 10-10-2007   36 The Enterprise and Scrum/Ken Schwaber/ 13-6-2007   37 Algorithms in a Nutshell /George Heineman, Gary Pollice, Stanley Selkow/ 1-11-2008   38 The Software Project Manager’s Bridge to Agility/Michele Sliger, Stacia Broderick/ 29-5-2008 39 Designing Web Interfaces: Principles and Patterns for Rich Interactions/Bill Scott, Theresa Neil/ 15-1-2008   40 If I Only Changed the Software, Why is the Phone on Fire?/Lisa K. Simone/ 23-3-2007   41 Puzzles for Programmers and Pros/Dennis Shasha/ 7-5-2007   42 Managing the Test People/Judy McKay/ 27-4-2007   43 Practical Project Initiation: A Handbook with Tools/Karl E. Wiegers/ 8-8-2007   44 Simple Architectures for Complex Enterprises/Roger Sessions/ 19-5-2008   45 How We Test Software at Microsoft/Alan Page, Ken Johnston, Bj Rollison/ 16-8-2008   46 The One Page Project Manager for IT Projects/Clark A. Campbell/ 4-8-2008   47 The Art of Lean Software Development: A Practical and Incremental Approach/Curt Hibbs, Steve Jewett, Mike Sullivan/ 15-12-2008   48 Code Leader: Using People, Tools, and Processes to Build Successful Software/Patrick Cauldwell/ 5-5-2008   49 Scrumban - Essays on Kanban Systems for Lean Software Development/Corey Ladas/ 12-1-2009   50 Software Requirement Patterns/Stephen Withall/ 13-6-2007       ","categories": ["reading"],
        "tags": ["books"],
        "url": "http://localhost:4000/reading/books-that-i-want-to-read-sometimes/",
        "teaser":null},{
        "title": "They were meant for each other.",
        "excerpt":"그 사람들은 천생연분이다.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/they-were-meant-for-each-other/",
        "teaser":null},{
        "title": "Windows에서 DNS cache  지우기",
        "excerpt":"windows 자체도 DNS caching을 하므로,  콘솔에서 다음과 같이 입력한다.  ipconfig /flushdns   ","categories": ["systems"],
        "tags": ["dns"],
        "url": "http://localhost:4000/systems/windows-dns-cache-flushing/",
        "teaser":null},{
        "title": "영어로 임대인? 임차인?",
        "excerpt":"landlord  the person who actually owns the building   manager the person who cares about everything in the building   tenant the person who is renting the building   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/landlord-tenant/",
        "teaser":null},{
        "title": "Recommended Books for S/W & H/W Developers (@Intel)",
        "excerpt":"Recommended Reading List   Books for H/W Developers  I/O Interconnection Technologies  Designing High-Speed Interconnect Circuits/Dennis Miller  Jitter, Noise, and Signal Integrity at High-Speed/Mike Peng Li  Timing Analysis and Simulation for Signal Integrity Engineers/Greg Edlund  High-Speed Signal Propagation: Advanced Black Magic/Howard W. Johnson  Introduction to PCI Express*/Adam Wilen, Justin Schade, and Ron Thornburg  PCI Express* Electrical Interconnect Design/Dave Coleman, Scott Gardiner, Mohammad Kolbehdari and Stephen Peters  PCI Express* System Architecture/Mindshare Inc, Ravi Budruk, Don Anderson, Tom Shanley  USB Design by Example, Second Edition/John Hyde  InfiniBand Architecture: Development and Deployment/William T. Futral   Power and Thermal Management  Thermal Guidelines For Data Processing Environments/TC9.9 MISSION CRITICAL FACILITIES  Building the Power Efficient PC/Jerzy Kolinski, Ram Chary, Andrew Henroid and Barry Press  Datacom Equipment Power Trends and Cooling Applications/Refrigerating and Air Conditioning Engineers American Society of Heating  Hot Air Rises and Heat Sinks: Everything You Know About Cooling Electronics Is Wrong/Tony Kordyban  Thermal Guidelines For Data Processing Environments/TC9.9 MISSION CRITICAL FACILITIES  Power Management in Mobile Devices/Findlay Shearer  Thermal Management Handbook: For Electronic Assemblies/Jerry E. Sergent, Al Krum   Storage Technologies  Serial ATA Storage Architecture and Applications/Knut Grimsrud, Hubbert Smith  Storage Network Performance Analysis/Huseyin Simitci  Storage Networking Fundamentals/Marc Farley   Wireless Technologies  802.11 Wireless Networks: The Definitive Guide/Matthew Gast  Advanced Wireless Communications: 4G Technologies/Savo G. Glisic  Implementing 802.11, 802.16, and 802.20 Wireless Networks/Ron Olexa  Mobile Web Services/Ariel Pashtan  Multi-Platform Wireless Web Applications: Cracking the Code/Dreamtech Software Team  Real 802.11 Security: WiFi Protected Access and 802.11i/Jon Edney, William A. Arbaugh  Ultra-wideband Radio Technology/Kazimierz Siwiak, Debra McKeown  Ultra-wideband Wireless Communications and Networks/Xuemin Shen, Mohsen Guizani  WiMAX Handbook/Frank Ohrtman  WiMax Operator's Manual: Building 802.16 Wireless Networks/Daniel Sweeney  Fundamentals of WiMAX: Understanding Broadband Wireless Networking/Jeffrey G. Andrews, Arunabha Ghosh, Rias Muhamed  Wireless Hacks, 2nd Edition/Rob Flickenger, Roger Weeks     Books for S/W Developers  S/W Threading for Many Core Architectures  Computer Architecture: A Quantitative Approach, Third Edition/John L. Hennessy and David Patterson  Concurrent Programming in Java*: Design Principles and Patterns, Second Edition/Douglas Lea  Multi-Core Programming/Shameem Akhter, Jason Roberts  Multithreading Applications in Win32: The Complete Guide to Threads/Jim Beveridge, Robert Wiener  Parallel Programming in C with MPI and OpenMP/Michael J. Quinn  Parallel Programming in OpenMP/Rohit Chandra, et al  Using OpenMP: Portable Shared Memory Parallel Programming/Barbara Chapman, Gabriele Jost, Ruud van der Pas  Parallel Programming with MPI/Peter Pacheco  Patterns for Parallel Programming/Timothy G. Mattson, Beverly A. Sanders, Berna L. Massingill  Programming with Hyper-Threading Technology/Andrew Binstock, Richard Gerber  Programming with POSIX Threads/David R. Butenhof  The Software Optimization Cookbook, Second Edition/Richard Gerber, Aart J.C. Bik, et al  Optimizing Applications for Multi-Core Processors/Stewart Taylor  Intel Threading Building Blocks: Outfitting C++ for Multi-core Processor Parallelism/James Reinders  Java Concurrency in Practice/Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, Doug Lea  Threads Primer: A Guide to Multithreaded Programming/Bil Lewis, Daniel J. Berg   Software Development  Beyond BIOS/Vincent Zimmer, Michael Rothman, and Robert Hale  Breaking Through the BIOS Barrier/Adrian Wong  C Interfaces and Implementations: Techniques for Creating Reusable Software/David R. Hanson  Code Complete, Second Edition/Steve McConnell  IA64 Linux Kernel: Design and Implementation/David Mosberger, Stephane Eranian  Intel® Integrated Performance Primitives/Stewart Taylor  Mac OS X* Internals: A Systems Approach/Amit Singh  Microsoft Windows* Internals, Fourth Edition/Mark E. Russinovich, David A. Solomon  The C Programming Language, Second Edition/Brian W. Kernighan, Dennis Ritchie  The Common Language Infrastructure Annotated Standard/Jim Miller, Susann Ragsdale  The Software Vectorization Handbook/Aart J.C. Bik  UPnP† Design by Example/Michael Jeronimo, Jack Weast  VTune™ Performance Analyzer Essentials/James Reinders  Windows System Programming, Third Edition/Johnson M. Hart   High-Performance Computing  Software Optimization for High-Performance Computing: Creating Faster Applications/Isom Crawford, Kevin Wadleigh  Grid Computing: The New Frontier of High-Performance Computing (Volume 14)/Lucio Grandinetti  High-Performance Cluster Computing: Architectures and Systems/Rajkumar Buyya  High-Performance Computing Systems and Applications/Robert D. Kent, Todd W. Sands  High-Performance Linux Clusters with OSCAR, Rocks, OpenMosix and MPI/Joseph D Sloan  High-Performance Computing: Paradigm and Infrastructure/Laurence T. Yang, Minyi Guo  Principles of Transaction Processing/Philip A. Bernstein, Eric Newcomer  Transaction Processing: Concepts and Techniques/Jim Gray, Andreas Reuter  Using MPI2/William Gropp, et al   Graphics and Gaming Technologies  OpenGL® Programming Guide: The Official Guide to Learning OpenGL®, Version 2.1, Sixth Edition/Dave hreiner, Mason Woo, Jackie Neider, Tom Davis  OpenGL® Shading Language, Second Edition/Randi Rost  Game Programming Gems 6/Mike Dickheiser  Game Programming Gems 7/Scott Jacobs  ShaderX5: Advanced Rendering Techniques/Wolfgang Engel  OpenGL® Library, Fourth Edition/Dave Shreiner, Randi Rost   Digital Home Technologies  Audio in the 21st Century/Scott Janus  Designing for Product Sound Quality/Richard H. Lyon  Developing User Interfaces: Ensuring Usability Through Product &amp; Process/Deborah Hix, H. Rex Hartson  Digital Video and HDTV Algorithms and Interfaces/Charles Poynton  Fundamentals of Audio and Video Programming for Games/Peter Turcan, Mike Wasson  High-Definition Audio for the Digital Home/David Roach, Scott Janus, and Wayne Jones  Power Line Communications and Its Applications (ISPLC), 2006 IEEE International Symposium IEEE Press  Psychoacoustics: Facts and Models, Third Edition/Hugo Fastl and Eberhard Zwicker  Usability in Practice: How Companies Develop User-Friendly Products/Michael E. Wiklund   IT Strategic Considerations  Managing Information Technology for Business Value/Martin Curley  Measuring the Business Value of Information Technology/David Sward  Managing IT Innovation for Business Value/Esther Baldwin and Martin Curley   Data Center Technologies  Advanced Server Virtualization: VMware and Microsoft Platforms in the Virtual Data Center/David Marshall, Wade A. Reynolds, Dave McCrory  Definitive XML Schema/Priscilla Walmsley  Service Oriented Architecture (SOA): Concepts, Technology, and Design/Thomas Erl  Service Oriented Architecture Demystified/Girish Juneja, Blake Dournaee, Joe Natoli, and Steve Birkel  Service Oriented Architecture (SOA): A Planning and Implementation Guide for Business and Technology/Eric A. Marks, Michael Bell  SOA Principles of Service Design/Thomas Erl  Applied Virtualization Technology/Sean Campbell and Michael Jeronimo  Design Considerations for Datacom Equipment Centers/Refrigerating and Air Conditioning Engineers American Society of Heating  Internet Communications Using SIP: Delivering VoIP and Multimedia Services with Session Initiation Protocol/Henry Sinnreich, Alan B. Johnston  IPTV Crash Course (Publish date Nov 2006)/Joseph M. Weber, Tom Newberry  Itanium® Architecture for Programmers: Understanding 64-Bit Processors and EPIC Principles/James S. Evans, Gregory L. Trimper  Patterns of Enterprise Application Architecture/Martin Fowler  Scientific Computing on Itanium®-based Systems/Marius Cornea, Ping Tak Peter Tang, John Harrison  Itanium® Architecture for Software Developers/Walter Triebel  Programming Itanium®-based Systems/Walter Triebel, Joseph Bissell, Rick Booth  Virtual Machines: Versatile Platforms for Systems and Processes/Jim Smith, Ravi Nair  Server Consolidation With the IBM Eserver Xseries* 440 and Vmware Esx Server*/Steve Russell, Keith Olsen, Gabriel Sallah, Chandra Seetharaman, David Watts  Switching to VoIP/Theodore Wallingford   Business Client Technologies  Building Applications with the Linux* Standard Base/Linux Standard Base Team  Flash Memory Technologies: A Comprehensive Guide to Understanding and Using Flash Memory Devices/Joseph E. Brewer  Virtualization: From the Desktop to the Enterprise/Chris Wolf, Erick M. Halter  Practical Guide to Trusted Computing/David Challener, Kent Yoder, Ryan Catherman, David Safford, Leendert Van Doorn  Cisco Network Admission Control, Volume I: NAC Framework Architecture and Design/Denise Helfrich, Lou Ronnau , Jason Frazier, Paul Forbes  Windows Server* 2008 Networking and Network Access Protection (NAP)/Joseph Davies and Tony Northrup  The Intel Safer Computing Initiative/David Grawrock       ","categories": ["reading"],
        "tags": ["books"],
        "url": "http://localhost:4000/reading/recommended-books-for-sw-hw-developers-intel/",
        "teaser":null},{
        "title": "예정보다 늦어지고 있다",
        "excerpt":"behind schedule 이 프로젝트는 예정보다 세 달 늦어지고 있다.   This project is three months behind schedule.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/behind-the-schedule/",
        "teaser":null},{
        "title": "남의 눈에 띄지 않으려 하다",
        "excerpt":"try to keep a low profile 그녀가 남의 눈에 띄지 않으려고 했던 것 같아. I guess she was trying to keep a low profile.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/try_to_keep_a_low_profile/",
        "teaser":null},{
        "title": "매 ~ 마다",
        "excerpt":"every + 기수 + 복수 = every + 서수 + 단수    eg. 5일마다 every 5 days = every 5th day   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/every-5-days/",
        "teaser":null},{
        "title": "종속과목강문계를 영어로?",
        "excerpt":"옛날에 학교에서 배웠던 종속과목강문계가 영어로 무엇인지 궁금했다. Database 책을 보다가 우연히 예제에서 보게 되어, 더 찾아보았다.   종&lt;속&lt;과&lt;목&lt;강&lt;문&lt;계  (계로 갈수록 더 범위가 커지는 것임)   계(界) kingdom 문(門) divisio, division (동물분류에서는 phylum) 강(綱) classis, class 목(目) ordo, order 과(科) familia, family 족(族) tribus, tribe 속(屬) genus 절(節) sectio 계(系) series 종(種) species 변종(變種) varietas, variety 품종(品種) forma 재배변종(栽培變種) cultivar 개체(個體) clone  위의 각 영어표기 앞에 sub를 붙이면 아(亞)-의 의미가 된다.  예를 들어 genus 앞에 sub를 붙여 subgenus를 만들면 아속(亞屬)의 의미를  갖는 단위의 표기가 된다.  따라서, 종&lt;속&lt;과&lt;목&lt;강&lt;문&lt;계는 species&lt;genus&lt;family&lt;order&lt;class&lt;division&lt;kingdom가 된다.  ","categories": ["english"],
        "tags": ["classification"],
        "url": "http://localhost:4000/english/classification/",
        "teaser":null},{
        "title": "from Data Model to Relational Schema",
        "excerpt":"        각 클래스에 대해 table을 만든다            각 속성에 대해 필드를 만들고, 적절한 타입을 할당한다.  필요시, 필드를 하위 필드의 조합으로 구성한다.            한 개 또는 여러 필드의 조합으로 primary key를 선정한다.   선정 과정에서, 해당 키의 고유성이 보장되는지 충분히 검토한다.            Many-Many Relationship은, 한 개의 새로운 중간 클래스를 연계하여,  두 개의 1-Many relationship으로 분할하여 처리한다.            1-Many relationship의 경우, 1쪽의 primary key를 Many 쪽에 foreign key로 등록하여 처리한다.            1-1 relationship은 서로 비교하여, 해당 정보를 보다 필요로 하는 쪽으로 나머지 클래스의   primary key를 자신의 foreign key로 등록한다.            필수 항목에 대해, foreign key가 null이 아니어야 한다는 제약 조건을 부여한다.            상속에 대해, 1-1 relationship의 규칙을 적용한다. parent의 key를 child에 foreign key로 등록한다.       from Beginnig Database Design, pp 136, Clare Churcher   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/from-data-model-to-relational-schema/",
        "teaser":null},{
        "title": "Isn't she leaving? vs. Is she leaving?",
        "excerpt":"모두 알고 있는 의문문의 형태지만, 막상 내가 이런 형식의 문장을 쓴다면 상대방이 어떻게 받아들일지 생각해 보고 쓰자.   Isnt’ she leaving? : 듣는 사람은, ‘저 사람은 그녀가 떠날 것이라고 생각하고 있군’ 이라고 느낄 것이다. Is she leaving : 든는 사람은, ‘저 사람은 그녀가 떠나는지 아닌지 모르고 있군’이라고 느낄 것이다.   스티비 원더의 Isn’t she lovely? 라는 노래를 보면, 오히려 명확해진다.   Is she lovely? 와 Isn’t she lovely? 후자는 분명히, she is lovely 라고 말하고 있다.   다 아는 얘기지만, 가려서 쓰자…   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/isnt-she-leaving-vs-is-she-leaving/",
        "teaser":null},{
        "title": "Costco에서 Ipod Touch 2G 16GB를 사다",
        "excerpt":"사실 산지는 벌써 한 달이  됐다. 그 전에 쓰던, IPod Nano 2G는 와이프에게 넘어갔다.   그 전까지 끊임없는 웹 서핑을 통한, 정보 수집에 머물고 있었다. ‘아이폰이 나오면 사고 싶다’고 다짐하며..   계속되는 KT의 낚시질에도 지쳐갔다. 그러던 어느 날, 회사 동료로부터 문자가 왔다.   Costco에서 IPod Touch 8GB가 299,000원이다. 그 분은, 용량의 활용도를 생각해서 16GB를 419,000원에 사셨다고 했다.   다음 날, 나도 결국 16GB를 지르고야 말았다. 8GB는 내가 갔을 때, 이미 품절이었다.   목적은,     PDA 대용   mVOIP 소프트웨어 테스트 해보고 싶어서 (통신비 절감 등)   디자인이 예뻐서   신기술을 누려보고 싶어서   등등…   아이폰이 나오면, 과다한 데이터 요금제로 인해, 배보다 배꼽이 더 커질 것만 같다. 많은 애플리케이션들이 wi-fi를 기반으로 동작하지만, 지나가다 가끔씩 public AP를 발견하는 기쁨도 은근히 크다.   구입한지 한 달쯤 지난 현재, 다른 목적은 모르겠으나 기존에 쓰던 Tungsten TX로부터 완전히 옮겨왔다. 적어도 용도의 이식은 성공한 셈이다.  ","categories": ["life"],
        "tags": [],
        "url": "http://localhost:4000/life/ipod-touch-2g-16gb/",
        "teaser":null},{
        "title": "javadoc과 package.html",
        "excerpt":"작성한 자바코드를 표준 doclet의 javadoc으로 돌리면, package의 설명이 휑허니 빈칸으로 나온다. 어떻게 주석을 달면, package에 대한 설명을 넣을 수 있을까? package의 entry에 package.html을 작성해 주면 이 문제가 해결된다. 만약 package ab.cd.ef.* 가 존재한다면, 디렉토리가 ab/cd/ef가 존재할 것이다. 따라서, ab/cd/ef/pacakge.html을 작성해주면 javadoc 실행시 package의 description을 채워준다. 이 때, 작성 형식은 매우 간단하다. 다음과 같이 기록하기만 하면 body 부분의 설명이 그대로 반영된다. ```  \t \t \t \t\t넣고 싶은 설명 \t  ``` ","categories": ["development"],
        "tags": ["javadoc"],
        "url": "http://localhost:4000/development/javadoc-package.md/",
        "teaser":null},{
        "title": "fast가 동사로 쓰이면?",
        "excerpt":"‘빠르다’는 형용사이다. 동사로는 단식하다. (종교적으로) 정진하다의 뜻을 갖는다.      단식하다.    I have been fasting all day. (하루종일 굶었다)    2. (종교적으로) 정진하다.   fast on bread and water (빵과 물만 먹고 정진하다)   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/fast-as-a-verb/",
        "teaser":null},{
        "title": "rally가 군중회합?",
        "excerpt":"경기와 관련된 뜻만 있는 줄 알았는데, protest 와 관련된 podcast를 듣다가 다른 뜻이 있는 줄 알게되었다. A rally is a large public meeting that is held in order to show support for something such as a political party.    ex) About three thousand people held a rally to mark international human rights day.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/rally-other-meanings/",
        "teaser":null},{
        "title": "Eclipse에서 java 소스로부터 클래스 다이어그램을 쉽게 만들자/ eUML2",
        "excerpt":"eUML2라는 eclipse 플러그인을 설치하여, 기존에 작성했던 java project의 클래스 다이어그램 및 클래스 연관 관계를 쉽게 그려서 볼 수 있다. 라이센스는 ‘free’ 버전과 ‘studio’ 버전이 있으므로, free를 골라서 설치하자. eclipse에서 자신의 eclipse 버전에 맞는 것으로 골라서, download 받고 설치하면 된다.   대략 다음의 장점이 있다고 광고하고 있다.    Real-time code/model synchronization  UML2.1 compliant and support of OMG XMI  Advanced reversed engineering  Powerful true dependency analyze tools  JDK 1.4 and 1.5 support  Customizable template support. More…   해당 플러그인에 대한 자세한 정보는 http://www.soyatec.com/euml2/ 에서 확인할 수 있다.   ","categories": ["development"],
        "tags": ["java","eclipse","diagram","uml"],
        "url": "http://localhost:4000/development/eclipse-java-uml/",
        "teaser":null},{
        "title": "on the verge of",
        "excerpt":"verge          brink, point, edge, threshold              border, edge, margin, limit, boundary, threshold, brimcome near to, approach, border on, resemble, incline to, be similar to, touch on…       &lt;/span&gt;   가장자리, 경계   on the verge of : ~의 경계에 있으므로 뭔가 변화가 일어나기 직전.                          막 ~ 하려고 하는 중인.   ex)   The country was on the verge of becoming prosperous and successful.   Carole was on the verge of tears.   cf. brink   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/on-the-verge-of/",
        "teaser":null},{
        "title": "style에서 알게 된 edge의 다른 뜻",
        "excerpt":"결국 사전에서 찾아보게 되었다. 그 전에는 다른 뜻이 있는 줄 몰랐는데. ‘edge있게~’ 업계에서는 많이 쓰는 모양이네.   사전에 보면, 다음의 뜻이 있다. If you say that someone or something has an edge, you mean that they have a powerful quality.   ex)     Featuring new bands gives the show an edge.     Greene’s stories had an edge of realism.   from dic.naver.com   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/style-edge/",
        "teaser":null},{
        "title": "set names euckr",
        "excerpt":"만약,  DB의 한글 설정이 UTF-8로 되어 있고 (show variables like ‘c%’  로 확인) 웹서버든 프로그램 상에서든 인코딩을 맞춰서 넣어준다고 치자.   컴퓨터에서 HeidiSQL 같은 프로그램을 써서 DB에 접속해 보면, 한글이 문제 없이 디스플레이 된다.   그러나, command로 mySQL client 접속해보면, 한글이 깨지는 경우가 있다.   그럴 때, set names euckr 이라고 하면, 테이블의 데이터가 정상으로 디스플레이된다.   물론, 그 상태에서 update 문을 실행해도 이상없이 업데이트가 수행된다.   ","categories": ["mysql"],
        "tags": ["encoding","db","mysql","korean"],
        "url": "http://localhost:4000/mysql/set-names-euckr/",
        "teaser":null},{
        "title": "여당을 영어로?",
        "excerpt":"여당은 영어로 governing party 라고 하면 되겠다. 사전에서는 ruling party, government party 라고 소개하고 있다.   최근 6/2 지방 선거 이후, NYT에서 이를 governing party로 활용한 예가 있어 링크를 남겨보고자 한다.   http://www.nytimes.com/2010/06/03/world/asia/03korea.html  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/governing-party/",
        "teaser":null},{
        "title": "I think (I don't think)  & I guess",
        "excerpt":"guess는 think보다 약간 약한 느낌? think는 ‘~라고 생각해’인 반면, guess는 ‘~일 껄’ 의 의미.   I think you can do that. (나는 네가 할 수 있을 거라 생각해.) I guess you can do that. (네가 할 수 있을 거야.)   활용해 보면,   I guess I should go now. I guess we can’t meet today.   그런데, 부정문을 쓸 경우는 think와 guess가 좀 다르다고 한다. 예를 들어,   I guess we can’t go now. 의 문맥을 think로 옮기자면,   I don’t think we can go now. 또는 I think we can’t go now.   가 될텐데, 영어에서 부정어는 되도록 앞에 두고자 하기 때문에  I don’t think we can go now.가 맞다고 한다. 그러나, guess의 경우는 I don’t guess 보다는 I guess 가 더 선호되는 듯.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/i-think-i-dont-think-i-guess/",
        "teaser":null},{
        "title": "Hudson - 설치하기",
        "excerpt":"이제는 일일 빌드보다 Continuous Integration(지속적인 통합)이 트렌드이다. 이와 관련하여, 수많은 종류의 tool이 존재하나, 그 중 Hudson이라는 무료 CI 툴을 설치하고, 주요 설정 방법, 사용방법에 대해 정리하고자 한다.   1. 준비물     JDK   Tomcat (설치 방법에 따라 다름)   Hudson   2. 설치 준비하기  (1) JDK     http://java.sun.com   JAVA_HOME을 jdk 설치 디렉토리로 지정함   (2) Tomcat     http://tomcat.apache.org 로부터 tomcat을 다운로드하여 설치함   CATALINA_HOME을 tomcat의 설치 디렉토리로 지정함   (3) Hudson     http://hudson-ci.org   hudson.zip을 받았다면, hudson.war로 이름 변경   3. 설치하기  (1) Tomcat에 연동하여 설치하기     tomcat 설치 디렉토리의 webapps에 hudson.war를 복사한다.   tomcat을 시작시킨다.   tomcat이 실행되면서 hudson.war의 설치가 진행된다.   http://localhost:8080/hudson 을 입력한다. 끝.   (2) Tomcat과 독립적으로 설치하기     항상 웹 서버를 띄워놓고 작업하는 것만이 좋은 해결책은 아님.   별도의 애플리케이션으로 독립적으로 띄워보자.   hudson.bat 또는 hudson.sh을 다음과 같이 작성한다.   java -DHUDSON_HOME=C:/dev/hudson -jar hudson.war -Xms370m -Xmx370m --httpPort=8001 --ajp13Port=8019 &gt; error.log.txt     단, 본인은 hudson.war의 위치가 C:/dev/hudson이다.   콘솔로 hudson이 동작한다.   http://localhost:8001 을 입력한다. 끝.      ","categories": ["development"],
        "tags": ["ci","hudson"],
        "url": "http://localhost:4000/development/hudson-install/",
        "teaser":null},{
        "title": "Let's go dancing? Let's go to dance? Let's go dance?",
        "excerpt":"어떤 것이 맞는가? 어제 퇴근 길에 라디오에서 이 내용이 나왔다.   Let’s go to dance. 이 문장이 문법적으로 맞는 문장이다. 그러나, 네이티브는 이 문장을 쓰지 않는다고 한다.   대신,   Let’s go dancing이나 Let’s go dance. 의 형태 문장을 더 많이 쓴다고.   알아두자.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/lets-go-dancing-lets-go-to-dance-lets-go-dance/",
        "teaser":null},{
        "title": "무료 SVN - www.unfuddle.com",
        "excerpt":"개인이 사용할 수 있는 무료 SVN 사이트로 www.unfuddle.com을 사용해 보고자 한다. 장점은 “무료”라는 점과, 무료임에도 “비공개”로 사용할 수 있다는 점, Issue 관리를 할 수 있다는 점이다. 단점은 “느리다”는 것이다. 용량은 200MB.    계정을 만들고 로그인하면, 다음과 같은 dashboard를 접할 수 있다.  느리지만, (Assembla, google에서 제공하는 SVN에 비해) 좋은 점이라면 역시 privacy가 보장되고, issue tracking이 된다는 점이다.   출처 : http://www.unfuddle.com   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/svn-wwwunfuddlecom/",
        "teaser":null},{
        "title": "무료 SVN - www.assembla.com",
        "excerpt":"무료로 제공되는 SVN 중에 꽤 괜찮아 보이는 곳으로 알려진 사이트이다. 인터페이스도 예쁘고, www.unfuddle.com에 비해 빠르다. google의 SVN에 비해서도 좋다고 한다.  assembla에서 제안하는 plan은 다음과 같다.    즉, 비싸다. 무료 옵션도 존재하는데, 다음의 두 가지 옵션이 있다.  다만, private 옵션을 사용하게 되면 SVN 만 사용할 수 있고, trac 등 다른 툴을 사용할 수 없다. trac 등을 포함한 다른 툴을 쓰려면, public으로 써야 한다. public 프로젝트를 한다면 이 옵션도 괜찮아 보인다.   속도도 나쁘지 않다.   출처 : http://www.assembla.com   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/svn-wwwassemblacom/",
        "teaser":null},{
        "title": "I can't wait to get an iPhone4",
        "excerpt":"can’t wait to V 는 V 할 때까지 기다릴 수 없다는 뜻이지만, ‘몹시 ~하고 싶다’라는 뜻이 더 자연스럽다.   I can’t wait to get an iPhone4. iPhone을 몹시 사고 싶다. 월드컵만 끝나면 곧이다.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/i-cant-wait-to-get-an-iphone4/",
        "teaser":null},{
        "title": "Google Maps API Tutorial Link",
        "excerpt":"Mike Williams의 홈페이지 http://econym.org.uk/gmap/   Google Maps API를 사용하여 할 수 있는 것들과 방법에 대한 정리가 되어 있다. 참고하자.   ","categories": ["development"],
        "tags": ["google","api"],
        "url": "http://localhost:4000/development/google-maps-api-tutorial-link/",
        "teaser":null},{
        "title": "무료 SVN - www.unfuddle.com 화면 구성 소개",
        "excerpt":"SCJD를 준비 하면서, 준비하는 내용을 SVN에 관리하고, 빌드 및 테스트 등은 Hudson을 통해 진행하려고 한다.  이 과정에서 SVN은 www.unfuddle.com으로 선정하였다.   그 이유는,    무료이다. 적어도 200MB 정도의 공간은 제공한다. issue 관리가 가능하다. 비공개이다.   본 포스팅에서는 www.unfuddle.com의 각 메뉴를 간단하게 소개하고자 한다. 혹시, 관심이 있는 분들은 미리 살짝 볼 수 있을 것이다. (예전에도 말했지만, 공개 프로젝트라면 www.assembla.com도 상당히 좋을 것이라 생각한다.)   1. Dashboard  대쉬보드에서는 메시지 추가, 마일스톤 추가, 티켓 추가, 프로젝트 추가, 프로젝트원 초대 등 프로젝트 전반에 대한 기능을 수행할 수 있도록 되어 있다. 물론, 최근 메시지나 마일스톤, 액티비티 등을 보여주고, 달력을 통해서도 일의 진척사항을 보는데 도움을 준다.   2. Projects  본 화면에서는 프로젝트의 설정을 바꾸거나, 새 프로젝트를 등록할 수 있다. 무료 계정이라면 1개의 프로젝트를 사용할 수 있다. 3. Repositories    SVN 리파지터리는 무제한으로 준다고 한다. 4. Tickets    티켓으로 이슈를 관리할 수 있다. report 기능은 아직 써보지는 않았지만, 발행 또는 완료된 티켓들을 조합하여 보고서를 만드는 것으로 보인다.     5. People  프로젝트 구성원에 대한 정보를 보여준다. 무료 계정에서는 최대 2인의 프로젝트원을 구성할 수 있다.   6. Settings  계정의 plan (무료 또는 유료 상품들)을 결정할 수 있고, timezone을 설정할 수 있다. 현재 계정의 사용량을 보여준다. (무료기준 200MB기준)  ","categories": ["development"],
        "tags": ["svn","unfuddle"],
        "url": "http://localhost:4000/development/svn-wwwunfuddlecom/",
        "teaser":null},{
        "title": "FindBugs - EQ_COMPARETO_USE_OBJECT_EQUALS 해결",
        "excerpt":"사용자가 compareTo()를 override 했을 경우, 볼 수 있는 warning이다. FindBugs의 code inspection에 따르면,     This class defines a compareTo(...) method but inherits  its   equals() method from java.lang.Object. \tGenerally, the value of compareTo should return zero if and only if \tequals returns true. If this is violated, weird and unpredictable \tfailures will occur in classes such as PriorityQueue. \tIn Java 5 the PriorityQueue.remove method uses the compareTo method, \twhile in Java 6 it uses the equals method.  From the JavaDoc for the compareTo method in the Comparable interface:   It is strongly recommended, but not strictly required that (x.compareTo(y)==0)  == (x.equals(y)). Generally speaking, any class that implements the Comparable interface and violates this condition should clearly indicate this fact. The recommended language is \"Note: this class has a natural ordering that is inconsistent with equals.\"    이라는 설명을 볼 것이다.   이 문제의 해결책은, hashCode()와 equals()를 실제 구조에 맞게 다시 override함으로써 해결 가능하다.   ","categories": ["development"],
        "tags": ["java","findbugs","coverage"],
        "url": "http://localhost:4000/development/findbugs-eqcomparetouseobjectequals/",
        "teaser":null},{
        "title": "Eclipse의 내장 hashCode()+equals() VS. Apache의 HashCodeBuilder+EqualsBuilder",
        "excerpt":"Eclipse에 내장된 기능으로도 특정 객체의 hashCode()와 equals()를 작성할 수 있다. 사용법은, 마우스 오른쪽 클릭하여 나오는 context 메뉴로부터 Source &gt; Generate hashCode() and equals() … 를 클릭하면 된다.    그러면, 다음과 같은 코드가 생성될 것이다.   \t@Override \tpublic int hashCode() { \t\tfinal int prime = 31; \t\tint result = 1; \t\tresult = prime * result + ((link == null) ? 0 : link.hashCode()); \t\tresult = prime * result \t\t\t\t+ ((menuCode == null) ? 0 : menuCode.hashCode()); \t\tresult = prime * result \t\t\t\t+ ((menuName == null) ? 0 : menuName.hashCode()); \t\tresult = prime * result \t\t\t\t+ ((upperCode == null) ? 0 : upperCode.hashCode()); \t\treturn result; \t}  \t@Override \tpublic boolean equals(Object obj) { \t\tif (this == obj) \t\t\treturn true; \t\tif (obj == null) \t\t\treturn false; \t\tif (getClass() != obj.getClass()) \t\t\treturn false; \t\tMenuItem other = (MenuItem) obj; \t\tif (link == null) { \t\t\tif (other.link != null) \t\t\t\treturn false; \t\t} else if (!link.equals(other.link)) \t\t\treturn false; \t\tif (menuCode == null) { \t\t\tif (other.menuCode != null) \t\t\t\treturn false; \t\t} else if (!menuCode.equals(other.menuCode)) \t\t\treturn false; \t\tif (menuName == null) { \t\t\tif (other.menuName != null) \t\t\t\treturn false; \t\t} else if (!menuName.equals(other.menuName)) \t\t\treturn false; \t\tif (upperCode == null) { \t\t\tif (other.upperCode != null) \t\t\t\treturn false; \t\t} else if (!upperCode.equals(other.upperCode)) \t\t\treturn false; \t\treturn true; \t}   그런데, Apache의 라이브러리를 사용하면, 보다 깔끔한(!) 코드와 가독성을 얻을 수 있다. 우선, common-lang*.jar를 구하고, 다음과 같이 작성한다.   import org.apache.commons.lang.builder.EqualsBuilder; import org.apache.commons.lang.builder.HashCodeBuilder;  ... \t@Override \tpublic int hashCode() { \t\treturn new HashCodeBuilder(3, 11) \t\t.append(upperCode) \t\t.append(menuCode) \t\t.append(menuName) \t\t.append(link) \t\t.toHashCode(); \t} \t \t@Override \tpublic boolean equals(Object obj) { \t\tif (this == obj) \t\t\treturn true; \t\tif (obj == null) \t\t\treturn false; \t\tif (getClass() != obj.getClass()) \t\t\treturn false; \t\t \t\tMenuItem other = (MenuItem) obj; \t\t \t\treturn new EqualsBuilder() \t\t.append(this.upperCode, other.upperCode) \t\t.append(this.menuCode, \tother.menuCode) \t\t.append(this.menuName, \tother.menuName) \t\t.append(this.link, \t\tother.link) \t\t.isEquals(); \t}   ","categories": ["development"],
        "tags": ["eclipse","hash","equals"],
        "url": "http://localhost:4000/development/eclipse-builtin-hashcodeequals-vs-apache-hashcodebuilderequalsbuilder/",
        "teaser":null},{
        "title": "nothing close to : ~와 거리가 먼",
        "excerpt":"What I have seen was nothing close to being civilized.   내가 본 것은 성숙된 것과는 거리가 멀었다.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/nothing-close-to/",
        "teaser":null},{
        "title": "how to go about ~ing vs. how to ~",
        "excerpt":"~하는 것에 대해 자세히 하나씩 알려주다.   Q) Please tell me how to use it. A) 매뉴얼 읽어봐   Q) Please tell me how to go about using it. A) 우선, 어찌어찌하고, 그 다음은 어떻게 한 다음에, 이렇게 저렇게 해봐.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/how-to-go-about-ing-vs-how-to/",
        "teaser":null},{
        "title": "composition vs. aggregation",
        "excerpt":"UML 다이어그램에서 특히 집합 관계를 표시할 때, 헷갈리는 용어이다. ‘A가 B에 속해있다.’ 혹은 ‘B는 A를 포함한다’의 개념을 표현하고자 할 때 사용하는 표기인데,  어떤 경우에 이 관계는 composition 또는 aggregation이라고 말할 수 있는 것일까?   이에 대해, googling을 하던 중, 적어도 나에게는 와닿는 포스팅이 있어  소개하고자 한다.   물론, 단지 이 글 뿐만 아니라 다른 곳으로부터도 참고하였다.   참고 :  http://www.bestarticle.org/computer/association-aggregation-and-composition-what-are-they-and-how-do-they-differ/   Aggregation   ◇--, 실선으로 표시  대상 객체에 대한 possession을 나타낸다. (ownership 아님) 순환 관계는 안됨 (자기 자신을 포함할 수 없음) \"has-a\" 관계 \"A가 B를 포함한다\"고 했을 경우, B는 A의 구성요소이기도 하지만, A와 별도의 객체이다. 즉, 별도로 존재가능하다. \"근로자\"와 \"주소\" 객체가 있다고 할 경우, 모든 근로자는 주소를 갖고 있지만, 근로자 객체가 소멸한다고 해도 주소는 유효하다.   Composition   ◆--, 실선으로 표시  대상 객체는 ownership을 나타낸다. aggregation과 비슷하나, 큰 차이점은 전체-부분에 대한 관계성이며, 부분 객체는 전체 객체 없이 별도로 존재할 수 없다. 즉, 'A가 B를 포함한다\"고 했을 경우, composition에서는 A가 없는 상태에서의 B는 존재할 수 없으며, A가 소멸될 경우 B도 함께 소멸한다. 예를 들어, \"근로자\"와 \"email주소\" 객체가 있다고 했을 경우, 해당 근로자 객체가 소멸할 경우, 해당 근로자의 email주소 객체도 함께 소멸한다. \"contains-a\" 관계 순환 관계는 안됨 (자기 자신을 포함할 수 없음)   Aggregation vs Composition   aggregation은 \"has-a\" 관계를, composition은 \"contains-a\" 또는 \"전체-부분\" 관계를 표현 aggregation은 \"전체\"-\"부분\" 모두 독립적으로 존재할 수 있으나, composition은 \"전체\"가 \"부분\"의 생명 주기를 통제한다. aggregation은 composition에 비해 약한 관계임 composition은 한 개의 전체에 한 개만 속할 수 있으나, aggregation은 여러 개의 전체에 0~n개 속할 수 있다.    ","categories": ["development"],
        "tags": ["UML","aggregation","composition"],
        "url": "http://localhost:4000/development/composition-vs-aggregation/",
        "teaser":null},{
        "title": "버스/지하철/택시를 타고",
        "excerpt":"어제 퇴근 길에 라디오에서 들었다. “나는 버스/지하철/택시를 타고 가는 중이야.”   영어로는,   I’m on the bus.I’m on the subway.I’m in a taxi.   숙어란다.  그냥 외우라던데.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/on-the-bus-in-a-taxi/",
        "teaser":null},{
        "title": "walk somebody through",
        "excerpt":"누군가를 차근 차근 가르쳐주다.   He’ll walk you through it.   그 사람이 그것을 차근차근 알려줄 것이다.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/walk-somebody-through/",
        "teaser":null},{
        "title": "hudson - 프로젝트 생성하기",
        "excerpt":"Hudson을 설치했으므로, 이제 프로젝트를 생성하자.   “새작업”을 클릭하여, 새 프로젝트를 생성한다. 임의의 프로젝트 이름을 입력하고, “Build a free-style software project”를 선택한다.   다음과 같은 세부 설정 화면을 볼 수 있다. 필요한 정보를 모두 입력한다.    이번에 www.unfuddle.com에 생성한 무료 SVN을 연결하여 프로젝트를 생성하기로 한다. Source Code Management 메뉴로부터, Subversion을 선택하면, 다음 화면을 볼 수 있다.    Repository URL 옆의 ? 버튼을 클릭하여, SVN 위치와 인증 정보를 모두 입력하자. 아래의 화면과 같이 나올텐데, “this link”를 클릭하면 인증 정보를 입력할 수 있는 화면이 나온다.    다음 작업은 Hudson으로 build 작업을 지정하여 소스 변경시 혹은 주기적으로 Continuous Integration이 되도록 해야 할 것이다.  ","categories": ["development"],
        "tags": ["hudson","jenkins"],
        "url": "http://localhost:4000/development/hudson-starting-new-project/",
        "teaser":null},{
        "title": "hudson - 빌드 자동화 설정하기",
        "excerpt":"지금까지는 수동으로 Build Now를 클릭하여, build를 하는 것이었다면,  이제 Continuous Integration을 위해, 소스 변경본을 감지하여 자동으로 프로젝트를 build 하도록 설정을 해야한다. 우선, SVN 설정 부분에서 다음과 같이 체크박스를 설정한다.    그리고, 주기적으로 소스에 변화가 있는지 검사하도록 다음과 같이 Trigger 옵션을 설정한다.    SCM을 polling 한다는 뜻은, 소스에 변화가 있는지 보고 변화가 있을 경우 build를 수행한다는 뜻이라고 보면 된다.   위의 그림에서 보는 바와 같이,   매 5분마다 검사하기 : */5 * * * * 매일 오전 9시에 검사하기 : 00 09 * * *   두 개의 옵션을 부여하였다. 이와 같이 설정하기 전에는,    와 같았으나, polling 옵션을 설정 후 일정 시간이 지나자 자동으로 빌드를 시작하였다.    ","categories": ["development"],
        "tags": ["ci","hudson","jenkins"],
        "url": "http://localhost:4000/development/hudson-setup-autobuild/",
        "teaser":null},{
        "title": "for sale, on sale, having a sale",
        "excerpt":"   The store is for sale.   The store is on sale.   The store is having a sale.   이 문장들의 차이는 무엇일까? 그리고 원래 의도는?      이 가게는 (팔려고) 내놨다. (가게 자체를 파는 것임)   “이 가게는 세일 중이다.” 라고 쓰면 가게에서 파는 물건이 sale 이 아니다. 가게 자체를 싸게 판다는 뜻   이 가게에서 세일을 하고 있다. 세일 행사 중 = 가게의 items 가 on sale 이다.   보통은, “그 가게는 세일 하고 있어” 라는 문장을 생각하면서, 1이나 2로 작성했다면 얼른 고치자.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/for-sale-on-sale-having-a-sale/",
        "teaser":null},{
        "title": "hudson - emma와 연동하기 (1/2)",
        "excerpt":"지난 글t에서 hudson에 JUnit 테스트를 수행하는 방법에 대해 소개하였다. 물론, 코드의 품질은 어떤 테스트 코드를 어떻게 작성하느냐에 코드의 신뢰도가 달라진다.   그렇다면, 좋은 테스트는 테스트 케이스의 수에 단순히 비례할까? 두 말할 필요 없이 얼마나 양질의 테스트가 어떻게 수행되었는지가 중요할 것이다.   본 포스트에서 말하고자 하는 metric은 테스트의 커버리지(coverage)이다. 즉, 테스트의 커버리지가 높은 프로젝트 코드들은 검증을 거친 부분이 많으므로, 상대적으로 양질이라고 볼 수 있다.   Emma는 프로젝트 코드와, 프로젝트 코드를 테스트하는 테스트 코드를 조합하여 비교함으로써, 주어진 테스트 코드가 원본 소스 코드에 대해 어느 정도의 커버리지를 갖는지를 조사해 준다.   Emma는 어디에? Emma는 http://emma.sourceforge.net/에서 다운로드하거나, 설명을 찾아볼 수 있다.   Emma의 기능은? Emma의 공식 사이트에 소개된 기본 기능은 다음과 같다.     EMMA can instrument classes for coverage either \toffline (before they are loaded) or on \tthe fly (using an instrumenting application classloader).    Supported coverage types: \tclass, method, \tline, basic&nbsp;block. EMMA \tcan detect when a single source code line is covered only partially.    Coverage stats are aggregated at method, class, \tpackage, and \"all classes\" levels.    Output report types: plain text, HTML, XML. All \treport types support drill-down, to a user-controlled detail \tdepth. The HTML report supports source code \tlinking.    Output reports can highlight items with coverage \tlevels below user-provided thresholds.    Coverage data obtained in different \tinstrumentation or test runs can be merged together.    EMMA does not require access to the source \tcode and degrades gracefully with decreasing amount of debug \tinformation available in the input classes.    EMMA can instrument individial \t.class files or entire \t.jars (in place, if desired). Efficient coverage \tsubset filtering is possible, too.    Makefile and ANT build integration are supported \ton equal footing.    EMMA is quite fast: the runtime         overhead of added instrumentation is small (5-20%) and the bytecode instrumentor         itself is very fast (mostly         limited by file I/O speed). Memory overhead is a few hundred bytes per Java class.    EMMA is 100% pure Java, has no external         library dependencies, and works in any Java 2 JVM (even 1.2.x).    좋은 기능이지 아니한가? -_-   그렇다면, Hudson과의 연동은? 위의 기능 중,    Report가 가능하다 XML로 report를 생성한다. Ant task로 수행할 수 있다.   등의 기능을 활용하여, 기존의 build.xml에 연동해 볼 것이다.   Emma를 설치하자 앞서 말한, emma 사이트로부터 emma 배포본을 다운로드한다.  본인은 emma-release &gt; 2.0.5312 &gt; emma-2.0.5312.zip 를 다운로드 하였다.   Hudson이 설치된 서버의 적당한 작업공간에 해당 파일을 압축 해제한다.   예) c:devemma-2.0.5312   그리고, emma의 설치 디렉토리를 환경 변수로 지정한다.   예) EMMA_LIB=c:devemma-2.0.5312   build.xml에 emma 설정을 추가하자   build.xml에 다음과 같이 emma의 기본 환경변수를 선언한다.  &lt;!-- emma configuration/directories --&gt; \t&lt;property name=\"emma.enabled\" \t\tvalue=\"true\"/&gt; \t&lt;property name=\"emma.lib.dir\"\t\tvalue=\"${env.EMMA_LIB}\"/&gt; \t&lt;property name=\"emma.coverage.dir\" \tvalue=\"${report.home}/emma\"/&gt; \t&lt;property name=\"emma.origin.dir\"\t        value=\"${build.home}\"/&gt; \t&lt;property name=\"emma.instr.dir\"\t\tvalue=\"${basedir}/instr\"/&gt;  \t&lt;!-- emma libraries --&gt; \t&lt;path id=\"emma.lib\"&gt; \t\t&lt;pathelement location=\"${emma.lib.dir}/emma.jar\"/&gt; \t\t&lt;pathelement location=\"${emma.lib.dir}/emma_ant.jar\"/&gt; \t&lt;/path&gt;   emma가 수행하는 태스크를 일단, 다음의 순서대로 설정하고자 하였다. setup -&gt; instrument -&gt; emma 기반 test -&gt; emma report 생성   ant 태스크의 정의로부터, emma.setup, emma.instrument, emma-test, emma-report의 target은 다음과 같이 선언한다.   &lt;!-- target : emma-setup --&gt; \t&lt;taskdef resource=\"emma_ant.properties\" classpathref=\"emma.lib\"/&gt; \t \t&lt;target name=\"emma.setup\" depends=\"compile,test-compile\" description=\"emma setup\"&gt; \t\t&lt;mkdir dir=\"${emma.instr.dir}\"/&gt; \t\t&lt;mkdir dir=\"${emma.coverage.dir}\"/&gt; \t&lt;/target&gt; \t \t&lt;!-- target : emma-instrument --&gt; \t&lt;target name=\"emma.instrument\" depends=\"emma.setup\" description=\"emma instrument\"&gt; \t\t&lt;emma enabled=\"${emma.enabled}\"&gt; \t\t\t&lt;instr instrpath=\"${emma.origin.dir}\" \t\t\t\tdestdir=\"${emma.instr.dir}\" \t\t\t\tmetadatafile=\"${emma.coverage.dir}/metadata.emma\" \t\t\t\tmerge=\"true\"&gt; \t\t\t\t&lt;filter value=\"\" /&gt; \t\t\t&lt;/instr&gt; \t\t&lt;/emma&gt; \t&lt;/target&gt; \t \t&lt;!-- target : emma-test --&gt; \t&lt;target name=\"emma.test\" depends=\"emma.instrument\" description=\"junit test with emma\"&gt; \t\t&lt;mkdir dir=\"${report.home}/junit\" /&gt; \t\t&lt;junit printsummary=\"yes\" maxmemory=\"512m\" haltonfailure=\"false\" fork=\"true\"&gt; \t\t\t&lt;!--&lt;classpath refid=\"test.classpath\" /&gt;--&gt; \t\t\t&lt;classpath&gt; \t\t\t\t&lt;pathelement location=\"${emma.instr.dir}\"/&gt; \t\t\t\t&lt;pathelement location=\"${emma.origin.dir}\"/&gt;\t\t\t\t \t\t\t\t&lt;pathelement location=\"${test.build.home}\"/&gt; \t\t\t\t&lt;path refid=\"emma.lib\"/&gt; \t\t\t&lt;/classpath&gt; \t\t\t&lt;classpath refid=\"project.classpath\" /&gt; \t\t\t&lt;!-- &lt;formatter type=\"brief\" usefile=\"false\" /&gt; --&gt; \t\t\t&lt;formatter type=\"xml\"/&gt;  \t\t\t&lt;jvmarg value=\"-Demma.coverage.out.file=${emma.coverage.dir}/coverage.emma\"/&gt; \t\t\t&lt;jvmarg value=\"-Demma.coverage.out.merge=true\"/&gt; \t\t\t\t \t\t\t&lt;batchtest todir=\"${report.home}/junit\"&gt; \t\t\t\t&lt;fileset dir=\"${test.build.home}\" includes=\"**/*Test.class\"&gt; \t\t\t\t&lt;/fileset&gt; \t\t\t&lt;/batchtest&gt; \t\t&lt;/junit&gt;  \t\t&lt;junitreport todir=\"${report.home}/junit\"&gt; \t\t\t&lt;fileset dir=\"${report.home}/junit\"&gt; \t\t\t\t&lt;include name=\"TEST*.xml\"/&gt; \t\t\t&lt;/fileset&gt; \t\t\t&lt;report format=\"frames\" todir=\"${report.home}/junit\"/&gt; \t\t&lt;/junitreport&gt; \t&lt;/target&gt; \t \t&lt;!-- traget : emma-report --&gt; \t&lt;target name=\"emma.report\" depends=\"emma.test\" description=\"generate emma report\"&gt; \t\t&lt;emma enabled=\"${emma.enabled}\" verbosity=\"verbose\"&gt; \t\t\t&lt;report sourcepath=\"${src.home}\" sort=\"+block,+name,+method,+class\" \t\t\t\tmetrics=\"method:70,block:80,line:80,class:100\"&gt; \t\t\t\t&lt;fileset dir=\"${emma.coverage.dir}\"&gt; \t\t\t\t\t&lt;include name=\"*.emma\" /&gt; \t\t\t\t&lt;/fileset&gt; \t\t\t\t&lt;!-- \t\t\t\t&lt;fileset dir=\"${basedir}\" &gt; \t\t\t\t\t&lt;include name=\"*.ec\" /&gt; \t\t\t\t&lt;/fileset&gt; \t\t\t\t--&gt; \t\t\t\t&lt;xml outfile=\"${emma.coverage.dir}/coverage.xml\" depth=\"method\"/&gt; \t\t\t\t&lt;html outfile=\"${emma.coverage.dir}/coverage.html\" depth=\"method\"/&gt; \t\t\t&lt;/report&gt; \t\t&lt;/emma&gt;\t \t&lt;/target&gt;   이제 해당 디렉토리로부터 ant emma.report를 실행하면, 커버리지 리포트 디렉토리로 설정해 놓은 곳에  coverage.html과 coverage.xml이 생성되어 있는 것을 확인할 수 있다.   다음에는 이렇게 생성한 리포트를 hudson에 연동해보자.   ","categories": ["development"],
        "tags": ["ant","ci","coverage","emma","hudson","java","test"],
        "url": "http://localhost:4000/development/hudson-emma/",
        "teaser":null},{
        "title": "Syntax Highlighter로 인용할 때 괄호 오류 보정하려면",
        "excerpt":"Syntax Highlighter로 괄호를 인용할 때,   예를 들어  &lt;property name=\"abc\" value=\"123\"/&gt;   와 같이 인용하고자 한다면, 막상 인용해보면 위와 같이 표시되는 대신     과 같이 표시되고 만다.   게다가 nested expression이라면, 본인의 의도와는 다른 문서 구조로 표현되어 원치 않는 output을 만나게 될 것이다.   이럴 경우, 다음의 사이트를 사용하여 손쉽게 escape 시키자. Syntax Highlight 시킬 부분에 변환 코드를 복사해 넣으면 될 것이다.   http://accessify.com/tools-and-wizards/developer-tools/quick-escape/default.php   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/syntax-highlighter-escape/",
        "teaser":null},{
        "title": "more of A (than B)",
        "excerpt":"more of A than B 라고 하면,  “B라기 보다는 오히려/차라리 A”, “B보다는 A에 가깝다”라는 뜻이다.   amazon.com에서 안드로이드 책을 검색하는데, 리뷰 중에 다음과 같은  리뷰 제목이 있었다.   “More of an instruction manual than a book.” 즉, “책이라기보다는 지침 설명서”라는 리뷰 내용이다.   (물론, 좋은 별점을 받았을 리는 없다.)   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/more-of-a-than-b/",
        "teaser":null},{
        "title": "hudson - emma와 연동하기 (2/2)",
        "excerpt":"지난 글에 설정한 바대로 ant task를 정상적으로 진행했다면, ant task로 emma.report 태스크를 수행했을 때, coverage.html과 coverage.xml이 생성되었을 것이다. 참고로, 생성된 coverage.html을 살펴보자.    해당 패키지의 구성중, 클래스/메소드/블럭/라인 기준으로 어느 정도가 test로 커버 되고 있는지를 보여준다. 패키지 이름을 클릭하면,    패키지에 포함된 클래스들이 나타나고, 이 클래스들이 어느 정도 test로 커버되고 있는지 보여준다. 이 중, 아무 클래스나 또 클릭하게 되면,  클래스내의 메소드들이 test로 어떻게 커버되고 있는지 현황을 자세하게 보여주게 된다. 이 결과물은, 별도의 ant task를 수동으로 실행시켜 얻은 결과물이므로, 이제 hudson의 task에 연동하여, 자동으로 build가 되면 emma test를 수행하자. 그러면, hudson이 매 빌드마다 결과물을 자동으로 생성하여 dashboard로부터 볼 수 있도록 설정할 것이다.   “Hudson 관리 &gt; Manage Plugins”를 클릭한다. Available 탭을 클릭하여 살펴보면,    과 같이, Emma plugin 설치에 대한 체크박스가 있다. 이를 체크하고, 우측 하단 구석에 숨어있는 “Install” 버튼을 클릭하자.   성공적으로 Emma plugin을 설치했다면, 다음 화면을 보게 될 것이다.    설명 문구에 뜬 내용처럼, hudson을 재시작시켜 emma plugin이 정상 동작하도록 하자. hudson에 다시 접속해보면, 외형상 변화는 아무 것도 나타나지 않는다.  (이제부터 hudson에 emma 설정을 해야 하니까)   프로젝트명을 클릭하고, Configure를 선택한다. Post-build Actions 부분에 “Record Emma coverage report”가 생성된 것을 확인할 수 있다.   이 체크박스를 클릭하면, 세부 설정 화면이 생긴다.    예전에 ant task로 테스트 했을 경우 coverage.xml이 생성된 경로를 기억하는가? XML report 경로를 넣는 곳에, 프로젝트이름/report/emma/coverage.xml 등과 같은 형식으로 기록해 준다. Health reporting은 그냥 디폴트 값으로 사용하기로 한다. 즉, 비워두면 디폴트값이다. 아직 emma report를 생성하는 ant task가 동작하도록 설정하지 않았기 때문에, build 되더라도 emma report가 자동으로 생성되지는 않을 것이다.   따라서, Post-build Actions 위쪽에 있는 Build &gt; Invoke Ant 옵션을 고치자. Targets에 all.emma.report  라고 써주고, build.xml에 all.emma.report를 추가해 보자.    \t   이제, 해당 프로젝트의 왼쪽 메뉴탭을 보면,  와 같이, Coverage Report가 생성됨을 확인할 수 있다. 이 Coverage Report를 클릭해보자.    coverage trend를 표시하는 그래프가 graphics로 나타나는 것을 확인할 수 있다. 이 화면에서도 패키지명을 클릭해 보자.    임의의 클래스도 클릭해보자.    사실, 내용은 앞서 살펴보았던 html과 다를 것이 없다.  다만, 별도의 ant task를 사용자가 실행하지 않아도, 소스를 수정후에 SVN 서버에 commit 하는 것만으로도 충분하다는 것이다.   지금까지 한 작업은 다음의 과정을 자동화한다.     소스를 개발/수정 후 commit하면,   자동으로 소스를 컴파일하고,   테스트 코드도 컴파일해서 테스트를 시행하고,   테스트 결과를 보여준다.   또한 이 테스트의 적용 범위(coverage)가 어느 정도인지 검사해준다.   이로써, hudson에 ant task를 활용하여 emma를 연동해 보았다.   ","categories": ["development"],
        "tags": ["ci","coverage","emma","hudson","jenkins","java"],
        "url": "http://localhost:4000/development/hudson-emma-2/",
        "teaser":null},{
        "title": "hudson - javadoc 생성하기",
        "excerpt":"사실, 개발하면서 주석을 다는 것은 무척이나 흥미로운 귀찮은 일이다. 게다가 포맷을 지키고, 어떤 파라미터가 넘겨지고, 리턴 값은 어떻고, 어떤 상황에서 어떤 exception이 던져진다는 것까지 써야 한다면 더더욱 그렇다.   보통 프로그램부터 작성한 후, 주석을 달라고 한다면, 주석을 다는 것이 아주 하기 싫은 일이 될 가능성이 크다고 생각한다. 주석을 달면서, 코드 리뷰도 하고, 분석도 하고, 수정도 하는 선순환이 되기 보다는 상당히 형식적인 주석 작업이 될 확률이 더 높아진다.   오히려, 보다 양질의 주석을 달기에 좋은 시기는 해당 부분을 프로그램화 할 때라고 생각한다. 모든 프로젝트를 완료한 후, javadoc을 사용하는 대신에 초기부터 javadoc을 사용해 보자.   자신이 작성하는 코드와 비슷한 시기에 산출물을 생성하는 것이다. 생성된 산출물을 확인하여, 편집하는 수고를 나중으로 미루지 말자.   다행히, hudson에서는 기본적으로 javadoc을 publish하는 기능이 내장되어 있다.   Hudson의 설정 변경하기  프로젝트의 configure 메뉴를 선택한다. Post-build Actions에 보면, 별도의 플러그인을 설치하지 않아도 javadoc에 대한 옵션을 설정할 수 있도록 되어 있다.   Publish Javadoc 옵션을 체크하면, javadoc 문서의 위치를 지정할 수 있다. 프로젝트명/docs/javadoc 과 같이 적어준다. (본인의 경우, 프로젝트명/docs/javadoc에 javadoc 문서가 생성되도록 설정해 두었다.)      ant task 추가하기  javadoc의 디렉토리를 다음과 같이 선언하고,  &lt;!-- javadoc directory --&gt; \t&lt;property name=\"javadoc.home\" \t\tvalue=\"${basedir}/docs/javadoc\" /&gt; &lt;/pre&gt;   javadoc의 task를 다음과 같이 선언한다.  &lt;!-- target : javadoc --&gt; \t&lt;target name=\"javadoc\"&gt; \t\t&lt;javadoc sourcepath=\"${src.home}\" windowtitle=\"J's project\"  \t\t\tdestdir=\"${javadoc.home}\" encoding=\"UTF-8\" charset=\"UTF-8\"  \t\t\tdocencoding=\"UTF-8\"&gt; \t\t&lt;/javadoc&gt; \t&lt;/target&gt;   이제, javadoc의 task까지 준비하였다. ant task로서의 javadoc을 호출하기만 하면 될 것이다.   ant task 실행 설정하기  이제, configure에 Build에 보면, 지난 번에 설정한 all.emma.report task가 보일 것이다. 그 밑에 새로운 ant task를 추가하자.    위의 설정으로부터, 새로운 ant task는 javadoc이라고 이름을 붙였다. (일부러, all.emma.report에서 compile을 하도록 했으므로, 위의 ant task 작성시 depends를 설정하지 않았다.)   설정을 저장하고 나면, hudson 좌측 메뉴에 javadoc 메뉴가 나타날 것이다.      Build Now 를 클릭하거나, 자동으로 빌드가 된 후에 javadoc 생성 여부를 확인해보자. 이로써, hudson에서 javadoc을 생성하고 publsih해 보았다. 이제, 개발하면서 자신의 프로젝트가 빌드되면서 생성되는 산출물의 결과도 함께 확인하며, 문서 작성도 함께 해보자.  ","categories": ["development"],
        "tags": ["hudson","javadoc","ant","java"],
        "url": "http://localhost:4000/development/hudson-javadoc/",
        "teaser":null},{
        "title": "hudson - FindBugs 연동하기",
        "excerpt":"“내가 작성한 코드는 잘 작성한 것일까?” 내가 작성한 코드가 이상없이 동작하는지 검사하기 위해, JUnit 등을 사용하여 테스트를 수행해 왔다면, 이제 이런 질문을 던져볼 만도 하다.  프로그래머가 작성한 코드는 “논리”의 집합이다.   그렇다면, 테스트케이스는 “그 논리가 적합한가?” 혹은 “그 논리에 헛점이 있는가?”를 검증하기 위한 것이라고 할 수 있을 것이다. 그러면, 그 “논리를 세우는 방법이 잘 되어 있는가?”를 검증하는 방법도 있을 법하다. 그래서, “코드검사”를 수행한다!!  코드 검사는 내가 작성하는 코드가 표준에 맞는지, 어떤 잠재적인 오류 패턴을 내포하고 있는지 등을 검사해 준다.   CheckStyle, PMD, FindBugs 등 여러 가지가 있으나, FindBugs를 hudson에 연동하여 사용해 보자.   FindBugs는 어디에?  FIndBugs는 http://findbugs.sourceforge.net 로부터 정보를 얻을 수 있다. 다운로드는 물론이고, 설명도 잘 되어 있다.   설정과정은?  지금까지 hudson과 연결한 다른 플러그인의 설치 방법과 크게 다르지 않다.     FindBugs 설치하기/ 환경변수 등록하기   Ant task에 FindBugs 등록하기   Hudson내 설정하기 의 절차를 따를 것이다.   FindBugs 설치하기  왼쪽 메뉴에 크게 Downloads 섹션이 있다. 링크를 클릭하고, findbugs-1.3.9.zip을 다운 받았다. 이제 원하는 곳에 설치하자.  설치라고 해봐야, 압축풀고 환경변수에 등록하는 일 밖에 없다. 환경변수에   FINDBUGS_HOME=findbugs 설치디렉토리  로 지정하였다.   Ant Task에 FindBugs 등록하기  FindBugs 홈페이지에 자세한 설명이 되어 있다.   세부 내용은 다음 링크를 참조하자. http://findbugs.sourceforge.net/manual/anttask.html   환경변수 설정은 다음과 같이 한다.   &lt;!-- FindBugs configuration/directories --&gt; \t&lt;property name=\"findbugs.home\"  \tvalue=\"${env.FINDBUGS_HOME}\"/&gt;  \t&lt;path id=\"findbugs.classpath\"&gt; \t\t&lt;pathelement location=\"${findbugs.home}/lib/findbugs-ant.jar\" /&gt; \t&lt;/path&gt;  findbugs라는 task도 다음과 같이 정의한다.   &lt;!-- target : FindBugs --&gt; \t&lt;taskdef name=\"findbugs\"  \t\t\tclassname=\"edu.umd.cs.findbugs.anttask.FindBugsTask\"  \t\t\tclasspathref=\"findbugs.classpath\"/&gt; \t \t&lt;target name=\"findbugs\"&gt; \t\t&lt;mkdir dir=\"${report.home}/findbugs\" /&gt; \t\t&lt;findbugs home=\"${findbugs.home}\"                 output=\"xml:withMessages\"                 outputFile=\"${report.home}/findbugs/findbugs.xml\"  \t\t\t\texcludeFilter=\"${report.home}/findbugs/findbugs-filter.xml\"  \t\t\t\ttimeout=\"1800000\" jvmargs=\"-Xmx512m\"&gt; \t\t\t&lt;sourcePath path=\"${src.home}\" /&gt; \t\t\t&lt;class location=\"${build.home}\" /&gt; \t\t&lt;/findbugs&gt; \t&lt;/target&gt;   Hudson내 설정하기  먼저, FindBugs 플러그인을 설치하자.  Hudson의 왼쪽 메뉴에 있는 “Hudson 관리하기”를 클릭하고 나타나는 화면으로부터, Manage Plugins를 클릭한다. Availabe 탭을 클릭하여, 목록을 보면 다음과 같이 FindBugs plugin 항목이 보인다.      FindBugs를 체크하고, 우측 하단의 Install 버튼을 클릭한다. 해당 플러그인을 설치하면, 다음 화면이 나타난다.      이제, hudson을 재시작 해보자. 그러나, 아직 findbugs가 실행되는 것은 아니다. 앞에서 설정한 ant task가 실행되도록 설정하지 않았기 때문이다. 해당 프로젝트로부터 configure 를 클릭하여, ant task를 추가하자. Build 부분에 Inovke Ant를 추가하여, Targets로 findbugs를 기록한다.  (앞에서 findbugs task를 이미 build.xml에 추가하였다.)      이대로 마치면, 다음 빌드에 findbugs는 실행되지만 hudson에서 직접 결과를 볼 수는 없을 것이다. findbugs의 report를 publish 하자.   Post-build Actions에 가보면, 어느새 Publish FindBugs analysis results라는 옵션이 생겨있을 것이다. 이를 체크하고, 자신의프로젝트/report/findbugs/findbugs.xml 과 같은 형태로 적어주고 저장한다.    이제 새로 빌드해 보자. 그러면, 왼쪽 부분에 FindBugs Warnings라는 메뉴가 생길 것이다.      FindBugs가 무엇을 했는지는 클릭해 보면 알 수 있다.    (파일명은 일부러 가렸다.) 문제가 되는 부분의 파일명과 Distribution에 색깔로 위험 정도를 표시해 준다.   세부 항목을 잠시 살펴보면,    해당 부분에서 위와 같은 경고 사항이 발생했고, 해결 방법을 권고해 준다. 참고로 위의 예는, 변수를 선언했으나 사용하지 않았음을 알려주는 경우이다.   FindBugs 등의 코드 검사를 수행시키면, 참 많은 잔소리를 듣게 된다. 그러나, 이는 코드의 품질을 높일 수 있도록 도와주는 즐거운 잔소리가 분명하다.   보다 안전한 코딩을 위해, 기꺼이 잔소리를 들어보자!   ","categories": ["development"],
        "tags": ["ci","hudson","findbugs"],
        "url": "http://localhost:4000/development/hudson-findbugs/",
        "teaser":null},{
        "title": "there is no ~ing",
        "excerpt":"“~하는 것은 불가능하다”, “~할 수 없다”라는 뜻이다.   ~하는 것은 불가능하다, ~할 수 없다라는 말을 영작할 때, 습관적으로 It is impossible to ~, S cannot ~ 라고 했다면,   there is no ~ ing라고도 써보자.  내일 전화영어에는 꼭 써보리라. -_-+   There is no singing in the library. 도서관에서 노래 부를 수 없다. There is no asking how old she is. 그녀 나이를 물어볼 수 없다.   익숙해지자.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/there-is-no-ing/",
        "teaser":null},{
        "title": "a string of ~ 일련의",
        "excerpt":"한자어로  [一連]  이라고, 많이 쓰는데 영어에도 이런 표현이 있다.   string이 프로그램에서는 문자열이지만, 원 단어 뜻에는 “줄”이라는 뜻이 있고, 이를 이용하여 a string of ~ 라고 하면 , “일련의 ~” 라는 뜻이 된다. a string of ~ 라고 하면, ~에 해당하는 것이 마치 줄줄이 꿴 듯한 것을 떠올리자. 유사한 표현으로, a series of / a chain of / a train of / a succession of (다음 사전 참조) 등이 있다.   a string of data ~  a string of events ~ a string of meetings ~ a string of cars ~  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/a-string-of/",
        "teaser":null},{
        "title": "why ~? vs. how come ~?",
        "excerpt":"why와 how come은 둘 다, “왜~?”라는 문장을 구성한다.   그러나, 그 뒤의 문장 순서가 다르다는 차이가 있다. why로 시작하는 의문문은 우리가 알고 있는 의문문의 형태이다. 그러나, how come은 평서문의 순서를 취한다.   “왜 전화 안 했어?” Why didn’t you phone me? How come you didn’t phone me?   “왜 일찍 일어나지 않았어?” Why didn’t you get up early? How come you didn’t get up early?   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/why-vs-how-come/",
        "teaser":null},{
        "title": "JUnit으로 test coverage를 높이는 습관",
        "excerpt":"“우리나라 정서상 어렵다, 현실에 맞지 않다”는  말들을 하기도 하고, 듣기도 한다.   Rod Johson이 그의 저서 “Expert one-on-one J2EE Design and Development”에서  XP 기법을 소개하면서, 그 기법의 모든 것을 따르지는 않더라도 테스트 지향 개발 방법은 바람직하다고 하였다.   테스트에 대한 XP의 기법은,   코드를 작성하기 전에 먼저 테스트 코드를 작성하자  모든 코드는 단위 테스트 코드를 가져야 하고, 각 단위 테스트는 자동으로 실행될 수 있어야 한다. 버그가 발견되면 버그를 고치기에 앞서, 버그를 다시 재현해 내는 테스트 케이스를 정의한 후에 고쳐야 한다.   테스트 코드를 먼저 작성하는 것이 더 유용하다는 관점에 대해서는,   테스트 문서는 스펙 문서에 근거할 뿐만 아니라, 부가적인 정보를 제공한다. 클래스나 컴포넌트 자체로만으로는 불확실한 내용이 테스트에서는 명확해진다. (사용처나 목적을 분명히 알고 작성할 것이므로) 사실, 코드를 모두 작성한 후에 별도로 테스트 코드를 작성하는 것이 훨씬 어렵다.   그러므로, 한 메소드씩 작성하되, 코드-테스트코드의 순서가 아니라, 반대인 테스트코드-코드의 순서로 작성해 나가자.   Rod Jonhson은 test 메소드의 이름을 다음과 같이 작성하도록 권고한다.  test&lt;Method to be tested&gt;&lt;Description of test&gt;   예를 들면,  private void testCommaDelimitedListToStringArrayLegalMatch(String[] components); public void testCommaDelimitedListToStringArrayMatchWords(); public void testCommaDelimitedListToStringArraySingleString();  단순히 테스트의 coverage를 높이기 보다는, 테스트의 메소드 이름만으로도 테스트의 성격을 알 수 있도록 작성해 보자. 테스트내에서도 반복되는 테스트의 경우 private로 작성하고, 이를 활용하자.   위의 예는,  commaDelimitedListToStringArray(String s);  를 테스트한 예이다.   그리고, 테스트코드도 꾸준히 버전 관리를 해야 한다.  ","categories": ["development"],
        "tags": ["test","java","junit","coverage"],
        "url": "http://localhost:4000/development/junit-test-coverage/",
        "teaser":null},{
        "title": "Am I lucky or what? 의문문 + or what? - 의문문 강조하기",
        "excerpt":"의문문을 강조하는 여러 가지 방법 중, 다음과 같은 표현을 우리말로도 일상생활에서 많이 쓰고 있다.   “내가 운이 좋은 게 아니면 뭐겠어?” “나 천재 아냐?” “이거 정말 싼 거 아냐?”   등의 뉘앙스를 가진 의문문을  지금까지 tag question (부가 의문문)만 써왔다면,   “의문문 + or what?”을 써서 쉽게 만들어 보자.   즉,  I am lucky, aren’t I?  It is cheap, isnt’ it?   와 같은 문장을   Am I lucky or what? Is it cheap or what?   로도 써 보자.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/am-i-lucky-or-what/",
        "teaser":null},{
        "title": "JUnit에서의 예외 인식",
        "excerpt":"JUnit에서 작성한 어떤 테스트케이스가 Exception을 던지고,  그 Exception이 던져진 것이 맞는 상황임을 검증하고자 한다면,   JUnit3에서는   \tpublic void testDivideByZeroV3() { \t\ttry { \t\t\tint a = 3/0; \t\t} catch(Exception e) { \t\t\tassertSame(e.getClass(), ArithmeticException.class); \t\t} \t}  반면, JUnit4에서는  \t@Test(expected=ArithmeticException.class) \tpublic void testDivideByZeroV4() { \t\tint a = 3/0; \t}  예외 처리만으로도 JUnit4가 JUnit3보다 간략하다는 것을 확인할 수 있다.  ","categories": ["development"],
        "tags": ["exception","junit4"],
        "url": "http://localhost:4000/development/junit-exceptions/",
        "teaser":null},{
        "title": "this + 형용사 vs. that + 형용사",
        "excerpt":"우리말에서와 같이, ‘이렇게(이 정도로) ~ 하다’, ‘그렇(저렇)게(그/저 정도로) ~하다’의 표현을  하고자 한다면,   문장 뒤에 like this, like that을 붙이는 문장을 습관적으로 써왔다.   Is it expensive like this? Was he talkative like that?   대신, 이와 같이 써보자.    Is it this expensive? Was he that talkative?   this나 that 뒤에 형용사가 쓰이면, “이렇게 ~하다”, “저렇게 ~하다”의 뜻을 지닌다.   추가적으로, this + 형용사는 ‘현재’의 의미를, that + 형용사는 ‘과거’의 의미를 나타낸다.  ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/this-that/",
        "teaser":null},{
        "title": "at least ~ vs. ~ at least - at least가 문장 중 위치에 따라 뜻이 달라진다?",
        "excerpt":"at least는 흔히 알기로, 수식하려는 말 앞에 오면 “적어도 ~”을 뜻한다. “at least A” 는 “적어도 A’“라고 쓰는 것이 일반적이다. 그런데, at least가 문장 맨 뒤에 온다면 조금 다른 의미로 쓰인다고 한다.   Could you lend me at least $10?     최소(적어도) 10달러 빌려줄래?   그런데, 다음 문장을 보자.   Could you lend me $10 at least?- 다만 10달러라도 빌려 줄 수 있어?- 10달러라도 좋으니 빌려줄 수 있어?   라는 뜻이 된다.   수식어의 위치에 따라, 다른 뉘앙스를 나타낼 수 있으니 유의하자.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/at-least-vs-at-least-at-least/",
        "teaser":null},{
        "title": "back and forth about something ~에 대해 왔다갔다(갈팡질팡) 하다",
        "excerpt":"“아이폰 살거야? 아니면, 안드로이드 폰 살거야?” 라고 누군가 묻는다면,   I am back and forth about that.나도 그 점이 왔다갔다 해    ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/back-and-forth-about-something/",
        "teaser":null},{
        "title": "[eclipse plug-in] moreunit - 테스트 유닛은 어디에?",
        "excerpt":"moreunit은 작성중인 클래스에 테스트 코드가 작성되어 있는지를 시각적으로 보여주는 플러그인이다. 해당 플러그인에 대한 자세한 설명은   http://moreunit.sourceforge.net/index.html   에서 확인할 수 있다. 설치는, 직접 다운로드하여 플러그인 디렉토리에 풀어주거나,  이클립스 플러그인 설치 메뉴로부터,   http://moreunit.sourceforge.net/update-site/ 를 등록하여 다른 플러그인 설치 과정과 동일하게 설치하면 된다. 본인의 경우, 별다른 기본 설정없이도 작성중인 클래스에 대해 테스트 클래스를 찾아 보여주었는데, 설정이 동작하지 않는다면 properties context menu로부터 세부 설정이 가능하다. 동작시의 화면은 moreunit의 공식사이트에 있는 것과 별 차이가 없다. 직접 테스트해본 화면은 다음과 같다. 우선, 아래 그림은 moreunit을 설치하기 전의 파일 구성이다.    이제 설치를 완료하고 나면, 똑같은 파일리스트에 빨간 동그라미를 표시한 것과 같이 오른쪽 상단에 녹색마크가 표시된다. 이는 해당 클래스에 테스트케이스가 작성되어 있음을 나타낸다.    표시가 생긴 임의의 클래스를 살펴보면, 테스트 케이스가 작성된 메소드에는 메소드의 좌측 라인에 마찬가지로  테스트케이스가 있음을 나타내는 마크가 보인다.    해당 라인에서 마우스 오른쪽 버튼을 클릭하여 context메뉴를 통해 이동하거나, 단축키 ctrl + j를 누른다면, 해당 메소드의 테스트케이스로 직접 이동할 수 있다.      ","categories": ["development"],
        "tags": ["eclipse","plugin","java","unittest","moreunit","tdd"],
        "url": "http://localhost:4000/development/eclipse-plug-in-moreunit/",
        "teaser":null},{
        "title": "[oracle] 사용자 조회/ 비밀번호 변경하기",
        "excerpt":"   어떤 사용자 id가 등록되어 있는지 보려면,     select * from dba_users;           특정 사용자의 비밀번호를 변경하려면,     alter user 사용자명 identified by &lt;span style=\"color:rgb(165,42,42);background-color:yellow;font-weight:bold;\"&gt;비밀번호&lt;/span&gt;;           ","categories": ["system"],
        "tags": [],
        "url": "http://localhost:4000/system/oracle-check-id-password/",
        "teaser":null},{
        "title": "suggest 발음할 때, g를 넣자?",
        "excerpt":" 라디오에서 듣다보니, g가 두 번 나오는데, 한 번을 약하게 발음해주라는 내용을 우연히 듣게 되어 사전을 찾아 보게 되었다.    한글로, \"써제스트\"만 알고 있었는데, \"써ㄱ제스트\"라고 하라는 것이었다. Longman contemporary dictionary에도 두 가지의 발음이 소개된다.    사전에는 가운데 g가 들어간 발음이 US 발음이라고 소개되어 있다. 그러나, 중간 g는 약하게 발음하자. ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/suggest-pronounciation/",
        "teaser":null},{
        "title": "펌) Install and Configure FreeRadius with MySQL",
        "excerpt":"출처)    http://infodotnet.blogspot.com/2008/03/install-and-configure-freeradius-with.html ","categories": ["system"],
        "tags": ["freeradius","mysql"],
        "url": "http://localhost:4000/system/install-and-configure-freeradius-with-mysql/",
        "teaser":null},{
        "title": "windows에서 커맨드로 telnet client 설치하기",
        "excerpt":"pkgmgr /iu:\"TelnetClient\"   ","categories": ["system"],
        "tags": ["windows","telnet"],
        "url": "http://localhost:4000/system/windows-telnet/",
        "teaser":null},{
        "title": "사용하는 기본 캐릭터셋 변경",
        "excerpt":"   파일 : /etc/sysconfig/i18n   한글  LANG=\"ko_KR.UTF-8\"     영어  LANG=\"en_US.UTF-8\" ","categories": ["system"],
        "tags": ["characterset","i18n"],
        "url": "http://localhost:4000/system/i18n-charset/",
        "teaser":null},{
        "title": "TeamCity 6.5 설치하기",
        "excerpt":"주로 Hudson을 썼는데, 이번에는 TeamCity를 설치하고 사용해 보고자 한다. 다운로드 사이트 : http://www.jetbrains.com/teamcity/   2011/10/11 현재 최신 버전은 6.5이며, 만들 수 있는 설정 및 사용자 수에 제약이 있지만, Professional Edition 만으로도 테스트는 충분할 것이다.    플랫폼에 따라 또는 연동 방식에 따라 설치본이 나뉜다. 즉, 윈도우즈 서비스로 실행시킬 수도 있고, core만 설치하여 별도로 실행시킬 수도 있는 것 같다. 나열된 여러가지 플랫폼과 설치 방법중, 내가 택한 옵션은 \"윈도우즈 플랫폼에, 기존의 J2EE 컨테이너에 설치하기\"이다.  그러면, 나의 다운로드는 \"JavaEE Container 탭\"의 다운로드를 선택하면서 시작된다. 사이트로부터 다운로드한 파일명은 \"TeamCity-6.5.4.war\"이다.  Tomcat을 설치한 CATALINA_HOME의 webapps 밑에 TeamCity-6.5.4.war를 복사하는 것만으로도 1차 설치는 완료된다. TeamCity 사이트의 가이드에 안내된 바와 같이, CATALINA_HOME/conf/server.xml의 connector 태그에, ``` useBodyEncodingFoRUIR = \"true\" ```  라고 추가해주고, Tomcat을 띄우면 정상적으로 동작하는 것을 확인할 수 있다.  http://localhost:8080/TeamCity-6.5.4  그 다음부터는 계정 설정하는 화면이 곧바로 뜨기 때문에 설정을 잡아주면 될 것 같다. 다만, 이 방식은 Tomcat을 항상 띄워야 하기 때문에 번거로워서, exe 파일 타입으로 다시 설치를 진행해 보았다. 다운로드한 exe파일을 실행하면, 관리포트를 물어본다.  8111로 입력하고 나면,    와 같이 뜬다. 설치를 서비스형으로 하면, 매우 간단하게 설치가 완료된다. 설치 완료 후, 곧바로 웹 페이지를 띄우라고 하면, 곧바로 지정한 포트로 관리페이지가 뜨는 것을 볼 수 있다.    \"Register a New User Account\"를 클릭하여 사용자 계정을 추가하자.    생성한 계정으로 로그인을 하면, 다음과 같은 화면이 나타난다. 이는 Tomcat에 연동해서 설치한 것과 exe형으로 설치한 것과 (당연히) 동일한 화면이다.           ","categories": ["development"],
        "tags": ["teamcity"],
        "url": "http://localhost:4000/development/teamcity-65/",
        "teaser":null},{
        "title": "maven - eclipse indigo + m2eclipse",
        "excerpt":"2011/6/22에 릴리즈된 Eclipse Indigo의 기능중,     m2eclipse에 대한 지원이 강화되었다는 것   myln을 통해 Hudson을 연동할 수 있다는 것   등의 이유로, Helios에서 Indigo로 업그레이드 해보려고 한다. (그냥 다시 설치)   m2e plugin 설치  최신 m2eclipse 플러그인 이름이 m2e로 변경되었다. Help &gt; Install New Software 에서 m2e를 찾아보자.   Collaboration 밑에 m2e 항목을 발견할 수 있다.      체크를 하고 설치하자.     \"Finish\"를 하면, 프로젝트가 생성된다. 생성된 디렉토리 구조는 다음과 같다.   생성된 pom.xml은 아래와 같다.   기존의 WTP 프로젝트는 등록한 WAS에 곧바로 연결하여 사용할 수 있는데, 현재 maven으로 만든 프로젝트는 WAS에 곧바로 사용할 수가 없다. (불편하다!) 일단, 이렇게 생성한 프로젝트가 정상적으로 실행되는지부터 살펴보고 나중에 환경 설정을 바꿔보자. mvn package를 실행하면, target 디렉토리내 myweb.war이 생성된다.  $CATALINA_HOME/webapps에 myweb.war를 복사하고,  웹 브라우저에서 http://localhost:8080/myweb을 실행하면,   과 같이, Hello World!가 출력된다. 그러나, 매번 이렇게 war 파일을 복사하는 건 너무 불편하다. 분명 더 쉽게 쓰는 방법이 있을 것이다.          ","categories": ["development"],
        "tags": ["eclipse","maven","m2eclipse"],
        "url": "http://localhost:4000/development/maven-eclipse-indigo-m2eclipse/",
        "teaser":null},{
        "title": "Eclipse + Anyframe IDE + m2eclipse 설치하기",
        "excerpt":"Eclipse Indigo를 설치하고, maven 기반 프로젝트를 해보려고 한다. Eclipse Indigo에서 m2eclipse 지원이 좋아졌다고 하여 그대로 Anyframe과 연동해보려 하였으나, 아직 버전이 호환되지 않는 것 같다. Indigo를 설치하고 m2e를 설치하고 나니, Anyframe IDE를 설치할 때 에러가 발생하였다. 그래서, 그냥 Anyframe 사이트에 안내된 대로 m2eclipse를 설치하기로 한다.   1. m2eclipse 사이트 등록 Available Software에 m2eclipse 사이트와 m2eclipse extras를 지정한다.     m2eclipse 사이트 :http://m2eclipse.sonatype.org/sites/m2e   m2eclipse extras 사이트 :http://m2eclipse.sonatype.org/sites/m2e-extras   Anyframe 사이트도 지정한다.     Anyframe 사이트:http://dev.anyframejava.org/update   2. Anyframe IDE 설치 방금 등록한 anyframe 사이트를 선택하면, 아래와 같이 목록이 뜰 것이다. Anyframe IDE를 선택한다.    m2eclipse와 m2eclipse extras를 설치하기 위해, \"Contact all update sites during install to find required software\"를 체크한다. 설치를 마치면, Eclipse를 restart하라고 창이 뜬다. (restart 하자.)  3. Anyframe IDE 정상 설치 여부 확인 Help &gt; About Eclipse를 클릭하면, 다음 창이 뜬다.  Installation Details를 선택한다.    Anyframe IDE가 설치된 것을 확인할 수 있다.  4. Anyframe IDE 환경 설정 Windows &gt; Preferences에서 Anyframe IDE 탭에서 maven 관련 정보를 입력한다.   5. Server 정보 입력 Windows &gt; Preferences &gt; Server를 설정한다.     이것으로 Anyframe 사용을 위한 기본 환경 준비를 마쳤다. File &gt; New &gt; Other를 선택하면, Anyframe프로젝트를 만들 수 있다.   다음에는 Anyframe 프로젝트를 만들어 보자.           ","categories": ["development"],
        "tags": ["anyframe","eclipse","m2eclipse"],
        "url": "http://localhost:4000/development/eclipse-anyframe-ide-m2eclipse-setup/",
        "teaser":null},{
        "title": "Eclipse + Anyframe IDE를 활용한 Anyframe 프로젝트 만들기",
        "excerpt":"지난 번에 Eclipse에 설치한 Anyframe IDE를 활용하여 Anyframe 프로젝트를 만들어 보려고 한다. 1. Anyframe IDE를 활용한 프로젝트 생성    File &gt; New &gt; Other &gt; Anyframe &gt; Project를 선택한다. (Anyframe 항목이 보이지 않는다면, 아마도 이전 단계에서 Anyframe IDE가 제대로 설치되지 않았기 때문일 것이다.)   프로젝트 관련 항목을 입력하자.   DBMS를 hsqldb로 해보자. 만약, 앞서 anyframe 사이트에서 안내하는 예제를 실행했었다면, hsqldb jar 파일이 local repository에 이미 존재할 것이다. 그렇지 않다면, hsqldb jar를 별도로 다운로드하여 Driver Jar Path에 별도로 지정해주어야 한다.  2. 프로젝트를 생성했는데 에러가 난다? 내 경우, 프로젝트를 새로 생성했는데, 프로젝트 view에 엑스박스가 가득했다.   처음에는 심지어 pom.xml에도 에러마크가 떴다. 다음과 같은 내용을 검토하자.  1) m2eclipse 플러그인이 설치안되어 있는 경우   이전 포스트에서, Anyframe IDE를 설치할 때 관련 플러그인을 찾아서 설치하라고 체크박스에 체크했는데도   실제로는 설치가 되어 있지 않았다. 결국, 나중에 별도로 설치를 해야만 했다.  2) 관련 플러그인들이 다운로드가 안되었을 경우   내 경우, pom.xml에 에러 마크가 떠있었을 경우, anyframe core 외 2개의 플러그인을 찾을 수 없다는 에러 메시지가 떴다.   local repository에 직접 들어가서 보니, jar 파일이 있어야 할 곳에 *.jar.lastUpdated라는 확장자 파일들만 생성되어 있었다.    maven context 메뉴에서 Disable Dependency Management + Enable Dependency Management를 번갈아서 해보고,   Update Dependencies도 해보았으나 내 경우는 변화가 없었다.    즉, pom.xml 파일에 표기가 되어 있어도 repository에 다운로드가 정상적으로 되지 않는 상황이었던 것 같다.   Eclipse상에서는 여러 번, 반복해도 repository에 변화가 없는 것 같아서 command line에서 maven client를 직접    실행시켰더니, 다운로드가 된다! (그러나, 어떤 jar는 정상적으로 다운로드 될때까지 반복해서 실행했다.)    결국, 관련 jar를 모두 다운로드 받고나서야 에러 표시가 사라졌다. (당연하지)    3. 프로젝트 실행 WTP에 연동하여, 다음과 같이 등록하자.   그 다음부터는 동일하다. Servers 탭에서 시작시키고, 웹 브라우저에서 동작 여부를 확인하자. 프로젝트가 정상적으로 시작되었다면, 다음과 같은 웹페이지가 보이고, 링크를 클릭하면 영화리스트 조회 등을 할 수 있는 화면이 나타난다.    4. New Project는 New가 아니다?  그런데, Anyframe 프로젝트를 새로 생성했는데 왜 이런 모습이 생기는 것인가?  New Project를 했는데, Sample Project를 보여준다. 개인적인 생각으로는, 현재 Project 메뉴는 Sample Project로 바꾸고, New Project를 별도로 두면 어떨까 싶다.  그냥 틀을 잡아주는 것으로. 물론, 틀을 잡아주려면 그냥 Maven 프로젝트를 새로 만드는 것과 별로 차이는 없을 것 같기도 하다.         ","categories": ["development"],
        "tags": ["anyframe","eclipse","maven"],
        "url": "http://localhost:4000/development/eclipse-anyframe-prj/",
        "teaser":null},{
        "title": "[hibernate] cannot simultaneously fetch multiple bags",
        "excerpt":"hibernate를 쓰는 도중, 이 에러가 나오는 경우가 있다. model 객체 내에서 @OneToMany 표기하고 java.util.List를 사용할 때 일어날 수 있다. 이 경우, fetch 옵션을 LAZY(default)  가 아닌 다른 값으로 설정했는지 확인해 보자. (내 경우, EAGER fetch 옵션을 LAZY로 바꾸어 해결)   자세한 설명 혹은 그 밖의 해결책은, 아래 블로그를 참조하면 더 좋은 정보를 얻을 수 있다.   http://jroller.com/eyallupu/entry/hibernate_exception_simultaneously_fetch_multiple    ","categories": ["development"],
        "tags": ["hibernate"],
        "url": "http://localhost:4000/development/hibernate-cannot-simultaneously-fetch-multiple-bags/",
        "teaser":null},{
        "title": "hudson + maven project 설정하기",
        "excerpt":"Hudson에 maven 프로젝트를 설정해 보고자 한다. 기존에 작성된 maven 프로젝트가 있다고 가정하자.   1. Hudson에 연동할 기존 maven 프로젝트 준비 또는 신규 maven 프로젝트 준비    즉, 이렇게 생겼다고 가정하자. pom.xml이 존재하는 maven 프로젝트 파일이다. src 폴더에는 원본 소스파일과 함께 테스트 코드도 작성해 두었다.   2. Hudson, 새 프로젝트 만들기 다음과 같이 새 프로젝트를 만든다. Maven 연계 프로젝트를 선택한다. (물론, 이를 위해 Hudson - Maven 관련 플러그인이 설치되어 있어야 할 것이다.)     3. 소스 위치 지정하기 CVS, SVN, Git 등을 활용하여, 소스를 가져올 위치를 지정한다.   위치를 지정하고, 필요하면 계정정보 설정까지 마친다.  주기적으로 build 필요성 여부를 검사하고, build 시키기 위해 trigger 옵션을 설정한다.   일단, 여기까지 하고 save하자. 프로젝트가 정상적으로 생성되었다면, 다음과 같이 나타날 것이다.  일단, Build Now를 클릭하고, 정상적으로 설정되었는지 검사하자.  테스트 코드까지 정상적으로 실행이 되었다면, 다음과 같이 나타날 것이다.   다음에는, hudson에서 사용하는 플러그인을 설정해 봐야겠다.         ","categories": ["development"],
        "tags": ["hibernate","hudosn","maven","java"],
        "url": "http://localhost:4000/development/hudson-maven-project-setup/",
        "teaser":null},{
        "title": "war 배포시 root context로 하려면?",
        "excerpt":"        Web Application 배포 $CATALINA_HOME/webapps에 web application 디렉토리를 통쨰로 복사하면 배포가 된다. 또한, war 파일을 복사해도 배포가 된다. (설정이 잘 되어 있다면) tomcat이 war 파일을 압축 풀고, 배포하는 것을 확인할 수 있다.            Context 설정 만약, webapps/test 라는 디렉토리가 있다면, context명은 test이다. 배포를 하기 위해, web application 형태의 test 디렉토리를 통째로 복사했을 수도 있고, test.war 파일을 통해 배포했을 수도 있다.       이 때, context명을 $CATALINA_HOME/conf/server.xml에, 또는 $CATALINA_HOME/conf/context.xml에 지정할 수도 있다. (context 내용으로 docBase는 어느 디렉토리로 하고, path는 어디로 하겠다고 옵션을 준다.) 그러나, 이렇게 tomcat의 설정을 직접 변경하면, 설치하고자 하는 tomcat에 종속적이 된다.  (새로운 곳에 설치하려면, tomcat 설정을 잘 잡아줘야 된다. 그래서, 실수가 발생한다.)      배포시 Context를 함께 배포하자 웹 애플리케이션을 만들 때, webapps/META-INF/context.xml을 작성하자. 그러면, 내가 만든 context.xml이 tomcat에 배포될 때, 함께 설치된다. 내가 만든 context.xml이 $CATALINA_HOME/conf/${hostname}/${context-name}.xml로 복사될 것이다.   만약, META-INF/context.xml에 path=”/test”라고 표기했다면, $CATALINA_HOME/conf/…/test.xml이 생성되었을 것이고, 이로 인해, 사용자는 /test 라는 context를 호출할 수 있게 될 것이다.      root context로 배포하려면? 그런데, 만약 root context로 배포하려면 위의 설정만으로는 부족하다. 즉, META-INF/context.xml을 작성하고, path를 “/”로 표기한다 하더라도 root context로 인식되지 않는다. $CATALINA_HOME/webapps에 가보면, ROOT 디렉토리를 발견할 수 있다. 이것처럼 배포하자.   즉, 내가 배포할 web application의 디렉토리 이름이 ROOT가 되도록 하자. war로 배포하려면, ROOT.war (대문자를 지켜주자)로 배포하면 될 것이다.   결국, $CATALINA_HOME/webapps/META-INF/context.xml내, docBase=”ROOT”로, path=”/”로 적어놓고 배포하면 된다.   정리하면,     META-INF/context.xml을 활용할 것   루트 context로 배포하려면, ROOT.war로 배포할 것   ","categories": ["development"],
        "tags": ["context","deploy","root","tomcat","war"],
        "url": "http://localhost:4000/development/war-root-context/",
        "teaser":null},{
        "title": "한 서버에Tomcat 여러 개 띄우기 (multiple instances)",
        "excerpt":"이런 경우가 발생한다. 한 서버에 포트를 달리해서, tomcat을 여러 개 띄워야 하는 경우가. 개발시 servlet context만 달리해서 검사하곤 했는데, servlet context를 root로 fix해서 개발된 소스를 받아들게 되었다. (절대경로로 root context만 고려해서 작성된 소스코드)   두 개의 사이트를 띄워야 하는데, 둘 다 root context로 동작하게 되어있다. tomcat을 띄우고자 서버 두 개를 쓰는 것은 오버하는 것이고,  한 번에 두 개 혹은 그 이상의 tomcat을 띄워서 배포해 보자.   본 설정은 tomcat 6.x를 기반으로 설정하였다.   1. Tomcat 설치  http://tomcat.apache.org로부터 설치 파일을 다운로드 하였다. 설정의 편의상, apache-tomcat-6.0.35-windows-x64.zip을 다운로드 하였다. 설치를 원하는 디렉토리에 압축을 풀고, 예전에 설치했던 것과 마찬가지로 환경변수를 설치 디렉토리로 잡아주자. (java는 미리 설치했다고 가정한다.)   CATALINA_HOME = c:/dev/tomcat6  압축파일은 다음과 같은 디렉토리들을 포함하고 있다.           bin     conf     lib     logs     temp     webapps     work      2. 복사본 만들기  띄우고자 하는 사이트가 각각 adimweb, userweb이라고 하자. 그러면, tomcat 디렉토리의 내부 디렉토리들을 복사하자. 복사할 대상 디렉토리는           conf     logs     temp     webapps     work      이다.   tomcat6 디렉토리에 adminweb, userweb 디렉토리를 생성하고, 위의 디렉토리들을 각각 adminweb, userweb에 복사한다. (복사 후, 원래 존재하던 tomcat6의 위 디렉토리들은 삭제한다.)   즉, 다음과 같은 구조로 만든다.          tomcat6     bin     lib     adminweb                conf         logs         temp         webapps         work                 userweb                conf         logs         temp         webapps         work                  3. 포트 설정  이제 adminweb과 userweb의 사용 포트를 변경하자. 현재, 각각의 설정파일은 adminweb/conf/server.xml과 userweb/conf/server.xml에 저장되어 있다. 그런데, 두 설정값이 동일한 것이 문제이다.   두 파일중 하나를 골라, 포트 값을 바꿔주자. 안 겹치고, 사용중이지 않은 값으로 바꿔준다. (예 : 파일1에서 8010쓰고 있으면, 8011로 지정하는 식) 총 3-4곳의 port값을 변경해주면 될 것이다. 잘 저장한다.   4. 환경변수 추가 설정  여러 개의 인스턴스를 띄우려면, CATALINA_BASE 라는 환경변수를 선언해줘야 한다. 게다가, 실행의 편의를 돕기 위해, 각 인스턴스의 시작/종료 shell이 있으면 좋을 것이다. (원본도 그렇게 실행/종료 시키니까)   원본 startup,bat, shutdown.bat에서 필요한 부분만 똑 떼어와서 다음과 같이 파일을 만들고, 해당 디렉토리에 복사해 넣는다.  [adminweb/startup.bat] set \"CATALINA_BASE=%CATALINA_HOME%adminweb\" set \"EXECUTABLE=%CATALINA_HOME%bincatalina.bat\" call \"%EXECUTABLE%\" start  [adminweb/shutdown.bat] set \"CATALINA_BASE=%CATALINA_HOME%adminweb\" set \"EXECUTABLE=%CATALINA_HOME%bincatalina.bat\" call \"%EXECUTABLE%\" stop  [userweb/startup.bat] set \"CATALINA_BASE=%CATALINA_HOME%userweb\" set \"EXECUTABLE=%CATALINA_HOME%bincatalina.bat\" call \"%EXECUTABLE%\" start  [userweb/shutdown.bat] set \"CATALINA_BASE=%CATALINA_HOME%userweb\" set \"EXECUTABLE=%CATALINA_HOME%bincatalina.bat\" call \"%EXECUTABLE%\" stop   5. Tomcat 띄우기  adminweb, userweb의 startup을 각각 실행시키자. conf/server.xml에 설정한 포트로 접속을 시도하자. 그러면, 원래 tomcat을 띄운 후 보게 되는 초기화면을 각각 볼 수 있을 것이다.   더 많은 인스턴스를 띄우려면, 위의 작업을 반복하면 된다.   ","categories": ["system","development"],
        "tags": ["tomcat"],
        "url": "http://localhost:4000/system/development/running-tomcat-multiple-instances/",
        "teaser":null},{
        "title": "USB를 사용하여 CentOS 설치하기",
        "excerpt":"작년에 HP ProLiant MicroServer를 제 값 주고 샀다. 내가 산지 3개월 후 되니, 갑자기 여기저기에서 반값 이벤트를 한다.   -_-;   어쨌거나, 살당시, ODD를 별도 구매하지 않았다. 그래서, USB를 사용하여 linux를 설치해야 한다.   1. 설치 이미지 확보 centos.org에 방문하면 download할 수 있는 사이트를 찾을 수 있다. Downloads &gt; Mirrors로부터 원하는 사이트를 선택한다. USB의 용량 한계상, netinstall 버전으로 다운로드 받았다. (그만큼 설치에 오랜 시간이 걸릴 테지만)   2. USB에 linux 파일 복사 다음의 두 방법 중, 하나를 쓰면 될 것 같다. 1) 깨끗하게 포맷한 USB에 미리 다운로드 받은 CentOS netinstall 버전 iso 파일을 압축풀어 놓는다. 2) fedora의 liveusb-creator 도움을 받는다. https://fedorahosted.org/releases/l/i/liveusb-creator/   그 밖에도, USB로 설치를 도와주는 여러 유틸리티들이 있으나, 위의 방법중 하나를 쓰면 무난할 것 같다. 지난 번에는, unetbootin을 써서 CentOs를 설치했으나, 이번에는 fedora의 liveusb-creator를 사용하여 설치해봤는데, 잘 됐다.   3. CMOS 부팅 우선순위 확인 당연히, USB 부팅 우선순위가 높아야 한다.   4. 설치 설치는 똑같이 하되, 네트워크 설치이므로, URL적는 곳에 앞서 확보한 URL을 넣는다.   그 다음은, 계속 설치 진행~~   ","categories": ["system"],
        "tags": ["centOS"],
        "url": "http://localhost:4000/system/usb-centos-install/",
        "teaser":null},{
        "title": "HDD 이상이 있을 때, fsck",
        "excerpt":"지난 여름에 과전류로 정전이 두 차례 되었는데, 아마도 그 때 HDD가 문제가 생긴 것 같았다.   MircroServer에 연결해 놓은 HDD중, 마스터는 그냥 다시 linux를 설치했고, 두 번째 HDD에 이상이 있는지 검사해 보았다.   이 때 사용한 명령어가 fsck unmount된 상태에서 다음과 같이 실행하여 이상이 없음을 확인하였다. 휴우~    [root@localhost ~]# fsck -TV /dev/sdb1 [/sbin/fsck.ext3 (1) -- /dev/sdb1] fsck.ext3 /dev/sdb1&nbsp; e2fsck 1.41.12 (17-May-2010)  data: recovering journal data: clean, 8633/65544192 files, 81666168/262146654 blocks   [root@localhost ~]# fsck -TV /dev/sdb2 [/sbin/fsck.ext3 (1) -- /dev/sdb2] fsck.ext3 /dev/sdb2&nbsp;  e2fsck 1.41.12 (17-May-2010) backup: recovering journal backup: clean, 11/56565760 files, 3601452/226231346 blocks  ","categories": ["system"],
        "tags": ["hdd","fsck"],
        "url": "http://localhost:4000/system/hdd-fsck/",
        "teaser":null},{
        "title": "CentOS에 HP MFP M1005 연결 설정",
        "excerpt":"마이크로서버에 설치한 CentOS에 HP MFP M1005를 연결 성공하였다. 역시 google!   HP에서는 hplip(HP Linux Imaging and Printing)을 지원한다. google에서 검색어로 centos hplip를 입력하였다.   다음과 같은 링크를 얻어냈다. http://hplipopensource.com/hplip-web/install/manual/distros/centos.html    옆의 메뉴에 “Supported Printers”를 선택하여, 입력했다. 프린터 종류까지 선택하고, 조회하면   http://hplipopensource.com/hplip-web/models/laserjet/hp_laserjet_m1005.html    지원된다고 하니 아주 기쁘다. :) 프린터를 연결하자. (USB)   yum list | grep hplip   를 실행하여 hplip 라이브러리들을 확인하자. 그리고, 관련 hplip 라이브러리들을 설치한다. (yum install)   root 계정으로 바꿔서,   hp-setup   을 실행시키자. 그 다음부터는 끝까지 Next만 누르면 정말 끝이다. 맨 마지막에 테스트 페이지 인쇄까지 해보면 성공적으로 설치가 될 것이다.   ","categories": ["system"],
        "tags": ["print","m1005","hp"],
        "url": "http://localhost:4000/system/centos-hp-mfp-m1005-printer/",
        "teaser":null},{
        "title": "linux HDD 추가",
        "excerpt":"마이크로서버를 사고, HDD 2TB를 새로 추가하였다. 다음과 같이 추가 하드를 장착하였다.   1. 시스템 끈 상태에서 HDD 추가 장착 2. 파티션 추가  fdisk /dev/sdb      p : 파티션 정보 조회   n : 파티션 생성   데이터용과, 백업용으로 나누려고 2개로 나누었다. 그래서, /dev/sdb1, /dev/sdb2가 생성되었다. 잘 나뉘었는지, fdisk -l /dev/sdb로 조회하자.   3. ext3으로 포맷  mkfs -t ext3 /dev/sdb1 mkfs -t ext3 /dev/sdb2&amp;nbsp;   4. 마운트 포인트 생성  mkdir -p /mnt/data mkdir -p /mnt/backup&amp;nbsp;   5. 부팅시 자동으로 인식되도록 설정  mount /dev/sdb1 /mnt/data mount /dev/sdb2 /mnt/backup&amp;nbsp;  이라고 하면, 인식이 될 것이나 매번 실행하기는 힘들 것이다.   /etc/fstab에 등록하자.   /dev/sdb1 &amp;nbsp;/data &amp;nbsp;ext3 defaults 1 2 /dev/sdb2 &amp;nbsp;/backup ext3 defaults 1 2&amp;nbsp;   를 추가하자.   ","categories": ["system"],
        "tags": ["fdisk","fstab","hdd","linux"],
        "url": "http://localhost:4000/system/linux-hdd-attachment/",
        "teaser":null},{
        "title": "CentOS samba 설치/설정",
        "excerpt":"1. samba 설치  우선 samba를 설치하자.  yum install samba   2. 사용자 추가  samba를 사용할 사용자를 추가하고, 그 사용자를 samba 사용자로 등록하자.   useradd luran passwd luran   luran이라는 사용자를 추가하였다. 그러면, 이 계정은 다음과 같이 samba 사용자로 추가할 수 있다.   smbpasswd -a luran   3. samba 환경 설정  /etc/samba/smb.conf를 수정하자.   workgroup = WORKGROUP hosts allow = 192.168.1.  으로 설정하였다. 나의 데이터가 있는 곳을 추가해 주었다.   [mydata]  comment = my data  path = /mnt/data/share  public = yes  writable = yes  write list = luran  create mask = 0777  directory mask = 0777   4. selinux 설정  selinux가 사용중으로 설정되어 있으면 samba 접속이 안되므로, 사용 해제하자. 다음 방법 중, 한 가지를 하면 되겠다.   setenforce 0  또는  vi /etc/selinux/config SELINUX= disabled (기본 값은 enforcing)   sestatus -v 를 실행하면, selinux의 상태를 확인할 수 있다.   5. 방화벽 설정  vi /etc/sysconfig/iptables 를 열어서, 다음과 같이 추가한다.  -A INPUT -m state --state NEW -m tcp -p tcp --dport 137:139 -j ACCEPT -A INPUT -m state --state NEW -m udp -p udp --dport 137:139 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 445 -j ACCEPT  추가한 후에 저장하고, iptables를 재시작 시킨다.   6. samba 시작  일회적으로 혹은 지금 당장 실행시키려면,   service smb start  로도 충분할 것이다. 그러나, 부팅마다 자동으로 실행시키고 싶다면, 다음과 같이 하자.   # 현재 samba 설정 상태 조회 $ chkconfig --list smb smb       0:off  1:off  2:off  3:off  4:off  5:off  6:off  # ### 부팅시 자동 시작되게 바꾸기 $ chkconfig smb on $ chkconfig --list smb  smb       0:off  1:off  2:on  3:on  4:on  5:on  6:off&lt;/div&gt;  # runlevel 3, 5에서 시작되게 바꾸기 $ chkconfig --level 35 smb on $ chkconfig --list smb&lt;/div&gt; smb       0:off  1:off  2:off  3:on  4:off  5:on  6:off   7. 윈도우즈에서 접속 테스트  탐색기나, 실행 창을 띄워서 \\접속아이피를 입력하자. 그러면, 인증 창이 뜨고, 앞서 설정한 계정 정보를 요청할 것이다.   8. 한글 디스플레이 설정  global 옵션에 다음과 같이 추가한다.   dos charset = cp949 unix charset = cp949   9. socket option 최적화   사람들이 많이 하는 옵션이다. 이런 건 따라하자.  socket option = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192   10. 기타  내 경우, 위와 같이 설정한 후에 윈도우즈에서 연결했으나, 제한된 리소스라는 에러가 떴으나, service nmb restart 도 해줬더니 동작이 되었다.   ","categories": ["system"],
        "tags": ["centos","samba","smb"],
        "url": "http://localhost:4000/system/centos-samba/",
        "teaser":null},{
        "title": "CentOS 6, KVM에 guest OS를 bridge로 네트워크 설정하려면",
        "excerpt":"동기는 다음과 같다. 일단 호스트는 CentOS로 두고, 웬만하면 깨끗한 상태로 유지하려고 한다. 게스트로 CentOS를 설치하고, 여기에 이런저런 작업 및 테스트를 해보려고 한다. 그런데, VMWare는 상용이고, 리눅스용은 설치하자니 번거로울 것 같기도 하고, 마침 CentOS가 가상화를 지원한다. 그러므로, OS에서 지원하는 가상화를 써보자!   CentOS6에서는 가상화로 KVM을 기본적으로 지원한다. 예전에는 Xen과 KVM을 선택하여 썼다고 하는데, 이제는 KVM만 지원한다. 일단, 무작정 KVM에 guest OS로 CentOS를 설치했더니 NAT로만 동작하는 것을 확인했다.   VMWare를 사용했을 때는 guest OS의 네트워크를 NAT 또는 bridge로 추가/ 변경 설정이 쉬웠는데, KVM Virtualization Manager에서는 GUI상에서 설정할 수는 없는 것 같다.   인터넷에서 검색해 보니, 설정과 관련된 글을 여러 개 발견할 수 있었다.   KVM 설치  CentOS를 설치할 때, 세부 설정을 선택할 수 있다. Virtualization항목을 선택하면, KVM을 설치할 수 있다. 만약, 나중에 별도로 설치하고자 한다면,   yum groupinstall KVM  을 실행시키자.   bridge-utils 설치  만약, 설치가 안되어 있다면 설치하자.  yum install bridge-utils   network-scripts 변경 (host)  호스트가 기본적으로 사용하는 eth0의 설정을 변경하고, 브릿지로 맺어줄 설정을 추가해야 한다. /etc/sysconfig/network-scripts/ifcfg-eth0 파일을 열어서 한 줄을 추가하고, 저장한다.   BRIDGE=br0  이제 br0의 내용을 만들자. /etc/sysconfig/network-scripts/ifcfg-br0 파일을 만들자.   DEVICE=\"br0\" BOOTPROTO=none IPV6INIT=\"yes\" IPV6_AUTOCONF=\"yes\" ONBOOT=\"yes\" TYPE=\"Bridge\" IPADDR=192.168.1.7 GATEWAY=192.168.1.1 DEROUTE=yes IPV4_FAILURE_FATAL=yes DNS1=192.168.1.1 DELAY=0  위 내용은 직접 설정한 값이다. 이와 같이 설정을 마치면, 일단 eth0과 br0은 연결될 것이다.   이제 KVM에서 guest OS를 설치할 때, Advanced Option을 살펴보면, 네트워크 설정 옵션으로 Bridge이 선택 가능할 것이다. (기존에는 NAT만 설정 가능) 이대로 설치하면, guest OS를 bridge network로 설정한 상태가 된다.   NetworkManager, network 설정  chkconfig NetworkManger off chkconfig network on service NetworkManager stop service network restart    network-scripts 변경 (guest)  이제 guest OS를 시작시키자. 마찬가지 방법으로, guest OS의 IP 등 정보를 원하는 대로 수정하자. 이렇게 설정을 하고, guest OS에 ssh 접속하였더니 내가 설정한 IP로 접속이 잘 된다. 그런데, host OS에 ssh 접속을 하였더니 갑자기 잘 안된다!   살펴보니, 나의 경우 위와 같이 설정을 하고 설치를 하지 않고, 먼저 무작정 설치를 한 까닭에 NAT로 설치가 되며 호스트 OS의 네트워크 설정에 변화가 생겨버렸다.   virbr0 삭제  ifconfig으로 조회해 보니, virb0이 발견되었다. 가상 네트워크 리스트를 다음과 같이 확인할 수 있다.   virsh net-list   상태에 default 네트워크라고 이름이 뜨고, 상태는 active라고 표시되어 있다.  이것을 지워주자.  virsh net-destroy default virsh net-undefine default service libvirtd restart   다시 virsh net-list를 실행하여, 정상적으로 작업이 적용되었는지 확인하자. 나의 경우, virbr0 삭제까지 마친 후, 정상적으로 host, guest OS를 원하는 IP로 맞춰서 쓸 수 있게 되었다.   교훈)     KVM에 guest OS를 bridge를 쓰려면 먼저 bridge 설정부터 하자.   bridge 설정을 마친 후, 새로 install 하면 간단하다.  ","categories": ["system"],
        "tags": ["kvm","bridge","network"],
        "url": "http://localhost:4000/system/centos-6-kvm-guest-os-bridge/",
        "teaser":null},{
        "title": "64bit CentOS 6에 lame, mplayer 설치하기",
        "excerpt":"[lame 설치]     http://odiecolon.lastdot.org/el5/noarch/ 사이트로부터 최신 odiecolon-repositories를 다운 받는다. (wget)   다운받은 odiecolon-repositories rpm을 설치한다.   rpm -Uvh odiecolon-repositores*rpm      lame 패키지를 설치한다.     yum install lame           [mplayer 설치]   인터넷 글 중, 내 설정에 딱 맞게 한 번에 맞아 떨어진 것은   https://www.centos.org/modules/newbb/viewtopic.php?topic_id=34461&amp;forum=56    에서 찾을 수 있었다.   rpmforge/ repoforge 등을 설정하지 말고,   다음과 같이 하라는 것이 요지이다.   /etc/yum.respo.d에 atrpms.repo를 만들고 다음과 같이 적어 넣는다.   [atrpms] name=Fedora Core $releasever - $basearch - ATrpms baseurl=http://dl.atrpms.net/el$releasever-$basearch/atrpms/stable gpgkey=http://ATrpms.net/RPM-GPG-KEY.atrpms gpgcheck=1 enabled=1   이제 mplayer를 설치하자  yum install mplayer  ","categories": [],
        "tags": ["centOS","lame","Mplayer"],
        "url": "http://localhost:4000/64bit-centos-lame-mplayer/",
        "teaser":null},{
        "title": "MicroServer + CentOS + CUPS 조합으로 HP MFP 1005를 네트워크 프린터로 쓰기",
        "excerpt":"지난 번에 CentOS에 hplip를 설치하여, CentOS에서 직접 HP MFP1005로 인쇄가 되는 것을 확인하였다. 내 PC의 문제인지는 모르겠으나, HP에서 제공하는 프린터 드라이버를 다운로드 하여 프린터를 직접 연결하였으나 제대로 인식되지 않아 프린트를 하기 어려웠다.    &lt;HP MFP 1005와 궁합이 맞은 우리집 OS들&gt; * Windows XP : 동작 * Windows Vista : 오류 * Windows 7 : 오류 * CentOS 6 : 동작  그래서, 항상 켜 놓는 MicroServer를 프린터 서버로 활용하게 되었다.  1. CUPS(Common Unix Printing System)의 설치 우선 CUPS가 설치되어 있어야 한다.   yum install cups  &lt;/div&gt;  2. CUPS 시작 CUPS가 동작하고 있지 않다면, 실행시킨다.  service cups start  &lt;/div&gt;  3. 로컬에서 CUPS 관리페이지 접속 확인 우선, 서버에서 http://localhost:631로 접속해보자(localhost). 정상적으로 설치되었다면, 다음과 같은 관리 페이지가 나타날 것이다.     4. CUPS 환경 설정 이제, 클라이언트 PC에서도 이 관리 사이트에 원격으로 접속할 수 있도록 설정을 변경하자.&nbsp; /etc/cups/cupsd.conf를 열어보자.  # Restrict access to the server라는 부분을 찾아본다. 외부에서 접속할 시스템의 IP 또는 hostname을 적어주면 되는데, 기본적으로 localhost만 기록되어 있다. Allow 192.168.1.0/24 를 추가하였다.  # Restrict access to the admin pages를 찾아본다.  admin 페이지를 사용할 수 있는 시스템의 IP 또는 hostname을 적어주면 된다. 마찬가지로, Allow 192.168.1.0/24를 추가하였다. 이 옵션을 적어주지 않으면, 위와 같이 관리자 페이지가 뜨는 상태라 하더라도 Administration 탭을 사용할 수 없다. 설정을 변경하였으므로, service cups restart  5. 방화벽 설정 변경 관리자 페이지는 기본적으로 631 포트를 사용한다. 방화벽 룰에 추가하자.  ``` -A INPUT -m state --state NEW -m tcp -p tcp --dport 631 -j ACCEPT -A INPUT -m state --state NEW -m udp -p udp --dport 631 -j ACCEPT ``` 추가하고, service iptables restart   6. 리모트 PC로부터 CUPS 관리페이지 접속 확인 이제 다른 PC에서 관리자 페이지로 접속시도하여, 위와 같은 관리자 페이지가 정상적으로 나타나는지, Administration 탭에 접근 가능한지 확인해 보자.  위와 같이 성공적으로 관리자 페이지를 설치완료했다면,&nbsp;http://서버IP:631로 접속할 수 있을 것이다.  나의 경우는 http://192.168.1.x:631로 접속할 수 있고, Administration 탭내의 Manage Printers를 클릭하면, 설치되어 있는 프린터가 정상적으로 나타났다. 프린터 이름을 클릭하면, URL 부분이 다음과 같은 형태의 이름을 나타낼 것이다.  http://서버IP:631/printers/프린터명  7. [Windwos7/Vista] hostfile 변경&nbsp; 실제 접속도 위 형태의 URL을 활용하여 프린터를 설정하면 된다. 그런데, 서버IP를 그대로 하면 프린터 설정이 잘 안된다는 글도 보았고, 나 역시도 직접 IP를 적었더니 인식이 잘 되지 않았다.&nbsp;  대신, hostfile에 서버에서 사용하는 이름 그대로 등록해준다. (이름이 다르면 인식이 안되는 것 같다.) 서버가 192.168.1.x이고, 이름이 aaa 인 경우(uname -a), hostfile에도 그대로 적어준다. 내 hostfile에 bbb라고 적어줬더니 제대로 안되는 것 같다.  참고 :&nbsp;  http://www.owlfish.com/thoughts/winipp-cups-2003-07-20.html&nbsp;  당연히, http://내가적은hostname:631/printers/프린터명 으로 접속시도하면 원래 나타나는 관리자 화면이 보여야 한다.   8. 클라이언트 - 프린터 드라이버 설치 프린터를 이와 같은 방법으로 설정한 이유는, 앞서 언급했듯이 HP 사이트에 올라와 있는 드라이버를 집에 있는 PC들의 OS(Windows7, Vista)들에 &nbsp;각각 설치했으나 정상적으로 프린터를 쓸 수 없었기 때문이다.  프린터 추가를 선택한다. 네트워크 프린터를 선택한다. 이름으로 공유 프린터 선택하라는 란에, 위의 주소를 입력한다. (http://호스트이름:631/printers/프린터명) 모델은 \"Generic\"으로, 프린터는 \"MS Publisher Imagesetter\"로 설정한다. 테스트 페이지 인쇄를 해본다. 된다 ^^v       ","categories": ["system"],
        "tags": ["hp","cups","centos","printer"],
        "url": "http://localhost:4000/system/microserver-centos-cups-hp-networkprinter-setup/",
        "teaser":null},{
        "title": "CentOS 6 (64)에 BitNami로 RedMine설치",
        "excerpt":"BitNami로 RedMine을 설치하면, Apache, MySQL, Subversion이 함께 설치된다. 이전에 해당 프로그램들이 개별적으로 설치되어 있다면, RedMine을 직접 설치해야 하나, 그렇지 않다면, 편하게 BitNami를 이용하여 설치해볼만도 하다.   1. 필요 라이브러리 설치  gcc, gcc++, zlib, zlib-dev를 미리 설치해두자.   yum install gcc gcc++ zlib zlib-dev  나의 경우, zlib~를 설치하지 않고 진행했다가, 나중에 gem을 실행시킬 때, 런타임에러가 발생하였다. 만일, gem을 실행시킬 때 zlib관련 so 라이브러리가 없다는 에러가 발생한다면, zlib와 zlib-dev가 설치되어 있는지 확인하고, ruby를 다시 설치하자.   2. ruby 설치  다음과 같이 설치하였다.  wget ftp://ftp.ruby-lang.org/pub/ruby/1.8/ruby-1.8.7-p174.tar.gz  압축 풀고, 해당 디렉토리로 이동하여   ./configure make make install   3. rubygems 설치  다음과 같이 설치하였다.  wget http://rubyforge.org/frs/download.php/73883/rubygems-1.4.2.zip   압축 풀고, 해당 디렉토리로 이동하여  ruby setup.rb gem install rubygems-update  4. rails와 rack 설치  내 경우는 rails만 설치해도, rack도 함께 설치되었다.  gem install rails -v=2.3.5   위와 같이 실행하면, 다음과 같이 설치되는 것을 볼 수 있다.      Fetching: rake-0.9.2.2.gem (100%) Fetching: activesupport-2.3.5.gem (100%) Fetching: activerecord-2.3.5.gem (100%) Fetching: rack-1.0.1.gem (100%) Fetching: actionpack-2.3.5.gem (100%) Fetching: actionmailer-2.3.5.gem (100%) Fetching: activeresource-2.3.5.gem (100%) Fetching: rails-2.3.5.gem (100%) Successfully installed rake-0.9.2.2 Successfully installed activesupport-2.3.5 Successfully installed activerecord-2.3.5 Successfully installed rack-1.0.1 Successfully installed actionpack-2.3.5 Successfully installed actionmailer-2.3.5 Successfully installed activeresource-2.3.5 Successfully installed rails-2.3.5 8 gems installed       Installing ri documentation for rake-0.9.2.2… Installing ri documentation for activesupport-2.3.5… Installing ri documentation for activerecord-2.3.5… Installing ri documentation for rack-1.0.1… Installing ri documentation for actionpack-2.3.5… Installing ri documentation for actionmailer-2.3.5… Installing ri documentation for activeresource-2.3.5… Installing ri documentation for rails-2.3.5… Installing RDoc documentation for rake-0.9.2.2… Installing RDoc documentation for activesupport-2.3.5… Installing RDoc documentation for activerecord-2.3.5… Installing RDoc documentation for rack-1.0.1… Installing RDoc documentation for actionpack-2.3.5… Installing RDoc documentation for actionmailer-2.3.5… Installing RDoc documentation for activeresource-2.3.5… Installing RDoc documentation for rails-2.3.5…&gt;    5. RedMine 설치  wget http://bitnami.org/files/stacks/redmine/1.3.2-0/bitnami-redmine-1.3.2-0-linux-x64-installer.bin chmod +x *.bin ./bitnami-redmine-1.3.2-0-linux-x64-installer.bin  위와 같이 실행하면, 몇 가지 옵션을 Y/N로 확인한다. 답하고 나면, 알아서 설치 완료.   6. 설치 확인  설치된 디렉토리를 보면, apache2, git, mysql, perl, php, ruby, sqlite, subversion, mysql 등이 설치되어 있는 것을 확인할 수 있다. 실행은 ctlscript.sh로 가능하며, 실행 파라미터는 start/stop/restart/status이다.   ctlscript.sh start   를 실행하여 띄우고, 방화벽 설정도 마친 후에 웹 브라우저로 접속해보자. BitNami라고 사이트가 뜨면 정상이다. 혹은, http://아이피/redmine 로 접속하면, 곧바로 RedMine으로 접속 가능할 것이다.  ","categories": ["system"],
        "tags": ["bitnami","centos","redmine"],
        "url": "http://localhost:4000/system/centos-6-64-bitnami-redmine/",
        "teaser":null},{
        "title": "Tistory로부터 Wordpress 이전 테스트",
        "excerpt":"예전에 Wordpress 광풍이 불었을 무렵, 잠시 Wordpress 테스트를 했었다. 그리고 다시 시도해 본다. Hostinger로부터 무료 계정을 받고, 무료 도메인도 받았다.   기존 Tistory의 데이터를 일단 이전해보고, Syntax Highlighter 플러그인도 받아서 설치해봤다.   [plain] 플러그인 테스트 중 [/plain]   다시 한 번, 적응해보자. 그리고, 괜찮은 테마 좀 찾아서 설치해 봐야겠다.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/tistory-to-wordpress/",
        "teaser":null},{
        "title": "Google Task를 별도 창으로 설정하기",
        "excerpt":"구글 태스크의 주소는 다음과 같다.   https://mail.google.com/tasks/ig https://mail.google.com/tasks/android https://mail.google.com/tasks/iphone   Chrome에서 위의 주소중 하나를 입력하고, Menu로부터 Tools &gt; Create Application Shortcut을 클릭하여 원하는 곳에 바로가기 버튼을 만들어 두고 쓰자.   ","categories": [],
        "tags": ["tips"],
        "url": "http://localhost:4000/google-task/",
        "teaser":null},{
        "title": "Eclipse의 method 템플릿을 Unsupported Operation Exception을 던지도록 바꾸자",
        "excerpt":"Windows &gt; Preferences &gt; Java &gt; Code Style &gt; Code Templates &gt; Code &gt; Method body부분을 살펴보면 기본값으로   // ${todo} Auto-generated method stub  ${body_statement}  와 같이 지정되어 있다. 이것을 다음과 같이 바꿔서 써보자.   throw new UnsupportedOperationException();   그러면, 나중에 특정 interface를 implement하는 경우와 같이 자동으로 method를 구현시키는 경우, 자동으로 UnsupportedOperationException을 던지는 메소드를 작성하게 된다. 자동으로 구현된 메소드를 그냥 의미없이 null을 리턴하거나 아무 일도 하지 않는 채로 두는 것보다, UnsupportedOperationException을 던지는 채로 두어야 나중에 손보게 될 확률이 높아진다. (꼭 챙기게 됨)   ","categories": ["development"],
        "tags": ["eclipse","java"],
        "url": "http://localhost:4000/development/eclipse-method-unsupported-operation-exception/",
        "teaser":null},{
        "title": "Unknown Entity Exception : @Entity 사용시",
        "excerpt":"Hibernate를 사용하여, @Entity annotation을 사용하여 객체를 만들어 사용할 경우, 유의할 점이 있다.   @Entity를 import할 때,   org.hibernate.annotations.Entity javax.persistence.Entity   의 두 가지 중에, 선택해야할 것은 javax.persistence.Entity이다.   위의 옵션을 선택한 경우, Unknown Entity Exception이 발생한다. Hibernate 4.1 이후 org.hibernate.annotations.Entity 는 deprecated 된다고 한다.   결론) javax.persistence.Entity를 import 하자.   ","categories": ["development"],
        "tags": ["java","hibernate"],
        "url": "http://localhost:4000/development/unknown-entity-exception-entity/",
        "teaser":null},{
        "title": "@Entity 모델 객체는 Serializable을 꼭 구현해야 하나?",
        "excerpt":"“JSR 220: Enterprise JavaBeansTM,Version 3.0 Java Persistence API Version 3.0, Final Release May 2, 2006”에 따르면, 다음과 같이 기술하고 있다.   “If an entity instance is to be passed by value as a detached object (e.g., through a remote interface), the entity class must implement the Serializable interface.”   따라서, 이 객체를 어딘가로 전송하거나 세션에 기록하거나 등등 정말 serialization을 위한 용도가 아니라면, Hibernate상에서는 굳이 Serailizable을 구현하지 않아도 된다는 뜻이지만,   사람들은 흔히 권하기를 그래도 웬만하면 Serializable을 구현하는 것이 좋은 습관일 것이라고 권한다. 실제 임의의 클래스를 작성하여, implements Serializable을 하거나, 하지 않더라도 정상적으로 DB에 persist되는 것을 확인할 수 있다.  ","categories": ["development"],
        "tags": ["hibernate","serializable"],
        "url": "http://localhost:4000/development/entity-serializable/",
        "teaser":null},{
        "title": "현재 파일 경로는 어디?",
        "excerpt":"현재 애플리케이션이 실행되는 위치(경로)를 알기 어려울 때가 있다. 웹 애플리케이션을 작성하면서, 특정 위치에 있으려니 싶어 접근하다보면, 위치가 예상과 달라 configuration을 잡는데 의외로 많은 시간이 걸리곤 한다.      꼼수            임의의 파일을 만든다.       해당 파일의 경로를 살펴본다.           System.out.println((new File(&amp;quot;test.txt&amp;quot;)).getAbsolutePath());      보다 고급스러운 방법   System.getProperty(&amp;quot;user.dir&amp;quot;)   ","categories": ["development"],
        "tags": [],
        "url": "http://localhost:4000/development/current-dir/",
        "teaser":null},{
        "title": "Eclipse내 JUnit static import를 위한 설정",
        "excerpt":"jUnit4 이상을 사용하면서, hamcrest의 도움 없이는 온전한(가독성이 좋은) test case들을 작성하기 어렵다. 그런데, 이 jar들을 매번 import하자니 어렵고, 겨우 import해 놓으면 ctrl + shift + o를 누르는 순간 증발하기 일쑤다. 많은 개발자들도 이러한 점을 힘들어하여, 각 블로그에 여기저기 많이 남겨놓으셨다. 나도 이참에 기록을 남긴다.      static import할 라이브러리 등록하기            Windows &gt; Preferences &gt; Java &gt; Editor &gt; Content Assist &gt; Favorites 메뉴로 간다.       New Type을 클릭하고 다음과 같은 라이브러리를 등록한다.           org.hamcrest.CoreMatchers.* org.hamcrest.Matchers.* org.junit.Assert.*      *로 대체하기 기능 설정            Windows &gt; Preferences &gt; Java &gt; Code Style &gt; Organize Imports 메뉴로 간다.       Number of static imports needed for .* (몇 번 이상 겹치면 *로 처리해줄까?) 의 숫자를 변경한다. (예 : 99 -&gt; 1)           ","categories": ["development"],
        "tags": ["junit","eclipse"],
        "url": "http://localhost:4000/development/eclipse-junit-static-import/",
        "teaser":null},{
        "title": "Spring3.1 + Hibernate3 + jasypt1.8 연동테스트",
        "excerpt":"기존에 Spring 3.1과 Hibernate 3을 사용중이었는데, DB에 저장된 데이터를 암호화해야 할 일이 생겼다.   Hibernate를 사용한 목적에 맞도록, 암호화도 DBMS에 transparent한 솔루션으로 적용하고 싶다. 조사해보니, jasypt라는 라이브러리를 사용하면 이를 비교적 적은 비용으로 구현이 가능한 것 같아 설치 테스트해 보았다.   jasypt.는 (Java Simplified Encryption)이라는 뜻이며, 공식 사이트는 www.jasypt.org 이다. 2013/7/11 현재 최신 버전은 1.9.0이지만, 중앙 maven repository에 올라와 있는 버전으로는 1.8이고, Hibernate 관련 라이브러리가 1.8에는 동봉되어 있으나, 1.9.0에는 별도 관리되고 있는 것 같다. (결국, maven 테스트의 편리성 때문에 그냥 1.8로 테스트를 진행했다.)   1. Maven Dependency 추가 pom.xml에 다음의 내용을 추가하였다.   &lt;!-- Jasypt --&gt; &lt;dependency&gt;     &lt;groupId&gt;org.jasypt&lt;/groupId&gt;     &lt;artifactId&gt;jasypt&lt;/artifactId&gt;     &lt;version&gt;1.8&lt;/version&gt; &lt;/dependency&gt;  2. Spring bean 등록 (application-context.xml에 등록)   &lt;bean id=\"strongEncryptor\" class=\"org.jasypt.encryption.pbe.PooledPBEStringEncryptor\"&gt;     &lt;property name=\"algorithm\"&gt;         &lt;value&gt;PBEWithMD5AndTripleDES&lt;/value&gt;     &lt;/property&gt;     &lt;property name=\"password\"&gt;         &lt;value&gt;cUst0mP@sswOrd&lt;/value&gt; \t&lt;!-- Put whatever you want, it must be unique and strong --&gt;     &lt;/property&gt;     &lt;property name=\"poolSize\"&gt; &lt;value&gt;4&lt;/value&gt; \t&lt;!-- to be optimal, put the number of cores of your processor--&gt;     &lt;/property&gt; &lt;/bean&gt;  &lt;bean id=\"hibernateStringEncryptor\" class=\"org.jasypt.hibernate.encryptor.HibernatePBEStringEncryptor\"&gt;     &lt;property name=\"registeredName\"&gt;         &lt;value&gt;strongHibernateStringEncryptor&lt;/value&gt;     &lt;/property&gt;     &lt;property name=\"encryptor\"&gt;         &lt;ref bean=\"strongEncryptor\"/&gt;     &lt;/property&gt; &lt;/bean&gt;   3. Type 정의 www.jasypt.org에는 hibernate.cfg.xml에 환경 설정을 등록하는 방법과, Spring내에서 annotation으로 등록하는 방법을 가이드하고 있는데, 어차피 Spring에서 annotation을 사용하고 있으므로, 그냥 model 클래스에 선언하여 사용해 보았다.   해당 model(entity) 클래스에 다음과 같이 적어준다.   @TypeDef (name=\"encryptedString\", typeClass=EncryptedStringType.class,  parameters={@Parameter(name=\"encryptorRegisteredName\", value=\"strongHibernateStringEncryptor\")})   4. 코드내 적용 이제 암호화를 원하는 필드에 위의 타입을 annotation으로 적용하자.   @Type(type=\"encryptedString\") public String getName() {     return name; }   별도의 decryption 옵션을 지정하지 않아도, 이제 model 객체는 자동으로 encrypt/decrypt를 수행하며, 해당 model을 사용하는 DAO들은 별도의 수정작업이 없이도 해당 model의 일부 필드를 암호화할 수 있게 되었다.   5.  기타 : JCE 설정 jasypt를 설치/테스트하는 도중, JVM내 JCE가 설치되어 있지 않은 것 같다는 에러 메시지가 떴었다. 오라클 공식 홈페이지에서 JCE 관련 파일을 받아서 설치하자.   Java SE Downloads 메뉴로부터 스크롤하여, Additional Resources부분을 살펴보면, Java Cryptography Extension(JCE) Unlimited Strength jurisdiction Policy Files를 발견할 수 있을 것이다.   이 파일을 다운로드하여, 파일내 들어있는 jar파일들을 $JAVA_HOME/jre/lib/security에 풀어넣자. (기존에 있던 동일 이름의 파일들은 다른 곳에 복사해두자.) 이제 다시 실행시켜보면, 기존에 발생하던 JCE 라이브러리가 없다는 에러가 발생하지 않을 것이다.   6. 기타 : Hibernate 검색 관련 잠시만 생각해보면 당연하겠지만, 기존에 Hibernate의 criteria등을 써서 검색에 사용하던 필드가 암호화 대상 필드가 되었다면? 검색이 되지 않는다. 기본적으로 random salt를 사용하기 때문에 같은 값이라 하더라도, 다른 암호화 값으로 변화할 것이다. 동일한 ‘test’라는 문자열을 암호화했으나, salt가 달라서  다음과 같이 다르게 암호화 문자열이 생성된다.   Jq4AjAaYAjgM7oTJHySLZw== bjMPhCbaL3fxgg+3KquOoA==   Random이 아닌 Fixed Salt Generator를 사용한다면? 같은 암호 문자열은 얻을 수 있을지 모르지만, 덜 안전한 데이터 관리방안이 될 것이다.   게다가, == 연산 이외의 &gt;,&lt; 등의 연산이나 like 와 같은 연산은 사용할 수 없게 될테니 주의하자. (= 설계부터 잘 하자.)  ","categories": ["development"],
        "tags": ["hibernate","spring","jasypt"],
        "url": "http://localhost:4000/development/spring3-1-hibernate3-jasypt1-8/",
        "teaser":null},{
        "title": "jasypt의 type 선언 : annotation에서 xml로 변경해보기",
        "excerpt":"annotation 방식의 특성상, 정밀한 설정에는 용이하나 jasypt를 적용하면서 사용할 model 클래스에 매번 적어줘야 한다면, 역시 “불편”할 것이다. 그럴 때는, 역시 xml의 선언이 훨씬 편리할 것이다.      annotation 방식     @TypeDef (name=\"encryptedString\", typeClass=EncryptedStringType.class,  parameters={@Parameter(name=\"encryptorRegisteredName\", value=\"strongHibernateStringEncryptor\")})           XML 방식 XML로 객체를 만들지 않고, annotation을 사용하여 만든 상황에서 hbm.xml을 만들어 사용하는 식으로 연결을 해보았다. hibernate-configuration과 hibernate-mapping은 다른 dtd를 사용하기에 별도의 파일로 만들었다.   &lt;?xml version=\"1.0\"?&gt; &lt;!DOCTYPE hibernate-mapping PUBLIC \"-//Hibernate/Hibernate Mapping DTD 3.0//EN\"  \"http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd\"&gt;  &lt;hibernate-mapping package=\"com.luran.jasypt_test\"&gt;  &lt;typedef name=\"encryptedString\" class=\"org.jasypt.hibernate.type.EncryptedStringType\"&gt;  &lt;param name=\"algorithm\"&gt;PBEWithMD5AndTripleDES&lt;/param&gt;  &lt;param name=\"password\"&gt;jasypt&lt;/param&gt;  &lt;param name=\"keyObtentionIterations\"&gt;1000&lt;/param&gt;  &lt;/typedef&gt; &lt;/hibernate-mapping&gt;   현재 jasypt.org에 안내된 가이드는 1.9 기준이기 때문에, 해당 클래스가  org.jasypt.hibernate4.type.EncryptedStringType  로 되어있다.  본 테스트에서 사용한 버전은 1.8이기에 위와 같이 변경해줘야 제대로 인식이 된다. (namespace가 변경된 것 같음)   이제 기존에 작성한 hibernate.cfg.xml에서 이 mapping.hbm.xml을 읽어들이도록 변경해주자. hibernate-configuration.dtd에 보면, mapping attribute가 있다. 이 attribute에 방금 작성한 파일을 지정한다.   &lt;hibernate-configuration&gt;  &lt;session-factory&gt;  ...  &lt;mapping resource=\"META-INF/mapping.hbm.xml\"/&gt;  ...  &lt;/session-factory&gt; &lt;/hibernate-configuration&gt;   앞서 작성한 annotation 방식 typedef를 주석처리하고, 이제 다시 실행시켜 보자. 동일하게 동작하는 것을 확인할 수 있다.   ","categories": ["development"],
        "tags": ["hibernate","java","jasypt"],
        "url": "http://localhost:4000/development/jasypt-type-annotation-xml/",
        "teaser":null},{
        "title": "jasypt 1.9용 pom.xml 설정",
        "excerpt":"사실 잘 찾아보니(google), jasypt의 pom.xml 의존성 파일도 이미 mvn repository내 존재했다.    &lt;!-- Jasypt --&gt;  &lt;dependency&gt; \t&lt;groupId&gt;org.jasypt&lt;/groupId&gt; \t&lt;artifactId&gt;jasypt&lt;/artifactId&gt; \t&lt;version&gt;1.9.0&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt; \t&lt;groupId&gt;org.jasypt&lt;/groupId&gt; \t&lt;artifactId&gt;jasypt-hibernate3&lt;/artifactId&gt; \t&lt;version&gt;1.9.0&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt; \t&lt;groupId&gt;org.jasypt&lt;/groupId&gt; \t&lt;artifactId&gt;jasypt-spring3&lt;/artifactId&gt; \t&lt;version&gt;1.9.0&lt;/version&gt;  &lt;/dependency&gt;   ","categories": ["development"],
        "tags": ["jasypt"],
        "url": "http://localhost:4000/development/jasypt-1-9-pom-xml/",
        "teaser":null},{
        "title": "jasypt + BouncyCastle AES 설정",
        "excerpt":"Sun의 JCE에서도 AES를 지원하지만, Password Based Encryption방식으로 보다 많은 종류의 암호화 알고리즘을 지원하는 BouncyCastle 라이브러리를 설치하고, jasypt에서도 BouncyCastle을 사용한 AES 암/복호화를 사용하도록 설정해 보았다.   친근한 이름(어린 아이들이 좋아할)과는 다르게, 보안라이브러리이다.      BouncyCastle 다운로드 BouncyCastle의 공식 사이트는 www.bouncycastle.org이다. 본인이 사용하는 jvm 버전에 맞는 것으로 다운로드한다. 내 경우, jdk 1.6을 사용하고 있어 bcprov-jdk16-146.jar를 받았다. maven을 사용하고 있다면, pom.xml에 아래와 같이 추가하여 다운로드 할 수도 있다.    &lt;dependency&gt;  &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt;  &lt;artifactId&gt;bcprov-jdk16&lt;/artifactId&gt;  &lt;version&gt;1.46&lt;/version&gt;  &lt;/dependency&gt;           BouncyCastle 설치 jar 파일이므로 임의의 디렉토리에 복사 후, classpath에 추가하거나, $JAVA_HOME/jre/lib/ext에 복사하여 자동으로 classpath가 인식하도록 한다. maven을 사용한다면, 위의 dependency 설정으로도 충분할 것이다. 라이브러리 설치 자체는 이것으로 끝났으나, 추가 환경설정이 필요하다.            BouncyCastle을 사용하기 위한 환경설정 BouncyCastle을 JCE Provider로 인식시키기 위해 추가 설정이 필요하다. $JAVA_HOME/jre/lib/security/java.security 파일에 BouncyCastle을 등록하자.            jasypt에 BouncyCastle AES 사용설정 (1) application-context.xml에 hibernate.cfg.xml 설정 ```xml                (2) hibernate.cfg.xml에 mapping.hbm.xml 지정 (type 선언 내용 포함) ```xml  &lt;hibernate-configuration&gt;  &lt;session-factory&gt;  ...  &lt;mapping resource=\"META-INF/mapping.hbm.xml\"/&gt;  ...  &lt;/session-factory&gt;  &lt;/hibernate-configuration&gt;    (3) mapping.hbm.xml내 타입 선언부 중, 알고리즘 변경 (PBEWITHSHA256AND128BITAES-CBC-BC)   &lt;hibernate-mapping package=\"com.luran.jasypt_test\"&gt;  &lt;typedef name=\"encryptedString\" class=\"org.jasypt.hibernate3.type.EncryptedStringType\"&gt;  &lt;param name=\"algorithm\"&gt;PBEWITHSHA256AND128BITAES-CBC-BC&lt;/param&gt;   &lt;!-- &lt;param name=\"providerName\"&gt;BC&lt;/param&gt; --&gt;  &lt;param name=\"password\"&gt;jasypt&lt;/param&gt;  &lt;param name=\"keyObtentionIterations\"&gt;1000&lt;/param&gt;   &lt;!-- &lt;param name=\"stringOutputType\"&gt;hexadecimal&lt;/param&gt; --&gt;  &lt;/typedef&gt;  &lt;/hibernate-mapping&gt;  ","categories": ["development"],
        "tags": ["jasypt","java","bouncycastle"],
        "url": "http://localhost:4000/development/jasypt-bouncycastle-aes/",
        "teaser":null},{
        "title": "Windows에서 grails proxy 설정하기",
        "excerpt":"Windows 환경의 grails에서 proxy설정과 관련된 명령어를 수행하면 제대로 proxy 설정이 되지 않는다. 원래 다음과 같이 설정하라고 가이드가 되어 있다.   grails add-proxy client --host=someproxyserver --port= 8080 grails set-proxy client   c:users사용자명.grailsProxySettings.groovy 파일의 내용을 열어보자. 제대로 설정되었다면, http.proxyHost와 http.proxyPort등의 값들이 정상적으로 반영이 되어 있어야 한다.   해결 방법은 둘 중의 하나! 일단 생성된 ProxySettings.groovy 파일을 편집하여 본인이 원하는 proxy 설정값으로 변경해 넣거나, 위의 명령어에 따옴표를 활용하면 된다.   grails add-proxy client &amp;quot;--host=someproxyserver&amp;quot; &amp;quot;--port= 8080&amp;quot; grails set-proxy client  ","categories": ["development"],
        "tags": ["grails","development"],
        "url": "http://localhost:4000/development/windows-grails-proxy/",
        "teaser":null},{
        "title": "Toad + Oracle Thin Client",
        "excerpt":"우선, Oracle 사이트에서 oracle thin client를 다운로드 해야 한다. 이 때, 본인이 실제 사용하는 OS의 bit가 아닌, Toad의 bit와 동일한 버전의 thin client가 필요하다. 즉, 64bit Windows라도 32bit Toad 라면, 32bit Oracle thin client 설치 필요.   이후, 환경 설정은 Oracle thin client의 환경 변수 설정이다.      regedit를 통해 레지스트리를 변경하거나,   시스템 환경변수로 선언하거나,   batch 파일내 선언하여 toad가 실행될 때 참조하도록 하는 방법 등이 있다.   설정의 편의상 3 방법이 가장 편할 것이다.   1. 레지스트리 변경   regedit를 실행하고, 아래 내용을 등록한다.  HKEY_LOCAL_MACHINESOFTWAREORACLE내 ORACLE_HOME=설치위치 NLS_LANG=KOREAN_KOREA.KO16MSWIN949   2. 시스템 환경변수로 선언   PATH에 Oracle thin client 경로 추가 ORACLE_HOME=설치위치 TNS_ADMIN=설치위치 NLS_LANG=KOREAN_KOREA.KO16MSWIN949   3. batch 파일로 등록   set PATH=설치위치;%PATH% set ORACLE_HOME=설치위치 set TNS_ADMIN=설치위치 set NSL_LANG=KOREAN_KOREA.KO16MSWIN949 start &amp;quot;toad&amp;quot; &amp;quot;toad설치위치&amp;quot;   ","categories": [],
        "tags": ["tips"],
        "url": "http://localhost:4000/toad-oracle-thin-client/",
        "teaser":null},{
        "title": "jasypt 사용중 org.jasypt.exceptions.EncryptionOperationNotPossibleException가 발생한다면?",
        "excerpt":"먼저, encrypt에 사용한 설정이 decrypt에 사용한 설정과 동일한지 확인하자. 그리고, DB의 필드가 충분히 긴지 확인하자.   나의 경우는, 위의 두 경우와 다른 경우였다. output type을 default인 Base64에서 Hexadecimal로 변경하니, 위의 문제가 해결되었다. 이에 대한, jasypt의 공식 FAQ는 다음과 같다.    I keep on receiving EncryptionOperationNotPossibleException exceptions when trying to decrypt my data.  EncryptionOperationNotPossibleException is a very general exception which jasypt raises whenever there is a problem with encryption or decryption operations. It does not provide any further information to prevent the encryption infrastructure from showing too much information about what is going on (we wouldn't want an attacker to get any algorithm-specific errors...)  When you get that error while decrypting, most of the times it will simply mean that the encrypted string you input was not adequate for the algorithm/password/keyObtentionIterations configuration you provided. Check that your encryptor is configured in exactly the same way as the one with which you originally encrypted the data.  Also, if you are storing your encrypted data into a database, check that the table columns that you use to store it are big enough to host the encrypted data (which is always bigger than the original data). If you are transmitting your encrypted data via HTTP, check that you are not having problems with the transmission of BASE64-encoded data as URL parameters (BASE64 uses characters which are forbidden in URL parameters, like \"=\"). For these uses, try using hexadecimal output.  ","categories": ["development"],
        "tags": ["hibernate","java","jasypt"],
        "url": "http://localhost:4000/development/jasypt-org-jasypt-exceptions-encryptionoperationnotpossibleexception/",
        "teaser":null},{
        "title": "Tomcat SSL인증서 설정시 APR 관련 에러 발생시",
        "excerpt":"Tomcat에 SSL인증서를 설정하다 다음과 같은 에러가 날 경우에 대한 조치 방법이다.    org.apache.coyote.AbstractProtocolHandler init 심각: Failed to initialize end point associated with ProtocolHandler [\"http-apr-8444\"] java.lang.Exception: Connector attribute SSLCertificateFile must be defined when using SSL with APR at org.apache.tomcat.util.net.AprEndpoint.bind(AprEndpoint.java:468) at org.apache.tomcat.util.net.AbstractEndpoint.init(AbstractEndpoint.java:482) at org.apache.coyote.AbstractProtocolHandler.init(AbstractProtocolHandler.java:354) at org.apache.catalina.connector.Connector.initInternal(Connector.java:910) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:101) at org.apache.catalina.core.StandardService.initInternal(StandardService.java:559) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:101) at org.apache.catalina.core.StandardServer.initInternal(StandardServer.java:781) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:101) at org.apache.catalina.startup.Catalina.load(Catalina.java:572) at org.apache.catalina.startup.Catalina.load(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.catalina.startup.Bootstrap.load(Bootstrap.java:262) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:430) 2013. 7. 30 오전 11:20:02 org.apache.catalina.core.StandardService initInternal 심각: Failed to initialize connector [Connector[HTTP/1.1-8444]] org.apache.catalina.LifecycleException: Protocol handler initialization failed at org.apache.catalina.connector.Connector.initInternal(Connector.java:912) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:101) at org.apache.catalina.core.StandardService.initInternal(StandardService.java:559) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:101) at org.apache.catalina.core.StandardServer.initInternal(StandardServer.java:781) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:101) at org.apache.catalina.startup.Catalina.load(Catalina.java:572) at org.apache.catalina.startup.Catalina.load(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.catalina.startup.Bootstrap.load(Bootstrap.java:262) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:430) Caused by: java.lang.Exception: Connector attribute SSLCertificateFile must be defined when using SSL with APR at org.apache.tomcat.util.net.AprEndpoint.bind(AprEndpoint.java:468) at org.apache.tomcat.util.net.AbstractEndpoint.init(AbstractEndpoint.java:482) at org.apache.coyote.AbstractProtocolHandler.init(AbstractProtocolHandler.java:354) at org.apache.catalina.connector.Connector.initInternal(Connector.java:910) ... 13 more   조치 방법은 간단하다. $CATALINA_HOME/conf/server.xml 중 다음의 설정을 찾아 주석처리하자.    &lt;Listener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\"&gt;    ","categories": ["system"],
        "tags": ["tomcat","certificate","ssl","apr"],
        "url": "http://localhost:4000/system/tomcat-ssl-certificate-ssl-apr-error-handling/",
        "teaser":null},{
        "title": "Tomcat Web Application 모니터링 툴 - javamelody 설정 방법",
        "excerpt":"Tomcat Web Application을 모니터링하거나, profiiling하기 위한 괜찮은 툴을 소개한다. Javamelody(https://code.google.com/p/javamelody)를 설정하는 방법은 무척 간단하다.   1. Jar 설정 WEB-INF/lib 내 javamedloy.zip내의 javamelody.jar와 jrobin-x.jar를 복사한다. 내 경우는 maven 을 사용하므로, 기존의 pom.xml에 다음과 같이 dependency를 추가하였다.    &lt;dependency&gt;  &lt;groupId&gt;net.bull.javamelody&lt;/groupId&gt;  &lt;artifactId&gt;javamelody-core&lt;/artifactId&gt;  &lt;version&gt;1.43.0&lt;/version&gt;  &lt;/dependency&gt;   2. web.xml 설정 기존에 실행하던 웹 애플리케이션을 모니터링/분석해야 하므로, web.xml의 내용을 편집한다. 보통 web.xml에서는 선언된 순서로 우선순위를 지니므로, 다른 서블릿 선언보다 위쪽에 다음의 설정을 추가로 기록한다.    &lt;filter&gt;  &lt;filter-name&gt;monitoring&lt;/filter-name&gt;  &lt;filter-class&gt;net.bull.javamelody.MonitoringFilter&lt;/filter-class&gt;  &lt;/filter&gt;  &lt;filter-mapping&gt;  &lt;filter-name&gt;monitoring&lt;/filter-name&gt;  &lt;url-pattern&gt;/*&lt;/url-pattern&gt;  &lt;/filter-mapping&gt;  &lt;listener&gt;  &lt;listener-class&gt;net.bull.javamelody.SessionListener&lt;/listener-class&gt;  &lt;/listener&gt;   3. 동작 확인 기본 설정은 이것으로 일단 끝! 이제 web application을 실행시키자.    http://&lt;host&gt;/&lt;context&gt;/monitoring  과 같이 실행시키면, 기존 사이트에 대한 모니터링 페이지가 정상 적으로 보일 것이다. 대략 다음과 같은 화면이 뜬다.   http://javamelody.googlecode.com/svn/trunk/javamelody-core/src/site/resources/screenshots/graphs.png   4. 보안설정(ID/Password 기반) role 기반으로 간략하게 보안 설정을 해보자. web.xml에 다음의 내용을 추가한다.    &lt;login-config&gt;  &lt;auth-method&gt;BASIC&lt;/auth-method&gt;  &lt;realm-name&gt;Monitoring&lt;/realm-name&gt;  &lt;/login-config&gt;  &lt;security-role&gt;  &lt;role-name&gt;monitoring&lt;/role-name&gt;  &lt;/security-role&gt;  &lt;security-constraint&gt;  &lt;web-resource-collection&gt;  &lt;web-resource-name&gt;Monitoring&lt;/web-resource-name&gt;  &lt;url-pattern&gt;/monitoring&lt;/url-pattern&gt;  &lt;/web-resource-collection&gt;  &lt;auth-constraint&gt;  &lt;role-name&gt;monitoring&lt;/role-name&gt;  &lt;/auth-constraint&gt;  &lt;/security-constraint&gt;   만약, 이대로 tomcat을 restart하고, /monitoring으로 접근한다면, HTTP BASIC Authentication으로 인증 요청을 할 것이다.   5. tomcat-user 추가 위의 인증창에 접근할 수 있는 사용자를 추가하자. tomcat-users.xml을 찾아 다음과 같이 사용자 정보를 추가한다.    &lt;tomcat-users&gt;  ..  &lt;role rolename=\"monitoring\"/&gt;  &lt;user username=\"monitoring\" password=\"monitoring\" roles=\"monitoring\" /&gt;  ..  &lt;/tomcat-users&gt;   이제 Tomcat을 재시작하고, BASIC authentication popup window가 뜨면, 위의 정보로 인증하고, 처음에 본 모니터링 페이지가 정상적으로 뜨는지 확인한다.  ","categories": ["development"],
        "tags": ["monitoring","javamelody","tomcat"],
        "url": "http://localhost:4000/development/tomcat-web-application-javamelody/",
        "teaser":null},{
        "title": "c3p0 timeout과 idle_test_period의 설정",
        "excerpt":"hibernate를 사용하는 도중, DB connection이 끊긴다면? DB connection이 계속 유지되도록 하려면, 가장 간단한 방법은 주기적으로 사용해야 할 것이다. dummy query를 실행시켜서 connection 자체를 유지시키면 되는데, connection pool library인 c3p0에서는 이를 옵션으로 설정할 수 있도록 가이드한다.   hibernate에서는 이 속성들을 그대로 선언하여 connection을 유지시킬 수 있다. 바로 c3p0.timeout과 c3p0.idle_test_period 값이다.   jboss에서는 다음과 같이 안내한다.    idleTestPeriod Must be set in hibernate.cfg.xml (or hibernate.properties), Hibernate default: 0 If this is a number greater than 0, c3p0 will test all idle, pooled but unchecked-out connections, every this number of seconds. timeout Must be set in hibernate.cfg.xml (or hibernate.properties), Hibernate default: 0 The seconds a Connection can remain pooled but unused before being discarded. Zero means idle connections never expire   사실, 개발자들이 주고 받은 Q&amp;A가 더 유용한데, StackOverFlow에 다음과 같은 내용들이 공유되어 있었다.     The database server may close a connection on its side after a certain amount of time - causing some error in your application, because it'll attempt to send a query on a connection which is no longer available on the server side.  In order to avoid this you can let the pool periodically check a connection (Think of a ping) for it's validity. This is what idle_test_period is for. timeout is the timespan after which the pool will remove a connection from the pool, because the connection wasn't checked out (used) for a while and the pool contains more connections than c3pO.min_size.  http://stackoverflow.com/questions/10175957/the-use-of-c3p0-idle-test-period    사실 좀더 끌리는 설명은 다음과 같다.    Actually this is probably too late, but the problem is quite simple: hibernate.c3p0.idle_test_periods must not be higher than hibernate.c3p0.timeout or connections closed by the database will not be properly detected.  Moreover, the deadlock detection warnings look like some part of your code is not properly returning the connections to the pool (i.e. session.close()) The MysqlIO exceptions occur when your application idles and MySQL closes the connection on the server. Now if C3P0 does not properly check whether a connection is still actually connected you get the EOFExceptions.  I hope this might be helpful.  http://stackoverflow.com/questions/475893/what-are-the-required-c3p0-settings-for-hibernate-in-order-to-avoid-deadlocks   “끊기기 전에 주기적으로 connection 유지 여부를 확인할 것.”   ","categories": ["development"],
        "tags": ["hibernate","connnection"],
        "url": "http://localhost:4000/development/c3p0-timeout-idle_test_period/",
        "teaser":null},{
        "title": "VirtualBox Linux- 호스트 Windows간 폴더 공유 설정하기",
        "excerpt":"VirtualBox에 설치한 CentOS instance와 내 PC의 OS인 Windows간 폴더 공유를 위한 설정 도중, 발생하는 일에 대해 메모를 남겨둔다. 일단, VirtualBox상 CentOS이미지로부터 설정 &gt; 공유폴더 메뉴를 활성화 시키면, 어떤 폴더를 공유할 것인지 설정할 수 있다.   예)   폴더경로 d:/vbox/shared   폴더이름 shared   그 후, Linux로 들어가면 끝! ..   이면, 정말 좋겠다.   shared라는 이름으로 공유하기로 했다만, linux상에서 저 녀석을 알 수 있도록 해줘야 할 것이다. linux에 들어가서, 다음과 같이 입력해보자.   cd /mnt mkdir shared mount -t vboxsf shared /mnt/shared   아마, 별다른 설정을 예전에 하지 않았다면,   /sbin/mount.vboxsf: mounting failed with the error: No such device   와 같은 에러가 발생할 것이다. 그래서, 저 device를 사용할 수 있도록 VirtualBox 화면 상단의 메뉴탭으로부터 “장치 &gt; 게스트확장 설치”를 찾아 설치를 시작한다. 이전에 별도 설정을 하지 않았다면 또다시 다음과 같은 에러가 발생할 수 있다.   Building the main Guest Additions module [FAILED] (Look at /var/log/vboxadd-install.log to find out what went wrong)   그래서, 저 로그 파일을 찾아 열어보니, 다음과 같은 내용이 있었다.  /tmp/vbox.0/Makefile.include.header:97: *** Error : unable to find the source of your current Linux kernel. Specify KERN_DIR=&amp;amp;lt;directory&amp;amp;gt; and run Make again. Stop.   일단, kernel을 업데이트하자.   yum update kernel* reboot   이제 패키지를 설치하자.  yum install gcc kernel-devel kernel-headers dkms make bzip2   이제 다시, 게스트 확장 설치를 시도하자. 바탕화면에 존재할 이미지를 클릭하여 곧바로 실행시키거나,   mkdir /media/VirtualBoxGuestAdditions mount -r /dev/cdrom /media/VirtualBoxGuestAdditions   와 같이 마운트한 후, 해당 디렉토로 이동하여 실행시키자. 그러면, 이전에 실패한 부분이 정상적으로 패스되는 것을 알 수 있을 것이다. 이제 다시 마운트시켜보자.   mount -t vboxsf shared /mnt/shared   앞서 발생한 device를 발견할 수 없다는 에러 대신, 원래 공유하고자 했던 내용이 정상적으로 리스트로 나타날 것이다. 재부팅 후, 다시확인해보자. 그런데, /mnt/shared에 가보면, 내용이 보이지 않는다. 즉, 다시 마운트 해줘야 된다. 번거롭게. 한편, /media 디렉토리에 가보면, sf_로 시작하는 디렉토리가 있을 것이다.   위의 설정대로라면, sf_shared라는 디렉토리. 결국, 제일 먼저 설정한 VirtualBox에서 항상 마운트하기 옵션을 활성화 시켰을 때, 버추얼박스에서 sf_라는 접두어를 붙여 자동으로 마운트해준다.   결국, /media/sf_내가부여한이름으로 액세스할 수 있게 된다. 만약, 앞서 테스트한 바와 같이 /mnt/shared로 계속 액세스 하고자 한다면, /etc/profile에 mount 명령을 적어주자.   /mnt/profile에   mount -t vboxsf shared /mnt/shared  그러면, /media/sf_shared로도, /mnt/shared로도 공유폴더를 액세스할 수 있을 것이다.   ","categories": ["system"],
        "tags": ["virtualbox","host"],
        "url": "http://localhost:4000/system/virtualbox-linux-directory-share-windows/",
        "teaser":null},{
        "title": "yum lock 해제하기",
        "excerpt":"만약, 비정상적인 종료 등으로 인해 yum이 lock이 걸렸고, 기다려도 풀리지 않는다면, 다음과 같이 강제로 lock을 해제시킬 수 있다.   cat /var/run/yum.pid   을 실행시켜서 확인 후,   불필요하다면 해당파일을 지워준다.   ","categories": ["linux"],
        "tags": ["tips"],
        "url": "http://localhost:4000/linux/yum_unlock/",
        "teaser":null},{
        "title": "PDT 시간을 local time으로?",
        "excerpt":"거의 5-6년만에 python으로 한 번 변환하는 로직을 짜보게 되었다. google님 감사합니다. PDT 시각을 현재 시각으로 변환해 보고자 한다.   약 16시간의 차이가 있기 때문에, 내 시각에서 PDT를 구하려면 -16시간을, 역으로는 +16시간을 연산해주면 되는 아주 간단한 요구사항이다.   본 포스트에서는, now - 16을 하는대신, (이것은 구글에서 pdt now 라는 검색어로 검색해봐도 금방 나오는 값이다.) PDT 시간을 입력하면 현지 시각으로 변환하는 로직을 작성해 보고자 한다.   from datetime import datetime, timedelta  now = datetime.now() hours_16 = timedelta(hours = 16)   user_input = raw_input('Enter PDT in yyyymmddHHMMSS format : ') pdt_time = datetime.strptime(user_input, &amp;quot;%Y%m%d%H%M%S&amp;quot;) converted_time = pdt_time + hours_16  print converted_time    ","categories": ["python"],
        "tags": [],
        "url": "http://localhost:4000/python/pdt_to_localtime/",
        "teaser":null},{
        "title": "pst, pdt time 변환 업데이트",
        "excerpt":"역시 요구사항은 변화한다.   pst, pdt를 옵션에 따라 변환해 볼 수 있도록 다시 한 번 변경해보았다.   #!/usr/bin/python   import optparse from datetime import datetime, timedelta  def main():         p = optparse.OptionParser()         p.add_option('--type', '-t', default=&amp;amp;quot;pdt&amp;amp;quot;)         p.add_option('--time', '-i')         options, arguments = p.parse_args()          # check time type whether it is pst or pdt         if options.type in ['pst', 'PST'] :                 hours_gap = timedelta(hours = 17)         else :                 hours_gap = timedelta(hours = 16)          # get time value from user         if options.time == None :                 time = raw_input('Enter %s time in yyyymmddHHMMSS : ' % options.type)         else :                 time = options.time          # get converted time         pxt_time = datetime.strptime(time, &amp;amp;quot;%Y%m%d%H%M%S&amp;amp;quot;)         converted_time = pxt_time + hours_gap          print 'converted time(%s) : %s' % (options.type, converted_time)  if __name__ == '__main__':         main()  ","categories": ["development"],
        "tags": ["python"],
        "url": "http://localhost:4000/development/pst_pdt_conversion/",
        "teaser":null},{
        "title": "YUM local repository 구축 + httpd 연동",
        "excerpt":"여러 가지 이유로, local YUM repository를 구축할 이유가 있을 것이다.   본 포스팅에서는 local에 YUM repository를 구축하고, private 네트워크 상에서 http로 yum repository에 액세스할 수 있도록 설정하는 방법을 적는다.   원본 rpm 파일 확보  이미 존재하는 repository로부터 rpm 파일들을 ftp, scp 등으로 복사하거나, 설치 이미지로부터 rpm 파일들을 추출하자. 이미지로부터 복사하려면, www.centos.org에 접속하여 원본 설치용 DVD iso 파일들을 다운로드 한다.   CentOS-6.4-x86_64_bin-DVD1.iso CentOS-6.4-x86_64_bin-DVD2.iso   파일을 다운로드 하였다. 다운로드한 iso 파일을   mount -o loop /mnt/centos CentOS-6.4-x86_64_bin-DVD1.iso   와 같이 DVD1, 2에 대해 각각 mount 후, Package 디렉토리에 존재하는 rpm 파일들을 /srv/repo에 모두 복사한다.   createrepo 설치  yum repository를 설치하려는 시스템상에서 yum 명령어를 쓸 수 있는 상태라면 (online)   yum install createrepo  을 실행하는 것이 가장 쉬운 방법이다. 그러나, offline이라서, yum을 통해 패키지를 설치하지 못하는 상황이라면 좀 돌아가야 한다. createrepo 패키지를 설치해야 하는데, 이는 앞서 복사한 createrepo rpm 파일을 설치하면 된다. 초기 환경 구성중이라면, createrepo rpm 설치시 사전 필요한 의존 라이브러리들이 아직 준비되지 않았을 것이다.   rpm -ivh createrepo-0.9.8-4.el6.noarch.rpm  를 수행하여 createrepo를 설치해보자. 내 경우는, 두 개의 라이브러리가 없다고 각각 리포트 되었다. (createrepo 설치 시도시, python-deltarpm이 없다고 에러 뜸, python-deltarpm 설치 시도시, deltarpm이 없다고 에러 뜸)   결국, 해당 라이브러리들을 역순으로 설치해주면 된다.  rpm -ivh  deltarpm-3.5-0.5.20090913git.el6.i686.rpm rpm -ivh  python-deltarpm-3.5-0.5.20090913git.el6.i686.rpm rpm -ivh createrepo-0.9.8-4.el6.noarch.rpm   한편, 이미 createrepo가 설치된 다른 머신에서   rpm -q --requires createrepo  명령을 수행해보면, 위의 라이브러들이 필요하다는 정보를 확인할 수 있다.   createrepo 실행   createrepo /srv/repo  rpm 들을 /srv/repo에 복사했다면, 위의 명령어를 수행한다. 그러면, /srv/repo/repodata/repomd.xml까지 생성이 된다.   local yum 설정 (파일 기반)  /etc/yum.repos.d 디렉토리에 임의의 repo 설정 파일을 생성한다. 내 경우는 luran.rep를 생성하고, 다음과 같이 기록하였다.   [luran] name = luran repository baseurl = file:///srv/repo gpgcheck = 1 enabled = 1   여기까지 설정했다면, 일단 로컬 파일 기반으로 local yum repository를 사용할 수 있는 준비가 된 것이다.   확인  yum repolist  를 실행하여, 새로 추가한 yum repository가 에러 메시지 없이 뜨는지 확인한다. 현재 상태에서는 기본으로 설정된 다른 repository까지 함께 사용하게 되므로, 기본 설정으로 잡혀있는 다른 repository를 disable 시킨다.   /etc/yum.repos.d/CentOS-Base.repo 파일을 열고, enabled =1 을 모두, enabled = 0으로 변경하고 저장한다. (없으면, enabled=0을 추가 기록)   다시, yum repolist를 실행하면, 기존에 보이던 yum repository는 이제 보이지 않고, 내가 신규 설치한 yum repository만 보일 것이다.   local yum 동작 확인  yum repolist yum list | grep vim   등과 같이 실행하여 에러 없이 동작하는 것을 확인한다.   Apache httpd 설치/설정  이제 apache httpd web server를 설치한다. local file 기반의 yum repository를 사용하는 것을 넘어, 이제는 동일 네트워크 상에 존재하는 (이 yum repository에 액세스 가능한) 서버들도 이 repository를 사용할 수 있도록 http로 서비스를 설정하자.   지금까지 설정한 local yum repository가 정상적으로 동작한다면, yum 명령어로 httpd를 설치할 수 있을 것이다.   yum install httpd  그러나, 내 경우는 설치시 에러가 났다. sign이 되어 있지 않다는 에러가 떴는데, 이는 앞서 mount한 원본 CentOS-DVD 이미지에 있는 파일을 사용하면 된다.   해당 파일을 복사해 와서,   rpm --import RPM-GPG-KEY-CentOS-6  와 같이 실행하고,   yum install httpd  를 재실행하자.   이제, /repo라는 별칭으로 /srv/repo를 액세스 할 수 있도록 /etc/httpd/conf.d/repo.conf 파일을 생성하고,   Alias /repo \"/srv/repo/\"  를 추가한다.   service httpd start  를 실행한 후, 브라우저를 사용하여 또는 lynx를 사용하여   http://localhost/repo/repodata/repomd.xml 을 접근해보자.   해당 내용이 뜬다면 일단 성공했다. 만약, selinux가 enabled 된 상태라면, 웹 접속이 되지 않을 것이다.   getenforce 명령을 실행하여, selinux 설정 상태를 확인하고, 변경한다.   setenforce 0   영구적용 하려면, /etc/sysconfig/selinux (/etc/selinux/config)   SELINUX=enforcing  을  SELINUX=disabled   로 바꿔 저장해 놓는다. 만약, http_proxy 등이 잡혀 있다면, local yum proxy를 웹으로 접속시 접속 실패할 것이다. 따라서, unset http_proxy를 해주거나,   export no_proxy=ip  로 추가 선언해줘서, local에서 테스트시, 정상적으로 될 수 있도록 해 둔다.   repo 설정 변경  이제 yum.repos.d/luran.repo 파일의 baseurl을 파일타입으로부터 http 형으로 변경하자. http://youripaddress/repo   동작 확인   yum clean all yum repolist yum list | grep vim   Apache list 확인  /etc/httpd/conf/httpd.conf 파일을 열고, 아래의 내용을 추가로 편집하여 리스트 등의 기능이 정상 동작하도록 설정을 변경하고,   &lt;Directory&gt;   Options Indexes FollowSymLinks Includes ExecCGI   AllowOverride All   Order deny,all  Allow from all &lt;/Directory&gt;   다시   service httpd restart  하였다.   lynx http://localhost/repo   또는 웹브라우저에서 http://localhost/repo 를 실행하여, 해당 rpm들이 정상적으로 리스팅 되는지 확인한다.   타 서버에서 접속 허용을 위한 방화벽 설정  iptables 설정을 변경하여, 80 포트로 외부 접속을 허용하여 다른 서버에서도 해당 repository를 http 기반으로 사용할 수 있도록 변경한다.   타 서버에서의 repository 등록  앞서 로컬에서 repository 설정을 repo 파일을 추가하여 했던 것과 동일하게 진행하면 된다. 해당 서버용 repo파일을 /etc/yum.repos.d 내에 생성하고, baseurl을 지금 구축한 yum repository 의 주소로 http://youripaddress/repo 형태로 적어준다.   해당 시스템 역시 offline이라면, 다른 base repo의 설정은 disabled 상태로 바꿔서 저장해 준다.  ","categories": ["system"],
        "tags": ["yum","repository"],
        "url": "http://localhost:4000/system/yum-local-repository-httpd/",
        "teaser":null},{
        "title": "Linux EPEL 설치 (metalink 에러 조치)",
        "excerpt":"Linux EPEL (Extra Packages for Enterprise Linux)을 설치하면서 겪은 것을 기록으로 남긴다. L2TP VPN 솔루션 중, XL2TP를 설치하려고 하려고 관련 글들을 찾아보았다.   yum install xl2tpd   블로그로부터 위와 같이 실행하면 되는 것으로 확인하고, 실행하였으나 설치가 되지 않는다. 기본 설정 상태의 yum repository들에는 xl2tpd가 존재하지 않기 때문이다.   xl2tpd가 존재하는 repository가 어디에 있는지 조사해 보니, Linux EPEL에 있다고 한다. 또한, 이를 위해 EPEL을 설정해 주어야 한다는 것을 확인했다.   yum repolist   를 실행하면 기본적으로 epel이 아직 보이지 않는다. 다음과 같이 epel을 설정해 주자.   wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm 후,   rpm -Uvh epel-release-6-8.noarch.rpm  이후, yum repolist를 실행했더니, 다음 에러가 발생하였다.     Error : Cannot retrieve metalink for repositroy: epel. Please verify its path and try again.   여러가지 방법을 시도해 보고, 다음과 같이 조치했더니 동작하였다.   /etc/yum.repos.d/epel.repo 파일을 아래와 같이 편집하였다.   조치 내용은 다음과 같다. metalink를 참조하는 mirrorlist를 주석처리하고, 그 위의 baseurl을 그대로 사용하도록 주석해제한다. enabled=1로 설정한다.    [epel]  name=Extra Packages for Enterprise Linux 6 - $basearch  baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch  #mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;amp;arch=$basearch  failovermethod=priority  enabled=1  gpgcheck=1  gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6   그리고, 다시 yum repolist를 수행하니, metalink 에러 없이, 정상적으로 repolist에 액세스하였고, yum install xl2tp 도 수행할 수 있었다.  ","categories": ["system"],
        "tags": ["linux","epel"],
        "url": "http://localhost:4000/system/linux-epel-installation/",
        "teaser":null},{
        "title": "FreeNX 설치 (CentOS 6.x; 64bits)",
        "excerpt":"VNC보다 성능이 좋다고 이름이 나있는 FreeNX (www.nomachine.com)을 설치하는 방법은 의외로 간단하다.   환경 : 서버측 CentOS6.4 (64bits) 클라이언트측 Windows7 (64bits)   CentOS6.x 기준으로 설치하고, 클라이언트 접속을 테스트해보자. 먼저 yum 명령어로 freenx를 다음과 같이 간편하게 설치할 수 있다. freenx는 extras repository에 이미 있기 때문에   yum install freenx yum install nxagent   (내 경우, nxagent를 처음에 설치하지 않았더니, freenx는 정상적으로 설치하고, 계정까지 생성했지만, 접속이 실패하였고 로그를 보았더니 nxagent를 찾을 수 없다는 에러가 발견되었다.)   이 명령만으로도 freenx의 설치는 일단 완료된다. freenx의 설정관련 내용은 /etc/nxserver에 설치된다.   server의 환경설정은 node.conf를 수정하는 것만으로 완료되는데, node.conf를 백업해두고, node.conf.sample의 사본으로 작업하자.   mv node.conf node.conf.bak cp node.conf.sample node.conf   vi node.conf를 실행하여, nxserver의 환경설정을 다음과 같이 변경/주석 해제한다.   SSHD_PORT=22 (주석 해제) ENABLE_PASSDB_AUTHENTICATION=&amp;quot;1&amp;quot; ENABLE_SSH_AUTHENTICATION=&amp;quot;1&amp;quot;   파일을 저장한다. nxserver를 실행하면 여러가지 실행 옵션이 있다. nxserver –status를 실행하여 현재 상태를 확인한다.   NX&gt; 100 NXSERVER - Version 3.2.0-74-SVN OS (GPL, using backend: not detected) NX&gt; 110 NX Server is running NX&gt; 999 Bye   현재 설치된 버전의 3.2.0-74이고, 동작중이라는 것을 알 수 있다.   nxsever --stop  을 실행하면, freenx가 종료된다. 이제 사용자를 추가한다.   nxserver --adduser luran   freenx 접속을 위해 luran이라는 계정이 생성될 것이다.   nxserver --passwd luran   luran이라는 계정의 패스워드를 물어올 것이다. 여기까지 실행하면, /home/luran/.ssh 디렉토리에 ssh 접속을 위한 키가 생성되었을 것이고, /etc/nxserver에도 해당 ID용 공개키가 client.id_dsa.key 로 생성되었을 것이다.   이제 client.id_dsa.key파일 또는 파일의 내용을 실제 접속할 클라이언트에서 쓸 수 있도록 복사하자. 만약, SSH 기본 포트 이외의 다른 포트로 변경했다면 방화벽 룰도 변경해줘야 한다.   nxserver --start  를 실행하여 띄운다. 클라이언트에서 설정할 내용은 간단하다. Windows의 경우, 3.5 버전의 NXClient를 구해서 설치한 후,   Host와 Port 정보를 기입하고, Key 버튼을 클릭하여 앞서 별도로 저장한 key 파일을 지정한다. Desktop은 Unix/GNOME으로 하고, 접속속도는 원하는대로 설정한다.   앞서 생성한 계정정보를 입력하는 것으로, 설정을 모두 마쳤다. 이제 접속하여 창이 뜨는지 확인하자.   ","categories": ["system"],
        "tags": ["freenx","linux"],
        "url": "http://localhost:4000/system/freenx-centos-6-x-64bits/",
        "teaser":null},{
        "title": "아이폰으로 집의 네트워크에 VPN으로 접속하려면?",
        "excerpt":"나중에 다시 할 수도 있는 재설치를 위해 기록으로 남겨둔다.   예전에 사용하던 IPTIME 공유기를 가지고 있었다면, 일단 우선적으로 공유기에서 제공하는 VPN 기능을 검토했을 것이다. (그랬으면 쉬웠을텐데..다시 살까?)   일단, 현재 집에서 사용하는 공유기는 Netgear 공유기이다. 이 모델에는 VPN 기능이 제공되지 않는다.   따라서, 검색/ 정보 수집을 통해 PPTPD, OpenVPN, OpenSwan 등을 조사했으며, 공유기 내부에서도 잘 동작한다고 알려진 OpenVPN을 설치하기로 하였다.   우선, OpenVPN에 대한 설치파일/세부 정보는 openvpn.net으로부터 확인할 수 있다. 최초 설치 시도는 CentOS VM상에 yum install을 통해 설치/실행하는 것을 진행하였으나, OpenVPN-AS(Access Server)가 설치가 쉽다는 내용을 접하고, OpenVPN-AS로 급선회하였다.   OpenVPN Server와 달리, AS의 경우 설치 과정이 매우 간단했다. 해당 RPM 설치 후, 모든 설정을 관리자웹 상에서 설정해 주기만 하면 될 뿐만 아니라, 사이트내에 각 기능별 설명들이 기본적으로 표기가 되어 있어서 도움이 되었다. (OpenVPN 서버의 경우, 기본 값들이 무슨 기능을 하는지 주석이 달려있기는 하지만, 다소 설명이 부족하다는 느낌을 받았다.)      집 네트워크 구성   PC1 -+          | PC2 -+—-+ 공유기 +—-+ ISP 공유기 +—-+ 인터넷 +—-+ 아이폰          | PC3 -+   예전에 케이블 인터넷을 사용했을 경우, ISP 공유기 대신 케이블 모뎀이 위치했고, 내 공유기 설정에서 포트포워딩을 설정해주는 것만으로 집 내부 PC들에 접근이 가능했으나, 이사 후 ISP 제공 공유기가 앞단에 더 존재하게 되어 ISP 공유기 설정도 함께 변경해 줘야만 내부 접근이 가능하게 되었다. (DMZ로 설정하거나, 다른 룰을 추가하거나)      VPN 연결 개요 위의 환경 구성을 바탕으로, VPN을 다음과 같이 환경을 구성하였다.        +——————————- VPN Tunnel ——————————–+ Linux     –+ (VPNGW)  | NIC#1(e)  | NIC#2      |                 | PC1       –+—-+ 공유기 +—-+ ISP 공유기 +—-+ 인터넷 +—-+ 아이폰 NIC#3      |      NIC#5(e)            NIC#7(e)                                 NIC#9                 |      NIC#6                 NIC#8 PC2       –+ NIC#4      e : external   집 내부 공유기 내부에서의 PC들간은 별도의 설정이 없어도 공유기에 연결한 것만으로 NIC#6를 통해 NIC#2, NIC#3, NIC#4끼리 통신할 것이다.   내부에서 외부로 접속하고자 한다면, NIC#5를 거쳐 나갈 것이다. 다만, 위의 경우와 같이 2개의 공유기를 거쳐야 한다면, NIC#5로부터 #8, #7을 거쳐 외부로 나갈 것이다.   Linux를 VPNGW로 삼고, 외부에서 이 Linux를 VPNGW로 접속하여 PC1,2로 연결하도록 하는 것이 목표이다.      OpenVPN-AS 설치 www.openvpn.net에 접속하여, OpenVPN-AS(Access Server)를 다운로드한다. 내 경우는 CentOS용 RPM을 다운로드하였고, 설치하였다.   passwd openvpn   을 실행하여, openvpn 계정의 비밀번호를 변경한다. 그러면, 추후 OpenVPN관리자 계정의 비밀번호로 사용할 수 있게 된다.   설치는 일반 OpenVPN 서버 설치 과정에 비해 OpenVPN-AS 설치 과정이 훨씬 간단했다. (easy-rsa를 사용한 server, client 인증서 생성 등의 과정을 별도로 할 필요가 없다.)   정상적으로 설치가 되었다면, https://vpngw의 비공인 IP:943으로 접속하면, OpenVPN-AS의 관리 페이지가 뜬다. 이 때, 인증서를 신뢰할 수 없다고 뜨면 무시하기를 눌러서 진행하자.   1) 서버용관리 - https://주소:943/admin 2) 클라이언트 - https://주소:943   서버용 페이지에 접속하여 살펴보면, 좌측의 큰 메뉴 기준으로 Status/Configuration/ User Management/Authentication/Tools 메뉴가 존재한다.   이 메뉴들 중, 실제로 내용 수정을 한 페이지 기준으로 기록한다.     Status : 별도 수정 항목 없음   Configuration   License : 기본 라이센스 (2명)   Server Network Settings   Hostname or IP Address : 내 집에서 달고 나가는 공인 IP (공유기 관리웹에서 확인하거나, 내 IP를 확인해 주는 사이트 등을 통해 확인 가능)   Interface and IP Address : NIC#2의 주소 기록   VPN Mode : Layer 3 (routing/NAT)   VPN Settings   Dynamic IP Address Network : Network Address 디폴트값 그대로 사용 (ex - 172.27.224.0)   Routing : using NAT   Specify private subnet : NIC#2, #3, #4용 (ex - 192.168.1.0/24)   나머지 값들은 기본 값을 그대로 사용해도 된다.   이렇게 서버를 설정하고 나서, 클라이언트의 원활한 설정을 위해 집 내부망에서 아이폰으로 https://주소:943으로 접속해보자. 그러면, 클라이언트를 다운로드할 수 있는 링크가 출력되고, 아이폰의 경우 결국 AppStore에서 OpenVPN 클라이언트를 다운로드할 수 있도록 연결된다.   클라이언트에서 접속한 웹 페이지로부터 하단의 링크를 클릭하면, 직접 profile을 다운로드 할 수 있다. 다운로드한 파일을 OpenVPN 클라이언트에서 열기를 지정해주면, 결국 이 설정 파일은 OpenVPN 클라이언트에 import 되면서, 자동으로 프로파일이 추가된다. (만약, OpenVPN-AS버전이 아니라 Community Edition으로 직접 수동 설치를 진행했다면, 인증서 파일 및 클라이언트 설정파일을 복사/수정하여 client.ovpn 파일을 만들어 주었어야 했을 것이며, opvn 파일 및 인증서 등을 itunes를 사용하여 동기화 해주거나, 이메일로 직접 전송해서 import시켜야 했을 것이다.)   이후, 아이폰의 네트워크를 3G/LTE 등 외부망으로 선택하고, OpenVPN 클라이언트를 통해 접속시도하면, 성공적으로 VPN 접속이 이뤄지는 것을 확인할 수 있다.   IP를 확인해보면, 터널IP를 새로 받은 것을 확인할 수 있으며, telnet 클라이언트를 이용하여 내부망에 있는 PC로 ping을 날려도 정상적으로 동작하는 것을 확인할 수 있다.   ","categories": ["system"],
        "tags": ["vpn","openvpn"],
        "url": "http://localhost:4000/system/openvpn/",
        "teaser":null},{
        "title": "Sonar + Jenkins 설정",
        "excerpt":"현재 Jenkins를 통해 CI(Continuouse Integration)를 하고 있고, 코드의 정적 분석을 FindBugs, PMD, CheckStyle을 써서 리포트를 보고 있긴 하지만, Sonar를 접하고 설치/ 사용해보고자 한다.   Sonar는 www.sonarqube.org에서 세부 정보를 확인할 수 있다. Installation Guide 링크를 따라 가보니, Confluence를 사용하여 위키를 꾸며놨다.   “Get Started in Two Minutes”의 설명에 따르면, 설치하고, 실행하는 방법은 무척이나 단순해 보인다.     SonarQube 배포판을 압축풀고, 실행시킨다.   SonarQube Runner 배포판을 압축풀고, 실행시킨다.   샘플 프로젝트를 압축풀고, sonar-runner를 실행시켜서 분석시킨다.   결과를 화면에서 확인한다. (http://localhost:9000)   Sonar에 대한 소개/분석 자료를 통해 좀더 알아보면, Sonar가 지원하는 DBMS는   Apache Derby (내장) mySQL Oracle PostgreSQL MS-SQL Server  등이다.   설치에 앞서 JDK가 1.5+으로 설치되어 있어야 하며, Runner(소스 분석을 위해 실행시키는 프로그램)로는     SonarQube Runner   Maven   Ant 를 지원한다고 한다.   따라서, 앞서 언급한 Installation guide는 최소한의 설정으로 가장 빨리 한 번 실행시켜보기 위한 설정이라 할 수 있다.   0. 설치/테스트 환경  OS : Windows7 (64) JDK : JDK1.6 DB : -   1. 설치  압축을 풀고 (c:\\dev\\sonarqube-4.0, 이하 $SONAR_HOME), $SONAR_HOME\\bin\\windows-x86-64\\bin에 있는 SartSonar.bat를 실행시킨다.   wrapper | --&gt; Wrapper Started as Console wrapper | Launching a JVM... jvm 1 | Wrapper (Version 3.2.3) http://wrapper.tanukisoftware.org jvm 1 | Copyright 1999-2006 Tanuki Software, Inc. All Rights Reserved. jvm 1 | jvm 1 | 2013.12.18 11:02:42 INFO Web server is started   와 같은 메시지가 보이면서 시작되었다. 브라우저를 통해, localhost:9000으로 접속해봐도 화면이 휑하다. Sonar의 기본 관리자 계정 정보는 admin:admin이니 접속해 보자.   2. 환경설정 확인  $SONAR_HOME\\conf\\sonar.properties 파일을 확인해 보자. 별도 DB 설정을 하지 않았기에, Apache Derby 내장 DBMS를 사용하도록 설정된 채이다.   sonar.jdbc.username=sonar sonar.jdbc.password=sonar  #----- Embedded database H2 # Note: it does not accept connections from remote hosts, so the # SonarQube server and the maven plugin must be executed on the same host. # Comment the following line to deactivate the default embedded database. sonar.jdbc.url=jdbc:h2:tcp://localhost:9092/sonar  와 같은 문구가 보이고, 그 밑으로는 Sonar가 지원하는 여러 다른 종류의 DBMS의 configuration들이 주석처리 되어 있다. 해당 DBMS를 사용할 경우, 변경하여 사용하면 될 것 같다.   3. 프로젝트 분석은 어떻게?  Sonar는 분석 결과를 보여주는 웹 애플리케이션이고, 결국 분석은 다른 프로그램들이 도와줘야 한다. http://docs.codehaus.org/display/SONAR/Analyzing+Source+Code 에 안내된 바에 따르면,   프로젝트 분석은 아래의 방법중 하나를 택하면 된다. Analyzing with SonarQube Runner (recommended analyzer) Analyzing with SonarQube Ant Task Analyzing with Maven Analyzing with Gradle CI Engines   Sonar에 안내된 2분안에 시작하기 튜토리얼에서도 샘플 프로젝트 디렉토리로부터, SonarQube Runner를 실행시키는 것으로 프로젝트 등록/분석을 시도하였다.   결국, Runner 등과 같은 프로그램이 분석 결과를 DBMS에 기록을 하면, Sonar 웹은 그 데이터를 DB로부터 읽어서 보여줄 뿐일 것이다.   프로젝트 등록을 위한 UI도 존재하지 않는 것 같은데, 공식 도움말에서도 이런 문구를 발견할 수 있다. Adding a project to SonarQube is not done through the web interface, but automatically when the project is analyzed for the first time. 어쨌든, 프로젝트를 등록하기 위해서는 일단 분석을 한 번 시도하면 된다는 이야기이다.   4. Jenkins + Sonar 설정  나는 Jenkins를 이미 사용하고 있으므로, Jenkins에 Sonar 설정을 하는 방법에 대해 살펴보고자 한다. Jenkins에 Maven 관련 설정이 되어있다고 가정한다.   Jenkins 관리 &gt; 플러그인관리 &gt; Sonar 관련 플러그인들을 찾아 설치한다. (Jenkins Sonar Plugin, Sonargraph Plugin) 이후, Jenkins를 재시작하고, Jenkins의 global 설정으로부터 Sonar 환경설정을 하자.   Jenkins 관리 &gt; 시스템설정 &gt; Sonar (고급 메뉴) 관련 항목을 지정한다. Apache Derby 사용 및 기본 설정으로 그대로 사용한다면, 아마 다음과 같이 입력하게 될 것이다.  Server URL : http://localhost:9000 Sonar account login : admin Sonar acocun password : admin Database URL : jdbc:h2:tcp://localhost:9092/sonar Database login : sonar Database password : sonar  Sonar Runner에 대한 설정은 별도로 수행하지 않았다. 프로젝트 설정의 Post-build Actions로부터 Sonar를 선택하고, jdk 설치 정보만 변경하고 저장하였다.  (다른 설정은 디폴트 사용) 이후, 빌드를 수행하면, 해당프로젝트의 대시보드에도 Sonar 아이콘이 생겨있고, 링크도 클릭가능하다. 해당 링크를 클릭하면, 결국 처음에 설정한 바와 같이 http://localhost:9000으로 리다이렉션 될 것이다. 그리고, 이전에 비어있던 화면에 무언가 정보가 채워져 있는 것을 발견할 수 있다.   당연히, Jenkins를 통하지 않고도, 브라우저에서 직접 http://localhost:9000을 접속하면 동일한 화면을 볼 수 있다.  (DB에 분석 데이터가 있으니까)   볼 수 있는 내용들은 다음과 같다. Lines of code Documentation Duplications Complexity Issues Package tangle index Unit Tests Coverage   해당 항목들을 클릭하면, 상세 내용을 볼 수 있다. 이슈들을 클릭하면, 어떤 부분이 문제가 되고, 어떻게 해결하면 좋을지도 조언해 주기 때문에 코드의 퀄리티를 높이는데 큰 도움이 될 것 같다.   Jenkins에 이미 SCM 관련 설정을 해두어, 소스코드 변동에 따른 폴링/자동 빌드를 설정해 두었기 때문에 소스 변동시마다 Sonar도 분석을 자동으로 업데이트 하게 되었다.   5. Maven 단독  Maven으로 소스 분석을 하려면, maven 관련 설정을 변경해줘야 한다. Sonar에 공식적으로 안내된, maven 설정은   &lt;settings&gt; \t&lt;profiles&gt; \t&lt;profile&gt; \t&lt;id&gt;sonar&lt;/id&gt; \t&lt;activation&gt; \t&lt;activeByDefault&gt;true&lt;/activeByDefault&gt; \t&lt;/activation&gt; \t&lt;properties&gt; \t&lt;!-- Example for MySQL--&gt; \t&lt;sonar.jdbc.url&gt; \tjdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8 \t&lt;/sonar.jdbc.url&gt; \t&lt;sonar.jdbc.username&gt;sonar&lt;/sonar.jdbc.username&gt; \t&lt;sonar.jdbc.password&gt;sonar&lt;/sonar.jdbc.password&gt;  \t&lt;!-- Optional URL to server. Default value is http://localhost:9000 --&gt; \t&lt;sonar.host.url&gt; \thttp://myserver:9000 \t&lt;/sonar.host.url&gt; \t&lt;/properties&gt; \t&lt;/profile&gt; \t&lt;/profiles&gt; &lt;/settings&gt;   실행은,  mvn clean install -DskipTests=true mvn sonar:sonar  와 같이 실행하면 된다. 그런데, 위의 설정만으로는 실제 사용시 번거로울 것 같다.   위의 설정에서 언급한 바와 같이, 분석을 어떻게 수행하느냐의 문제에 대해 생각해 보자. Jenkins에 설정한 경우,     Who : Jenkins   How : Maven 또는 Runner   When : 소스 변경이 되었을 때 로 생각할 수 있는데 비해,   Maven으로 단독 설정할 경우,     Who : 사용자   How : Maven   When : 사용자가 명령어를 실행할 경우 가 되어 버린다. 결국, 소스를 사용자가 직접 최신 버전으로 업데이트 한 후, maven 명령어를 돌려야 의미가 있을 것이다.   즉, 소스 변경과 함께 자동으로 최신 소스를 검사할 수 있도록 설정하지 않고서는 별도의 maven 실행을 통한 소스 검사만으로는 활용도가 낮아진다.   6. 요약     Jenkins와 같은 CI tool과 연계해서 Sonar 검사를 하도록 하거나        열심히 알아서 소스 갱신/ 검사 돌리자.            SCM Activity라는 플러그인이 있는 것을 확인했으나, 주기적으로 폴링하여 소스를 가져오고 검사하는 용도가 아닌 SCM blame 정보를 출력해주기 위한 용도인 듯하다. (http://docs.codehaus.org/display/SONAR/SCM+Activity+Plugin)        소스 정적 검사 및 해결책 제시가 제법 괜찮다. 쓸만 한 것 같다.   ","categories": ["development"],
        "tags": ["java","sonar","jenkins"],
        "url": "http://localhost:4000/development/sonar-jenkins/",
        "teaser":null},{
        "title": "Windows에서 overlay icon이 보이지 않을 때",
        "excerpt":"요즘에는 각종 설치하는 프로그램마다 오버레이 아이콘을 제각각 지원하는 프로그램들이 많다. 문제는, 그런 프로그램들이 많기 때문에 정작 내게 필요한 아이콘이 나오지 않아서 불편함이 생긴다는 것이다. 특히, 윈도우즈에서는 기본적으로 등록 우선순위상 15개의 아이콘만 보여주기 때문에, 사용상 불편함이 초래될 가능성이 높은 편이다.   Regedit을 실행시켜서 다음의 경로를 찾아가 보자.  HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellIconOverlayIdentifiers  그러면 여러 개의 아이콘들이 이미 등록된 것을 확인할 수 있다. 이 때, 불필요해보이는 오버레이 아이콘 정보를 지우자. 내 경우, Tortoise SVN의 아이콘들이 출력되었으면 하는 정보였기 때문에,   Tortoise SVN외의 다른 불필요한 정보는 지워줘서, 적어도 Tortoise SVN의 오버레이 아이콘이 15등 안에 들게 변경해 주었다.   ","categories": ["system"],
        "tags": ["windows"],
        "url": "http://localhost:4000/system/windows-overlay-icons/",
        "teaser":null},{
        "title": "Installer방식으로 Jira (you host)를 CentOS에 설치하기",
        "excerpt":"매우 유명한 issue tracker인 Jira를 어쨌든 한 번 써보고 싶었다. atlassian에서 운영하는 버전으로 말고, 직접 내가 설치하는 버전으로 구매했다.   You host 버전을 구매하며, 10 users 미만이면 $10가 든다. 그 옆에 Jira Agile은 $20가 든다고 한다.   그래서, 이왕이면 스크럼도 되고 칸반도 된다고 써있는 Jira Agile로 간다! 조금 더 읽어보니, Jira Agile은 Jira의 plugin이고, 구 Greenhopper라고 써 있다.   각각 $10로 총 $20 들었다.           설치 환경   - OS : Centos6   - JDK : OpenJDK7   - MySQL            JDK7 설치       yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel  을 설치한다.   .bash_profile내, JDK의 path를 잡아주었다.  JAVA_HOME=/usr/lib/jvm/java-openjdk PATH=$PATH:$JAVA_HOME/bin export PATH   설치/설정 후, shell상에서 java -version, javac -version 을 수행하여 java가 잘 설치되었는지 확인한다.      DB 설치/설정 HSQLDB 대시 MySQL을 향후 설정할 것이므로, MySQL을 설치하고, 기초 설정을 하자.   yum install mysql-server   를 수행하여 mysql 서버를 설치하자. 설치가 완료되면,   service mysqld start  를 실행시킨다. 친절하게 안내문이 나오는데,   /usr/bin/mysqladmin -u root password 'new-password'  를 수행하여, root에 새 비밀번호를 할당해 주라고 한다. 혹은 위의 방법 대신,   /usr/bin/mysql_secure_installation   을 실행하라고 안내문도 뜬다. mysql_secure_installation을 실행시켜 보자. 이것저것 interactive하게 묻는다.   * Set root password? [Y/n] * Remove anonymous users? [Y/n] * Disallow root login remotely? [Y/n] * Remove test database and access to it? [Y/n] * Reload privilege tables now? [Y/n]      Jira 다운로드/설치 atlassian에서 jira를 다운로드한다. bin/ tar.gz/ war 형식 등이 있으니 원하는 것으로 다운로드 하면 된다. (그러나, 결국 어떤 형식으로 받아도 나중에 확인해보면 web application일 뿐이다.)   get http://www.atlassian.com/software/jira/downloads/binary/atlassian-jira-6.2.1-x64.bin 로부터 jira를 임의의 디렉토리에 다운로드 한다. (어차피 세부 디렉토리 설정 등은 추후 인스톨 과정에서 설정할 것이다.) 만약, bin 파일이 아닌 다른 형식의 파일을 다운로드했다면, 직접 디렉토리도 생성하는 등의 관리를 해줘야 할 수도 있다.   admin 권한이 없는 계정으로부터 jira를 설치하려고 하면, 시스템 권한이 없다고 에러를 보게 될 것이다. 따라서, 계정에 시스템 권한을 주고 설치를 하거나, 나중에 후조치를 하면 될 것이다.   설치의 편의상 우선 root로 진행하였다.   chmod +x atlassian-jira-6.2.1-x64.bin ./atlassian-jira-6.2.1-x64.bin   다음의 메시지가 나오면서 설치가 진행된다. 내 경우는, Advanced Option으로 설치를 진행시켰고, 디렉토리는 각각 아래와 같이 명시했다.   Unpacking JRE ... Starting Installer ... Mar 22, 2014 6:29:55 PM java.util.prefs.FileSystemPreferences$1 run INFO: Created user preferences directory. This will install JIRA 6.2.1 on your computer. OK [o, Enter], Cancel [c]  Choose the appropriate installation or upgrade option. Please choose one of the following: Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing JIRA installation [3]  Where should JIRA 6.2.1 be installed? [/opt/atlassian/jira]  Default location for JIRA data [/var/atlassian/application-data/jira] /var/atlassian/jira/data Configure which ports JIRA will use. JIRA requires two TCP ports that are not being used by any other applications on this machine. The HTTP port is where you will access JIRA through your browser. The Control port is used to Startup and Shutdown JIRA. Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2] 8090 Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2]  JIRA can be run in the background. You may choose to run JIRA as a service, which means it will start automatically whenever the computer restarts. Install JIRA as Service? Yes [y, Enter], No [n]      계정/권한 변경 jira 계정을 만들고 owner를 변경해 준다.   useradd jira passwd jira chown -R jira:jira /opt/atlassian chown -R jira:jira /var/atlassian      실행 jira를 설치한 곳의 bin 디렉토리에 가면, tomcat 관련 파일들이 보이고 익숙해 보이는 shell 들이 눈에 띈다. startup.sh를 실행시키고 브라우저에서 접속 여부를 확인하자.   첫 실행시 다소 시간은 걸리겠지만, 잠시 후 다음과 같은 화면이 뜨면 일단 설치는 된 것이다. 이후 세부 설정을 하자. (다음 포스트)   &lt;/a&gt;   ","categories": ["system"],
        "tags": ["jira"],
        "url": "http://localhost:4000/system/jira-you-host-centos/",
        "teaser":null},{
        "title": "git 실행시 libcurl.dll에서 curl_multi_timeout 에러 발생시",
        "excerpt":"에러가 발생하는 원인/상황은 다양할텐데, 내 경우는 다음과 같이 조치하였다.      http://curl.haxx.se/libcurl/ 에 방문하여, libcurl.dll을 다운로드한다.   c:\\Windows\\System (32bits), c:\\Windows\\SystemWow64 (64bits)에    위 단계에서 다운로드한 libcurl.dll을 복사한다.             ","categories": ["system"],
        "tags": ["git"],
        "url": "http://localhost:4000/system/git-libcurl-dll-curl_multi_timeout-error/",
        "teaser":null},{
        "title": "tar.gz로부터 Jira(you host)를 CentOS에 설치하기",
        "excerpt":"Jira를 installer (bin)로부터 설치하는 대신,tar.gz로부터 설치하더라도 사실 별 차이는 없다. 다음과 같이 진행하였다.      admin 권한을 가진 계정으로 진행 (예: jira)   jira (tar.gz)를 atlassian으로부터 다운로드한다.        원하는 디렉토리에 해당 파일을 압축 해제한다. 예) /opt/atlassian-jira       압축을 해제하고 디렉토리 구조를 bin 파일을 설치한 것과 비교해 보면, 사실 동일하다. tomcat도 함께 패키징 되어 있다.       atlassian 사이트내 가이드에 따르면, 설치 방법이 installer를 사용하거나, 지금 방법과 같이 tar.gz로부터 설치하는 것을 권장한다고 한다.       war 파일을 통한 설정은 그다지 권하지 않는다고 한다.            jira의 데이터가 저장될 디렉토리를 생성하고, 환경변수에 JIRA_HOME으로 설정하자. 이 디렉토리도 jira 계정과 동일하게 owner 설정이 되어 있어야 한다.       jira를 설치한 디렉토리의 bin을 열어보면 start-jira.sh로 시작시키고, stop-jira.sh로 종료시키면 된다.  ","categories": ["system"],
        "tags": ["jira"],
        "url": "http://localhost:4000/system/tar-gz-jira-local-hosting/",
        "teaser":null},{
        "title": "Jira에 MySQL 연동하기",
        "excerpt":"어떤 방법을 선택했건, Jira를 띄울 수 있는 상태가 되었다면, 이제 MySQL을 연결해서 사용할 수 있도록 설정을 진행해보자.   MySQL 계정 설정   atlassian가이드에는 다음과 같이 안내되어 있다.   CREATE DATABASE jiradb CHARACTER SET utf8 COLLATE utf8_bin; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER,INDEX on &amp;lt;JIRADB&amp;gt;.* TO '&amp;lt;USERNAME&amp;gt;'@'&amp;lt;JIRA_SERVER_HOSTNAME&amp;gt;' IDENTIFIED BY '&amp;lt;PASSWORD&amp;gt;'; flush privileges;   DB 접속에 사용할 계정 정보로 빈 칸을 채워넣고 실행하자. 정상적으로 grant가 되었는지는,   show grants;   를 수행하여 확인할 수 있다.   MySQL Connector 설치   첫 시도 시, MySQL Connector를 yum을 사용하여 다운로드하고 진행했으나, DB 연결에서 실패하였다.   yum install mysql-connector-java   atlassian 가이드에 안내된 바와 같이, MySQL 공식 사이트로부터 (http://dev.mysql.com/downloads/connector/j) connector를 다운로드 하고 진행하니, 별 에러 없이 connection에 성공하였다.   MySQL 사이트로부터 다운로드한 파일로부터, mysql-connector-java.5.x.x-bin.jar를 찾아 jira를 설치한 곳의 lib 디렉토리에 복사한다. (내 경우, /opt/atlassian-jira에 jira를 설치했고, /opt/atlassian-jira/lib에 해당 connector jar를 복사하였다.)   Jira 설정 진행   지난 포스팅에서와 같이 Jira를 설치하고, 실행시켰다면 브라우저 상에서 http://:8080으로 접속 가능할 것이다.   이제 DB 종류를 MySQL로, 접속할 주소와, 계정 정보 등을 본인이 설정한 대로 모두 입력하자.   하단의 접속 테스트를 실행하여 정상적으로 연결이 된다고 뜬다면, 다음 단계로 넘어가자.   기타 설정   라이센스, 기타 정보 등을 모두 입력하고 나면, 다소 시간이 걸리지만, 설정이 모두 완료될 것이다. (내 경우는 꽤 오랜 시간이 걸려서 첫 화면이 떴다.)   Trouble shooting   Jira를 설치한 디렉토리의 /logs/catalina.out을 살펴보면 어떤 에러가 발생했는지 확인할 수 있을 것이다.   내 경우는,   ERROR The Gadget Dashboard bundled plugin is not available. Please contact an administrator to ensure the Gadget Dashboard plugin is enabled! Perhaps you need to log in to see the page. If you think this message is wrong, please consult your administrators about getting the necessary permissions.   과 같은 에러가 발생하였고, atlassian 사이트에 나와 있는 안내를 참고하여, $JIRA_HOME/plugins/.osgi-plugins를 삭제하고, Jira를 재시작 하였다. 그 이후, 다시 실행시켰더니 이번에는 몇몇 플러그인이 로딩되지 않았다고 에러 메시지가 떴다. 이 역시 atlassian 사이트로부터 bin/setenv.sh에 선언되어 있는 환경변수를 조정해 보라는 안내를 발견하여, 다음과 같이 옵션을 변경하였다.   JVM_SUPPORT_RECOMMENDED_ARGS=300   이후, Jira를 시작시켰더니 앞서 발생한 플러그인 로딩 실패와 관련한 에러는 발생하지 않고, 정상적으로 구동 되었다.   정리     jira를 어떤 방법이든 택하여 설치   MySQL Connector 최신 버전 설치   Connector jar를 lib에 복사   setenv.sh내 환경변수(JVM_SUPPORT_RECOMMENDED_ARGS=300 편집, 메모리 값 조정)   atlassian 가이드 많이 찾아보기   ","categories": [],
        "tags": ["jira"],
        "url": "http://localhost:4000/connecting-jira-mysql/",
        "teaser":null},{
        "title": "두 개의 서로 다른 물리 디스크를 하나로 묶어서 쓰려면? - lvm 설정 예",
        "excerpt":"   OS : CentOS6 (64 bits)   Virtualization S/W : VirtualBox   대략 순서는 다음과 같다. 하드드라이브설치/인식 -&gt; 파티션설정(lvm) -&gt; 물리볼륨설정 -&gt; 볼륨그룹생성 -&gt; 논리볼륨설정 -&gt; 파일시스템설정      VirtualBox 하드 디스크 추가 설정 기존에 존재하는 vm 이미지의 context menu로부터 repository를 추가한다. 가정한 시나리오는 물리 디스크 두 개이므로, 하드 디스크 두 개를 추가하였다. 해당 vm instance를 시작시켜서, 추가한 하드디스크가 정상적으로 인식되는지 확인한다.   # fdisk -l  을 수행하여, /dev/sdb /dev/sdc   두 개가 추가된 것을 확인한다.      LVM 파티션 생성 fdisk를 사용하여 하드디스크 /dev/sdb, /dev/sdc에 LVM 타입의 파티션을 다음과 같이 생성한다.   fdisk /dev/sdb - n (새 파티션) - p (primary partition) - 1 (partition number) - 엔터 (default) - 엔터 (default) - t (change partition's system id) - l (list codes) - 8e (LVM type) - w (write &amp;amp; exit)   이후 fdisk -l /dev/sdb 실행하면, System 부분이 Linux LVM으로 출력된다. /dev/sdc에 대해서도 동일하게 작업 수행한다.   각 디스크에 대해 한 개의 파티션만 생성했으므로, 위의 작업이 완료되면 /dev/sdb1, /dev/sdc1이 생성된 것을 확인할 수 있다.      물리볼륨 생성 위에서 생성한 LVM 파티션에 물리볼륨을 생성한다.   # 물리볼륨 생성 pvcreate /dev/sdb1 pvcreate /dev/sdc1  # 물리볼륨 조회 pvdisplay      볼륨그룹 생성 앞서 생성한 물리볼륨이 하나로 동작하도록 설정하기 위해 그룹으로 묶어준다.   # vgcreate 볼륨그룹명 물리볼륨1 물리볼륨2 ... vgcreate jsvg /dev/sdb1 /dev/sdc1  # 볼륨그룹 조회 vgdisplay      볼륨그룹내 논리볼륨 생성 시나리오상, 두 개의 물리디스크를 하나의 논리디스크로 사용하겠다고 했으므로, 다음과 같이 설정한다.   # lvcreate -n 논리볼륨명 -l 100%FREE 볼륨그룹명 # -l 100%FREE : 전체 크기를 하나의 볼륨으로 쓰겠다 lvcreate -n lv -l 100%FREE jsvg   만약, 앞서 가정한 시나리오 대신 10GB와 나머지 크기의 두 논리 디스크로 나누겠다면, 다음과 같이 설정할 수 있다.   lvcreate -n lv1 -L 10G jsvg lvcreate -n lv2 -l 100%FREE jsvg  # 논리볼륨 조회 lvdisplay      파일시스템설정 앞서 생성한 논리볼륨을 포맷하고, 마운트해보자.   # 앞서 생성한 논리볼륨을 ext3으로 포맷 mkfs.ext3 /dev/jsvg/lv  # mount할 포인트 생성 mkdir /mnt/data  # mount mount -t ext3 /dev/jsvg/lv /mnt/data   이제 /mnt/data로 디렉토리를 이동하여 살펴보자. df -l 을 수행하여, 디렉토리의 용량이 의도한 대로 (두 디스크의 용량이 합쳐진 것으로) 설정되었는지 검토한다. 부팅시 해당 파일 시스템이 자동으로 마운트 되도록   vi /etc/fstab을 수정한다.   /dev/jsvg/lv /mnt/data ext3 defaults 0 0   와 같이 변경한다.  ","categories": ["system"],
        "tags": ["linux","lvm"],
        "url": "http://localhost:4000/system/lvm-disk-managment/",
        "teaser":null},{
        "title": "DRBD, Pacemaker, Corosync 기반 PostgreSQL H/A 구성환경 설정",
        "excerpt":"환경 구성     VM 2개(node 설정용, 필수, VirtualBox/VMWare 사용) + 1개(테스트용, 선택)   CentOS 6.4 (64bits)   노드별 사전 준비사항     NIC : 2개 (VMWare/VirtualBox 관리 메뉴에서 추가)   별도 하드 파티션 (DRBD 전용) 설정 (VMWare/VirtualBox 관리 메뉴에서 추가)   주요 설치 순서     노드 네트워크 설정   DRBD 전용 파티션 설정   PostgreSQL 설치   Pacemaker, Corosync, DRBD, Heartbeat 설치   DRBD 환경설정   postgreSQL + DRBD 동작확인   corosync 환경설정   corosync 동작 확인   pacemaker 환경설정 (리소스 등록/관리)   실행 검사   설치/ 환경설정  표기/참고  제목 뒤에 해당 노드를 표기하고, 모든 노드에서 동일하게 수행해야 하는 작업은 both로 표기한다. 초기 환경 설정/ 프로그램 설치까지 중복되는 작업일 수 있으므로, 한 vm에 어느 정도 설정을 한 후 clone하는 것이 더 편리할 수 있다. 본 설치 예제에서는, node1.mycluster, node2.mycluster라는 이름으로 노드를 구성하였다.   설치 전 환경 설정 (both)  1) VMWare/VirtualBox 메뉴상 설정     Network Interface를 각 노드별 1개씩 추가한다.   Hard disk를 각 노드별 1개씩 추가한다. 이후, 부팅하여 fdisk -l로 확인하면, 추가된 디스크가 /dev/sdb 등과 같이 나타날 것이다. 새로 추가한 /dev/sdb를 향후 DRBD 전용으로 사용한다.   2) 네트워크 대역 구성     LAN : 10.0.0.x (DB의 virtual IP도 이 대역으로 할당)   Crossover : 172.16.0.x (두 노드 간의 통신 신뢰도 및 성능 향상을 위해 구성)   3) 노드 구성 전체 노드는 다음과 같이 구성하였다.       node1   node1.mycluster : 10.0.0.191 (LAN), 172.16.0.1 (cross)   node2   node2.mycluster : 10.0.0.192 (LAN), 172.16.0.2 (cross)   DB virtual IP (상황에 따라 active node로 설정 변경됨)   dbip.mycluser : 10.0.0.190   실제 application이 접속하게 되는 IP address   노드 기본 설정(특별한 언급이 없는 한, root 계정에서 실행)  1) SELINUX 설정 해제 (both)   getenforce  를 실행하여, SELINUX가 활성/해제 상태인지 확인한다. Enforcing으로 출력된다면, 활성화된 상태이므로,   vi /etc/selinux/config SELINUX=disabled   라고 변경한다.   2) hostname 지정   vi /etc/sysconfig/network   [node1]  NETWORKING=yes NETWORKING_IPV6=no HOSTNAME=node1.mycluster GATEWAY=10.0.0.9 (네트워크 구성상의 GW)   [node2]  NETWORKING=yes NETWORKING_IPV6=no HOSTNAME=node2.mycluster GATEWAY=10.0.0.9 (네트워크 구성상의 GW)   3) network 설정 (node1, node2 각각)  vi /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=eth0 BOOTPROTO=static IPADDR=10.0.0.191 (node2의 경우 10.0.0.192) NETMASK=255.255.255.0 ONBOOT=yes HWADDR=시스템설정값사용(VMWare/VirtualBox)   vi /etc/sysconfig/network-scripts/ifcfg-eth1  DEVICE=eth1 BOOTPROTO=static IPADDR=172.16.0.1 (node2의 경우 172.16.0.2) NETMASK=255.255.255.0 ONBOOT=yes HWADDR=시스템설정값사용(VMWare/VirtualBox)   vi /etc/hosts (both)  10.0.0.190 dbip.mycluster dbip 10.0.0.191 node1.mycluster node1 10.0.0.192 node2.mycluster node2   프로그램 설치 (postgreSQL, pacemaker, corosync, DRBD)  1) 관련 패키지 설치 (both)   yum install gcc gcc-c++ make autoconf readline wget readline-devel zlib zlib-devel openssl openssl-devel gettext gettext-devel python python-devel   2) postgres 사용자 생성/설정 (both)  useradd -d /home/postgres postgres passwd postgres su - postgres   vi .bash_profile 아래 내용을 복사하고, 저장   PATH=$PATH:$HOME/bin POSTGRES_HOME=/usr/local/pgsql PGLIB=$POSTGRES_HOME/lib PGDATA=/var/lib/pgsql/data PATH=$POSTGRES_HOME/bin:$PATH  export PATH export POSTGRES_HOME export PGLIB export PGDATA  LD_LIBRARY_PATH=$PGLIB export LD_LIBRARY_PATH   3) postgresql 설치 (both) postgresql 을 공식 사이트로부터 다운로드 하여, /usr/local/src로 복사   tar -jxf postgresql-9.3.4.tar.bz2 cd /usr/local/src/postgresql-9.3.4 ./configure --prefix=/usr/local/pgsql --enable-depend --enable-nls=ko --with-python make make install   4) pacemaker, corosync 설치 (both) CentOS6.x에서는 pacemaker와 corosync 설치를 위해 별도의 yum repository를 추가할 필요가 없다. (이전 버전에서는 필요)   yum install pacemaker corosync   5) DRBD 설치 아래와 같이 ELREPO를 추가한 후,   rpm -Uvh http://mirror.web24.net.au/elrepo/elrepo/el6/x86_64/RPMS/elrepo-release-6-5.el6.elrepo.noarch.rpm yum install drbd84-utils kmod-drbd84 heartbeat   또는  heartbeat-3.0.4-2.el6.x86_64.rpm heartbeat-libs-3.0.4-2.el6.x86_64.rpm kmod-drbd84-8.4.4-1.el6.elrepo.x86_64.rpm drbd84-utils-8.4.4-2.el6.elrepo.x86_64.rpm   을 복사/직접 설치한다.   DRBD 설정  1) DRBD 환경설정 (both)  vi /etc/drbd.conf   You can find an example in  /usr/share/doc/drbd…/drbd.conf.example   와 같은 문구가 보인다. 해당 디렉토리를 찾아가서 보면, drbd.conf.example이 존재하고, 다양한 설정 방법에 대한 예를 볼 수 있다.   include \"drbd.d/global_common.conf\"; include \"drbd.d/*.res\";   라는 문구가 보이는 것처럼, drbd.d 디렉토리 내에, postgres용 환경설정 파일을 만들어놓고 include 시키면 된다.   /etc/drbd.d/postgres.res resource postgres { \tstartup { \t\twfc-timeout 30; \t\toutdated-wfc-timeout 20; \t\tdegr-wfc-timeout 30; \t}  \tnet { \t\tcram-hmac-alg sha1; \t\tshared-secret sync_disk; \t}  \tsyncer { \t\trate 100M; \t\tverify-alg sha1; \t}  \ton node1.mycluster { \t\tdevice      /dev/drbd0; \t\tdisk        /dev/sdb; \t\taddress     172.16.0.1:7791; \t\tmeta-disk   internal; \t}  \ton node2.mycluster { \t\tdevice      /dev/drbd0; \t\tdisk        /dev/sdb; \t\taddress     172.16.0.2:7791; \t\tmeta-disk   internal; \t} }   위 설정은 앞서 추가한 디스크인 /dev/sdb를 각각 논리 디바이스 /dev/drbd0으로 매핑하여 사용하도록 한다. DRBD는 7791 포트를 통해 통신하도록 설정하였으므로, 양 노드에서 방화벽 등록을 해준다.   vi /etc/sysconfig/iptalbes  -A INPUT -m state --state NEW -m tcp -p tcp --dport 7791 -j ACCEPT   2) meta data 생성 (both) 앞서 선언한 resource인 postgres라는 이름으로 meta data를 생성한다.   drbdadm create-md postgres   만약, postgres라는 이름의 리로스가 없다고 뜬다면, 호스트이름과 on 뒤에 선언한 node 이름들이 일치하는지 확인한다. 혹은   drbdadm create-md all  이라고 실행시키면, 선언된 resource들에 대해 메타 데이터를 생성하려 시도하는데, 문제가 되는 부분을 좀 더 자세하게 출력해 준다. (예: on 부분에 선언된 hostname이 없거나, 오타가 의심된다는 등)   즉, on 뒤에 기록한 host이름과   uname -n   으로 조회한 이름이 일치해야 한다. 정상적으로 실행된다면,   Writing meta data... initializing activity log NOT initializing bitmap New drbd meta data block successfully created.   과 같은 메시지가 출력된다.   3) DRBD 서비스 시작  (both)   service drbd start   정상적으로 실행되면, 다음과 같은 메시지가 출력된다.   Starting DRBD resources: [ create res: postgres prepare disk: postgres adjust disk: postgres adjust net: postgres ]   4) Disk Sync (node1) 두 노드를 강제로 동기화 시킨다.   drbdadm -- --overwrite-data-of-peer primary all   양 노드에서 아래와 같이 실행시켜보면, 동기화 진행상태를 확인할 수 있다.   [node1]  cat /proc/drbd   version: 8.4.4 (api:1/proto:86-101) GIT-hash: 599f286440bd633d15d5ff985204aff4bccffadd build by phil@Build64R6, 2013-10-14 15:33:06 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r—n- ns:44960 nr:0 dw:0 dr:48608 al:0 bm:2 lo:0 pe:7 ua:4 ap:0 ep:1 wo:f oos:8345308 [&gt;………………..] sync’ed:  0.6% (8148/8188)M   finish: 0:09:39 speed: 14,336 (14,336) K/sec   [node 2]  cat /proc/drbd   version: 8.4.4 (api:1/proto:86-101) GIT-hash: 599f286440bd633d15d5ff985204aff4bccffadd build by phil@Build64R6, 2013-10-14 15:33:06 0: cs:SyncTarget ro:Secondary/Primary ds:Inconsistent/UpToDate C r—– ns:0 nr:137472 dw:137448 dr:0 al:0 bm:8 lo:3 pe:5 ua:2 ap:0 ep:1 wo:f oos:8250868 [&gt;………………..] sync’ed:  1.7% (8056/8188)M   finish: 0:08:59 speed: 15,272 (15,272) want: 31,720 K/sec 동기화가 일단 100%가 될 때까지 기다린다.   5) DRBD 동기화 간단 테스트 [시나리오]     node1에서 DRBD용 디렉토리 마운트   해당 디렉토리에 임의의 파일 생성   node1을 primary -&gt; secondary로 변경   node2를 secondary -&gt; primary로 변경   DRBD 디렉토리를 마운트하여 확인   [node1]  mkfs.ext4 /dev/drbd0 mkdir /mnt/test mount /dev/drbd0 /mnt/test cd /mnt/test touch a cd .. umount /mnt/test drbdsetup /dev/drbd0 secondary   [node2]  mkdir /mnt/test drbdsetup /dev/drbd0 primary mount /dev/drbd0 /mnt/test cd /mnt/test ls -al   PostgreSQL on DRBD 테스트  1) PostgreSQL 디렉토리 설정 (node1) 각 노드에서 모두 drbd를 시작시킨다.   service drbd start   node1이 primary로 정상적으로 동작하는지 확인한다.  cat /proc/drbd   version: 8.4.4 (api:1/proto:86-101) GIT-hash: 599f286440bd633d15d5ff985204aff4bccffadd build by phil@Build64R6, 2013-10-14 15:33:06 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r—– ns:8388320 nr:0 dw:4 dr:8388997 al:1 bm:512 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 postgresql에서 사용할 디렉토리를 지정/설정한다.   mkdir /var/lib/pgsql mount -t ext4 /dev/drbd0 /var/lib/pgsql chwon postgres.postgres /var/lib/pgsql   2) PostgreSQL DB 초기화 (node1) postgresql DB를 다음과 같이 초기화한다.  su - postgres initdb /var/lib/pgsql/data   다음과 같은 메시지가 출력될 것이다.   The files belonging to this database system will be owned by user “postgres”. This user must also own the server process.   The database cluster will be initialized with locale “en_US.UTF-8”. The default database encoding has accordingly been set to “UTF8”. The default text search configuration will be set to “english”.   Data page checksums are disabled. creating directory /var/lib/pgsql/data … ok creating subdirectories … ok selecting default max_connections … 100 selecting default shared_buffers … 128MB creating configuration files … ok creating template1 database in /var/lib/pgsql/data/base/1 … ok initializing pg_authid … ok initializing dependencies … ok creating system views … ok loading system objects’ descriptions … ok creating collations … ok creating conversions … ok creating dictionaries … ok setting privileges on built-in objects … ok creating information schema … ok loading PL/pgSQL server-side language … ok vacuuming database template1 … ok copying template1 to template0 … ok copying template1 to postgres … ok syncing data to disk … ok   WARNING: enabling “trust” authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or –auth-local and –auth-host, the next time you run initdb.   Success. You can now start the database server using: postgres -D /var/lib/pgsql/data or pg_ctl -D /var/lib/pgsql/data -l logfile start   3) hba(host based authentication) file 편집 node1, node2, db virtual ip을 추가한다.   vi /var/lib/pgsql/data/pg_hba.conf host all all 10.0.0.190/32 trust host all all 10.0.0.191/32 trust host all all 10.0.0.192/32 trust   4) postgresql.conf 편집  vi /var/lib/pgsql/data/postgresql.conf  listen_addresses = '*'   로 변경하고, 주석을 해제한다.   port=5432  의 값을 원하는 포트로 변경하고,주석을 해제한다. (당연히 기본값을 그대로 사용해도 됨)   5) start script 작성 (both)  cp /usr/local/src/postgresql-9.3.4/contrib/start-scripts/linux /etc/init.d/postgresql chmod 775 /etc/init.d/postgresql  postgresql 파일의 내용 중, 디렉토리 등의 설정을 자신의 설치환경에 맞게 변경한다. (예: PGDATA, PGUSER 등)   6) 계정 생성/샘플 데이터 준비 (node1) suser라는 super user 계정을 생성하고, test 데이터베이스를 생성한다.   su - postgres createuser --superuser suser --pwprompt psql -U suser -d postgres postgres=# create database test; postgres=# \\q psql -U suser -d test test=# create table department ( id int primary key not null, dept char(50) not null, emp_id int not null );  test=# insert into department(id, dept, emp_id) values(1, 'sales', 100); test=# select * from department;  id |                        dept                        | emp_id  ----+----------------------------------------------------+--------  1 | sales                                              |    100   test=# \\q   7) PostgreSQL on DRBD 테스트 [node1] postgreSQL을 내린다.   service postgresql stop   DRBD 디바이스를 언마운트시키고, 현재 노드를 primary -&gt; secondary로 전환시킨다.   umount /dev/drbd0 drbdadm secondary postgres   [node2] 위 작업의 역순으로 실행하여, node2에서 postgreSQL이 DRBD 기반으로 동작하는지 확인한다.   drbdadm primary postgres   위 명령어를 수행하기 전에는   cat /proc/drbd   version: 8.4.4 (api:1/proto:86-101) GIT-hash: 599f286440bd633d15d5ff985204aff4bccffadd build by phil@Build64R6, 2013-10-14 15:33:06 0: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r—– ns:0 nr:8437416 dw:8437416 dr:0 al:0 bm:512 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0   수행 후에는   cat /proc/drbd   version: 8.4.4 (api:1/proto:86-101) GIT-hash: 599f286440bd633d15d5ff985204aff4bccffadd build by phil@Build64R6, 2013-10-14 15:33:06 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r—– ns:0 nr:8437416 dw:8437416 dr:664 al:0 bm:512 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0   이제, DRBD device를 마운트하고 postgresql을 시작시킨다.   mkdir /var/lib/pgsql mount -t ext4 /dev/drbd0 /var/lib/pgsql service postgresql start  node1에서 작업한 내용을 확인하자.   su - postgres psql -U suser -d test test=# select * from department;  id |                        dept                        | emp_id  ----+----------------------------------------------------+--------  1 | sales                                              |    100   8) 테스트 환경 원복 앞서 테스트하느라 변경한 상태를 corosync 설정을 하기에 앞서 원래대로 돌려 놓자. 마지막에 테스트를 node2에서 수행했으므로, node2부터 변경한다.   [node2]  service postgresql stop umount /dev/drbd0 drbdadm secondary postgres   [node1]  drbdadm primary postgres   [both] DRBD나 postgreSQL은 이후, Pacemaker를 통해 시작/종료 등 관리할 것이므로, 자동 시작되지 않도록 설정을 변경한다.   service drbd stop chkconfig --level 35 drbd off chkconfig --level 35 postgresql off   Corosync 환경설정  1) 기본 설정 (both)  cp /etc/corosync/corosync.conf/example /etc/corosync/corosync.conf vi /etc/corosync/corosync.conf compatibility: whitetank  totem { \tversion: 2 \tsecauth: off \tthreads: 0 \tinterface { \t\tringnumber: 0 \t\tbindnetaddr: 172.16.0.0 \t\tmcastaddr: 226.94.1.1 \t\tmcastport: 5405 \t\tttl: 1 \t} }   logging { \tfileline: off \tto_stderr: no \tto_logfile: yes \tto_syslog: yes \tlogfile: /var/log/cluster/corosync.log \tdebug: off \ttimestamp: on \tlogger_subsys { \t\tsubsys: AMF \t\tdebug: off \t} }   amf { \tmode: disabled }   aisexec { \tuser: root \tgroup: root }  service { \t# Load the Pacemaker Cluster Resource Manager \tname: pacemaker \tver: 0 }   매뉴얼에 따르면, corosync가 사용하는 multicast 포트 설정은 설정값 - 1을 사용하여, send/receive에 사용한다고 한다. 즉, 5405 포트를 선언하면, 5404 포트도 필요하다. 방화벽 룰에 추가한다.   vi /etc/sysconfig/iptables -A INPUT -m state --state NEW -m udp -p udp --dport 5404 -j ACCEPT -A INPUT -m state --state NEW -m udp -p udp --dport 5405 -j ACCEPT   변경 사항을 저장하고,  mkdir /var/log/cluster serivce iptables restart service corosync start   모든 노드로부터   crm_mon -1   을 실행했을 때,  Last updated: Thu May 15 22:43:41 2014 Last change: Thu May 15 22:41:25 2014 via crmd on node1.mycluster Stack: classic openais (with plugin) Current DC: node1.mycluster - partition with quorum Version: 1.1.10-14.el6-368c726 2 Nodes configured, 2 expected votes 0 Resources configured  Online: [ node1.mycluster node2.mycluster ]  등과 같이 조회되어야 한다.   양 노드에 corosync를 서비스로 항상 실행되도록 설정한다.   chkconfig --level 35 corosync on   Pacemaker 설정  1) crmsh 설치 예전 버전의 pacemaker, corosync 설치를 했다면 crmsh이 기본적으로 설치되어 있을 것이다. 최신 버전의 패키지들은 기본적으로 pcs 를 활용하여, 설치를 가이드하고 있다.   그러나, 아직 레퍼런스가 crm을 사용하는 경우가 많으므로, crm을 설치해서 진행하도록 한다. crm을 써서 pacemaker 리소스를 관리하려면, 별도로 crmsh를 찾아서 설치해 줘야 한다.   wget -P/etc/yum.repos.d/ http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/network:ha-clustering:Stable.repo yum install crmsh.x86_64   와 같이 하거나, 해당 리파지토리로부터  pssh-2.3.1-3.3.x86_64.rpm crmsh-2.0+git46-1.1.x86_64.rpm   을 받아서 직접 설치해 줘야 한다.   2) 클러스터 일반 설정 (node1)   crm configure property stonith-enabled=false crm configure property no-quorum-policy=ignore crm configure rsc_defaults resource-stickiness=100   3) 클러스터 리소스 설정 (node1) 클러스터에 등록하는 FileSystem(DRBD), DBIP, Postgres 등등 모두가 클러스터의 리소스로 인식된다. 이들을 모두 등록한다.   [DBIP]  crm configure primitive DBIP ocf:heartbeat:IPaddr2 params ip=10.0.0.190 cidr_netmask=24 op monitor interval=30s   crm_mon -1   로 등록 상태를 확인하면, 아래와 같이 등록된 리소스를 확인할 수 있다.   Last updated: Fri May 16 01:16:13 2014 Last change: Fri May 16 01:14:57 2014 via cibadmin on pstg04 Stack: classic openais (with plugin) Current DC: node1.mycluster - partition with quorum Version: 1.1.10-14.el6-368c726 2 Nodes configured, 2 expected votes 1 Resources configured  Online: [ node1.mycluster node2.mycluster ]   DBIP   (ocf::heartbeat:IPaddr2):       Started node1.mycluster  [DRBD on cluster] (node1)  이어서, DRBD 설정도 순서대로 진행한다.   crm configure primitive drbd_postgres ocf:linbit:drbd params drbd_resource=\"postgres\" op monitor interval=\"15s\" crm configure ms ms_drbd_postgres drbd_postgres meta master-max=\"1\" master-node-max=\"1\" clone-max=\"2\" clone-node-max=\"1\" notify=\"true\" crm configure primitive postgres_fs ocf:heartbeat:Filesystem params device=\"/dev/drbd0\" directory=\"/var/lib/pgsql\" fstype=\"ext4\"   [PostgreSQL on cluster] (node1) PostgreSQL도 리소스로 등록해준다.   crm configure primitive postgresql ocf:heartbeat:pgsql op monitor depth=\"0\" timeout=\"30\" interval=\"30\"   [Resource Grouping] PostgreSQL과 관련하여 등록한 리소스들을 그룹으로 묶어주고, 순서를 부여한다.   crm configure group postgres postgres_fs DBIP postgresql crm configure colocation postgres_on_drbd inf: postgres ms_drbd_postgres:Master crm configure order postgres_after_drbd inf: ms_drbd_postgres:promote postgres:start crm configure location master-prefer-node1 DBIP 50: node1.mycluster   [Pacemaker 추가 설정] 이 상태에서 곧바로 crm_mon -1 을 수행하면, 설정 과정 중 발생했던 failed actions 메시지들이 그대로 남게 된다.   crm resource cleanup resource명   명령어를 수행하여 해당 fail message를 지워 줄 수 있다. 일단 reboot 하고, 정상적으로 동작하는지 확인한다. 그리고 나서, 정상적으로 동작하면 다행인데, 위의 설정을 그대로 따랐다면, 보통 pacemaker에서 에러가 발생한다.   vi /usr/lib/ocf/resource.d/heartbeat/pgsql  파일을 열어 보자.   OCF_RESKEY_pgctl_default=/usr/bin/pg_ctl OCF_RESKEY_psql_default=/usr/bin/psql  결국, postgreSQL을 설치한 위치가 달라서 pacemaker가 postgreSQL을 시작/중지 등을 시킬 수 없다는 에러가 발생할 것이다. 위 옵션의 경로에 지정된 디렉토리를 실제 설치된 곳으로 변경해주거나, 해당 디렉토리에 symbolic link를 생성해 주자.   crm_mon -1   을 수행하여, 해당 노드/ 리소스가 정상적으로 동작하는지 확인한다.   상태는 아래와 같이  Last updated: Fri May 16 03:27:27 2014 Last change: Fri May 16 01:31:08 2014 via cibadmin on node1.mycluster Stack: classic openais (with plugin) Current DC: node1.mycluster - partition with quorum Version: 1.1.10-14.el6-368c726 2 Nodes configured, 2 expected votes 5 Resources configured  Online: [ node1.mycluster node2.mycluster ]  Master/Slave Set: ms_drbd_postgres [drbd_postgres] Masters: [ node1.mycluster ] Slaves: [ node2.mycluster  ] Resource Group: postgres  postgres_fs        (ocf::heartbeat:Filesystem):    Started node1.mycluster DBIP       (ocf::heartbeat:IPaddr2):       Started node1.mycluster  postgresql (ocf::heartbeat:pgsql): Started node1.mycluster  정상적으로 보이지만, 사실은 DRBD가 정상 상태가 아닐 수도 있다.   cat /proc/drbd   를 통해 두 노드가 정상적으로 동기화 되고 있는지 반드시 확인한다. 상태가 Primary/Secondary 혹은 Secondary/Primary로 출력되는지 확인한다. 만약, Unknown 메시지가 보인다면, DRBD가 동기화 되고 있지 않다는 뜻이다.   4) 동작 확인 일단 동작하는 노드를 확인하려면 다음과 같이 수행한다.   crm_mon -1   VirtualIP로 동작하는 것을 확인하고, Remote로 직접 접속하여, 앞서 수행한 것과 동일한 테스트를 해보자.   psql -h 10.0.0.190 -U suser -d test  test=# select * from test; (node1에서 실행됨)  id |                        dept                        | emp_id  ----+----------------------------------------------------+--------  1 | sales                                              |    100   이 상태에서 primary를 내리면, secondary 노드가 대신 동작하는지 확인한다. primary로부터 (node1)   crm node standby   앞서 맺어놓은 세션은 일단 끊길 것이나,   test=# select * from test;  FATAL: terminating connection due to administrator command FATAL: terminating connection due to administrator command  The connection to the server was lost. Attempting reset: Succeeded.   세션을 다시 맺고 실행하면 정상 동작하는 것을 확인할 수 있다.   test=# select * from test; (node2에서 실행됨)  id |                        dept                        | emp_id  ----+----------------------------------------------------+--------  1 | sales                                              |    100   결과는 동일하지만, 접속한 노드는 node1 → node2로 변경되었다.   5) Spit Braing (DRBD 동기화 깨지는 경우) DRBD의 동기화 상태가 깨진 상태를 Split Brain이라 한다. 보통 데이터 확인 후 수동으로 복구하는 것을 권장하나, 자동 복구를 위한 설정도 가능하다. DRBD에서는 다음과 같은 옵션이 설정 가능하다.      discarding modifications made on the younger primary   discarding modifications made on the older primary   discarding modifications on the primary with fewer changes   graceful recovery from split brain if one host has had no intermediate changes   수동으로 작업하는 경우, 보통 아래의 작업을 수행하면 된다.   drbdadm -- --discard-my-data connect 리소스명 drbdadm disconnect 리소스명 drbdadm connect 리소스명   의 작업을 원하는 노드에서 선별적으로 수행하면, 보통 DRBD 세션은 붙게 된다. 만약, 위의 명령어로도 해결이 안 된다면,   drbdadm invalidate 리소스명   을 수행한 후, 재접속 시도를 시키면 될 것이다. 그러나, 실제 운영환경에서는 위와 같은 명령어를 수행하기에 앞서, 데이터 손실 여부/ 어떤 노드를 기준으로 삼고 동기화를 진행시킬지 적절한 의사 판단이 필요할 것이다.   6) Corosync 채널 이중화 설정  vi /etc/corosync/corosync.conf   rrp_mode : active로 선언하고, interface를 추가로 선언하고, 해당 port 및 port -1 에 대해 방화벽 룰을 추가해 준다.   compatibility: whitetank totem { \tversion: 2 \tsecauth: off \tthreads: 0 \trrp_mode: active \tinterface { \t\tringnumber: 0 \t\tbindnetaddr: 172.16.0.0 \t\tmcastaddr: 226.94.1.1 \t\tmcastport: 5405 \t\tttl: 1 \t}  \tinterface { \t\tringnumber: 1 \t\tbindnetaddr: 10.0.0.0 \t\tmcastaddr: 227.94.1.1 \t\tmcastport: 5407 \t\tttl: 1 \t} }  logging { \tfileline: off \tto_stderr: no \tto_logfile: yes \tto_syslog: yes \tlogfile: /var/log/cluster/corosync.log \tdebug: off \ttimestamp: on \tlogger_subsys { \t\tsubsys: AMF \t\tdebug: off \t} }  amf { \tmode: disabled }  aisexec { \tuser: root \tgroup: root }  service { \t# Load the Pacemaker Cluster Resource Manager \tname: pacemaker \tver: 0 }   7) DRBD의 채널 이중화 가능성 확인한 바로는 DRBD의 통신 채널을 이중화할 수 있는 옵션은 없는 듯 하다. 결국, Corosync 등의 통신 채널을 이중화 하더라도, DRBD 채널에 문제가 생긴다면(split brain) 앞서 설명한 바와 같이 DRBD 동기화는 자동/수동으로 별도로 맞춰야 할 것이다.  ","categories": ["linux"],
        "tags": ["drbd","pacemaker","corosync","postgresql"],
        "url": "http://localhost:4000/linux/drbd-pacemaker-corosync-postgresql-ha/",
        "teaser":null},{
        "title": "Outlook 2013이 프로필 로딩하면서 멈추면?",
        "excerpt":"이 현상을 겪는 사람들이 꽤 있나보다. 아웃룩을 안전모드로 실행시키면 되긴 하는데 (Outlook /safe), 매번 그렇게 쓸 수는 없고, http://triplescomputers.com/blog/casestudies/solution-microsoft-outlook-2013-hangs-at-loading-profile-after-office-update/ 를 참고하여, 레지스트리에 엔트리를 하나 추가해줬더니,   그 이후로는 잘 동작한다.   ","categories": ["system"],
        "tags": ["outlook","hang","profile"],
        "url": "http://localhost:4000/system/outlook-2013-hangs/",
        "teaser":null},{
        "title": "장마, 장마전선",
        "excerpt":"위키피디아로부터 찾아낸 바에 따르면, 장마 = 장(長) + 마(맣; 물) 이라고 한다. 결국, 오래 내리는 비라는 어원을 가지고 있다고 하는데, 뉴스를 보다 장마전선을 영어로 뭐라 하면 좋을지 찾아보았다.   사전에서는, a seasonal rain front 라고 하고 있다. 장마는,   monsoon, rainy season 등으로 쓸 수도 있는데,   사전을 좀 더 찾아보니, rainy spell 이라고도 하는 것을 발견했다. 예전에 한파를 cold spell이라고 했던 것과 비슷하네.   ","categories": ["english"],
        "tags": [],
        "url": "http://localhost:4000/english/rainy-spell/",
        "teaser":null},{
        "title": "맥에서 마우스 스크롤 방향 반대로 하려면",
        "excerpt":"맥에서 마우스를 쓰다보면, 스크롤 방향이 반대인데 윈도우즈만 쓰던 나로서는 여간 헷갈리는 것이 아니다. 역시 검색의 힘!   시스템 환경설정 &gt; 마우스 &gt; 스크롤방향 : 자연스럽게 를 토글해준다. 끝.   ","categories": ["tips"],
        "tags": [],
        "url": "http://localhost:4000/tips/osx-wheel/",
        "teaser":null},{
        "title": "Pacemaker, Corosync,  DRBD 기반으로 구성했는데,  failover가 안된다?",
        "excerpt":"1. 만약, 이전까지 잘 되던 failover가 갑자기 되지 않는다던지, 어느 정도 부하가 걸린 환경에서, failover까지는 정상적으로 수행했으나, failback을 시도했더니 정상적으로 수행되지 않는다면?   2. crm_mon 명령어로 조회하면, 등록했던 리소스에 문제가 있는지/없는지 확인할 수 있다. 때때로, crm_mon 명령어로는 문제점이 나타나지 않는데도 failover가 되지 않는다면? 이럴 떄는 특정 리소스의 failcount가 INFINITY로 바뀌어 있는 경우가 있다. 이 값을 0으로 변경해 줘야 다시 failover가 정상 동작한다.   failcount를 조회하려면 다음과 같이 수행할 수 있다.   crm resource failcount 리소스명 show 노드명   이렇게 조회했더니, 특정 리소스의 failcount가 INFINITY로 나타난다면? 다음과 같이 failcount를 리셋한다.   crm resource cleanup 리소스명   그런데, 에러 메시지를 없애고자, crm resource cleanup 자원명을 수행했는데도, failover가 되지 않는다면?   설정된 모든 노드에서 각각 failcount를 조회해보자. 각 노드에 등록된 리소스들의 failcount들 중, INFINITY가 존재한다면, 해당 노드에서 cleanup을 수행한다. 그리고 나서, 다시 failover 상황에서 검증한다.   ","categories": ["system"],
        "tags": ["pacemaker","corosync","drbd","failover"],
        "url": "http://localhost:4000/system/pacemaker-corosync-drbd-failover-not-working/",
        "teaser":null},{
        "title": "DRBD, Pacemaker, Corosync + PostgreSQL 환경에서 Failover시 Unmanaged라고 뜬다면?",
        "excerpt":"DRBD, Pacemaker, Corosync기반으로 H/A 환경을 구축하고, 그 위에 PostgreSQL을 실행시켰는데 H/A failover가 정상적으로 되는지 확인하기 위해 다음과 같은 테스트를 수행하였다.      Postgres kill 시키기 : auto restart   Virtual IP용 NIC 강제로 ifdown : fail over   crm node standby 명령어 실행을 통한 강제 switch over   위의 테스트를 트래픽이 없거나(적은) 환경에서는 별 문제 없이 수행할 수 있었는데, 트래픽을 많이 발생시킨 (heavy transaction) 환경에서는 기대와는 다르게 동작하는 것을 확인하였다. 그 에러 메시지가 crm_mon 명령어로 확인했을 때, PostgreSQL이 정상적으로 동작하지 않으면서 “Unmanaged” 라고 뜨는 것이었다.   이를 위해 crm resource edit를 하여, PostgreSQL 리소스의 정보를 아래와 같이 수정하였다.      op monitor role=Started timeout=120 interval=30 depth=0 \\         op start role=Stopped interval=0 timeout=120s \\         op stop role=Started interval=0 timeout=120s     모니터링, 시작, 종료시 interval과 timeout을 부여했는데, 특히 timeout의 값을 120초로 주었다.   만약, 이보다 더 작은 값으로 지정한다면 edit 종료시, crm shell에서 사실 “default보다 작은 값을 지정했다”면서 경고 메시지를 준다.   경고를 무시하면, 이와 같이 Unmanaged라는 상황을 만날 수 있다.   교훈) 테스트 환경 구축만으로 잘 되었다고 생각하면, 트래픽이 발생하는 운영 상황에서 auto failover가 안되는 수가 있다.  ","categories": ["system"],
        "tags": ["drbd","pacemaker","corosync","postgresql","failover"],
        "url": "http://localhost:4000/system/drbd-pacemaker-corosync-postgresql-failover-unmanaged/",
        "teaser":null},{
        "title": "OpenVPN-AS와 Google Authenticator/ Authy 연동 설정하기",
        "excerpt":"알고보니, OpenVPN-AS의 경우 Google Authenticator/ Authy 연동을 쉽게 할 수 있었다.   Authentication 메뉴에     General   PAM   RADIUS   LDAP   가 있길래, RADIUS 혹은 PAM으로 할까 했는데, 의외로 메뉴가 숨어 있었다.   Configuration &gt; Client Settings 메뉴로부터 설정 가능하다.      과 같은 체크박스가 있고, 체크해 주면 된다. 그리고, VPN 서버에 반영 혹은 재시작 시키자.   이것만으로는 접속시 OTP를 사용할 수 없다. (아직 OTP 등록도 하지 않았으니까)   기존에 OTP 설정 전에 받아 놓은 opvn 프로파일을 사용하여 VPN 접속을 시도하면, OTP를 묻지도 않고, 기존 방식과 동일하게 인증하고 접속시켜 버린다.   OTP 설정을 위해, 처음의 절차를 다시 밟자. 클라이언트로부터 https://주소:943을 접속해보면, 이전과는 다른 화면을 볼 수 있다.   ovpn 프로파일을 받는 부분 하단으로, google authenticator 안내문과 QR코드 그리고 직접 입력할 경우를 대비한 코드가 보일 것이다.   이 정보를 활용하여, QR코드로 찍든 코드를 직접 입력하든 선택하자. 이 단계에서 google authenticator를 사용하거나, authy를 사용하거나 상관없다. 내 경우는 authy를 사용해서 테스트 했고, 정상적으로 등록 가능했다.   그리고, 다시 OVPN 파일을 클라이언트로에 다운로드 하자. OTP 설정 이전의 프로파일은 클라이언트로부터 삭제하자.   새로 받은 프로파일을 사용하여 VPN 접속을 시도하면, 이제 기존의 id/password와 함께 OTP도 함께 묻는 것을 확인할 수 있다.  ","categories": ["system"],
        "tags": ["openvpn","authy","google","otp"],
        "url": "http://localhost:4000/system/openvpn-google-authenticator-authy/",
        "teaser":null},{
        "title": "Eclipse/STS에서 maven 프로젝트 UTF-8, JDK 버전을 매번 해줘야 하나?",
        "excerpt":"Eclipse/STS에 사용중인 maven project로부터      프로젝트를 새로 import할 때   Disable Maven Nature + Configure &gt; Convert to Maven Project를 할 때   인코딩이 MS949로 자꾸 바뀌어서 수동으로 UTF-8로 바꿔줘야 했다면? 또 JDK 버전이 자꾸 1.5로 바뀌어서 수동으로 JDK 버전을 상위 버전으로 바꿔줘야 했다면? 다음의 옵션을 pom.xml에 추가해 보자.   pom.xml 파일 윗쪽에   &lt;properties&gt; \t&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt;   와 같이 이 프로젝트는 UTF-8 인코딩이라고 선언해 주자. pom.xml의 build 부분에 (없다면 추가로 써주면 된다) 아래와 같이 maven-compiler-plugin 선언을 하고, 사용하고자 하는 JDK 버전을 명시한다.   &lt;build&gt; \t&lt;plugins&gt; \t\t&lt;plugin&gt; \t\t\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; \t\t\t&lt;configuration&gt; \t\t\t\t&lt;source&gt;1.7&lt;/source&gt; \t\t\t\t&lt;target&gt;1.7&lt;/target&gt; \t\t\t&lt;/configuration&gt; \t\t\t&lt;version&gt;2.3.2&lt;/version&gt; \t\t\t&lt;executions&gt; \t\t\t\t&lt;execution&gt; \t\t\t\t\t&lt;id&gt;default-testCompile&lt;/id&gt; \t\t\t\t\t&lt;phase&gt;test-compile&lt;/phase&gt; \t\t\t\t\t&lt;goals&gt; \t\t\t\t\t\t&lt;goal&gt;testCompile&lt;/goal&gt; \t\t\t\t\t&lt;/goals&gt; \t\t\t\t&lt;/execution&gt; \t\t\t\t&lt;execution&gt; \t\t\t\t\t&lt;id&gt;default-compile&lt;/id&gt; \t\t\t\t\t&lt;phase&gt;compile&lt;/phase&gt; \t\t\t\t\t&lt;goals&gt; \t\t\t\t\t\t&lt;goal&gt;compile&lt;/goal&gt; \t\t\t\t\t&lt;/goals&gt; \t\t\t\t&lt;/execution&gt; \t\t\t&lt;/executions&gt; \t\t&lt;/plugin&gt; \t&lt;/plugins&gt; &lt;/build&gt;  이렇게 해서, SCM에 commit 하면, 새로 프로젝트를 import 할 경우와, aven Feature를 껐다 켤 때에도 인코딩과 JDK 버전이 설정한 대로 유지된다.   ","categories": ["development"],
        "tags": ["eclipse","java","maven","sts"],
        "url": "http://localhost:4000/development/eclipsests-maven-utf-8-jdk-configuration/",
        "teaser":null},{
        "title": "PPAS에서 Oracle로 DB Link 연결하다 겪은 에러 - ORA-21561 : OID generation failed",
        "excerpt":"PPAS에서 Oracle로 DB Link를 연결하였고, 연결한 상태에서   SELECT * FROM xx@ora_link;   와 같이 실행했는데,   ERROR:  OCI error: ORA-21561: OID generation failed  ********** Error ********** ERROR: OCI error: ORA-21561: OID generation failed SQL state: 25000   와 같은 에러가 발생하였다.   문제를 확인하기 위해, SQLPlus를 사용하여 동일한 문제가 발생하는지 다음과 같이 확인하였다.   sqlplus username/password@servicename   그랬더니, 동일한 에러가 발생하였다. 결국, PPAS 자체의 문제가 아니라 Oracle client가 접속할 때 생기는 문제. 인터넷을 찾아보고 다음과 같이 조치/ 해결하였다.   hostname   를 실행하여, 호스트이름을 확인한다. /etc/hosts내에 다음의 내용을 추가한다.   127.0.0.1 localhost 호스트이름   그 후, sqlplus로 접속 시도를 했더니 정상적으로 연결이 되었으며, 앞서 생성한 DBLINK를 통한 연산도 정상적으로 수행되었다.   ","categories": ["system"],
        "tags": ["dbms","ppas"],
        "url": "http://localhost:4000/system/ppas-oracle-db-link-ora-21561-oid-generation-failed/",
        "teaser":null},{
        "title": "커맨드라인에서 Maven/java 웹 프로젝트를 Tomcat에  배포하려면?",
        "excerpt":"java 웹 애플리케이션을 Tomcat에 배포하려면 여러 가지 방법이 있다.     웹 애플리케이션 디렉토리 전체를 webapps 밑에 직접 복사하거나,   웹 애플리케이션 디렉토리를 압축한 WAR 파일을 webapps 밑에 직접 복사하거나,   Tomcat의 매니저 웹 GUI를 통해, WAR 파일을 배포할 수 있다.   그러나, 직접 특정 디렉토리에 복사하는 방법은 무엇보다 번거롭다. Tomcat의 매니저 웹을 통해 배포하는 것은 쉽지만, 자동으로 배포를 하고자 한다면 매번 사용하기 어렵다.   다음과 같은 경우, Cargo라는 프로그램/ 플러그인을 사용하면 좋을 것 같다.     커맨드라인으로부터 웹 애플리케이션을 배포하고 싶다.   WAS가 로컬/리모트로 접근 가능하다.   Ant/ Maven을 사용하고 있다.   WAS가 다른 것으로 대체될 가능성, 혹은 서로 다른 WAS가 내 웹 애플리케이션에 혼용될 수도 있다.   그러나, 동일한 방법으로 웹 애플리케이션을 여전히 배포하고 싶다. (커맨드라인으로부터)   Cargo에 대한 정확하고, 자세한 정보는 무엇보다 공식 홈페이지(http://cargo.codehaus.org)를 참조하는 것이 바람직하다.   홈페이지의 What is CARGO?란에서는 다음과 같이 Cargo를 소개하고 있다. Cargo is a thin wrapper that allows you to manipulate various type of application containers (Java EE and others) in a standard way.   주목할 만한 것은 결국, various type of application containers in a standard way라고 보면 될 듯 하다.       Cargo가 지원하는 Containers   Geronimo 1.x ~ 3.x   Glassfish 2.x ~ 4.x   JBoss 3.x ~ 7.4.x   Jetty 4.x ~ 9.x   Jo! 1.x   JOnAS 4.x ~ 5.x   JRun 4.x   Orion/OC4J 9.x ~ 10.x   Resin 2.x ~ 4.x   Tomcat 4.x ~ 8.x   TomEE 1.x   WebLogic 8.x ~ 12.1.x   WebSphere 8.5.x        WildFly 8.x       Standard way = Cargo의 사용법 + container 별 설정 앞서 언급한 다양한 WAS들은 결국 몇몇 property를 선언하는 환경 설정의 문제로 국한된다. 본 글에서는 Maven 기반 웹 애플리케이션을 Tomcat7에 배포하는 설정을 간단하게 적어보고자 한다.      Tomcat 설정 Tomcat에는 기본적으로 권한 및 룰이 등록되어 있지 않기 때문에 Cargo를 사용하여 웹 애플리케이션을 배포하려면 그에 필요한 내용을 선언해야 한다.   Tomcat의 설치 디렉토리를 $CATALINA_HOME이라 할 때, $CATALINA_HOME/conf/tomcat-users.xml을 편집하여, 다음의 내용을 추가한다.   &lt;role rolename=\"manager-script\" /&gt; &lt;user username=\"deploy\" password=\"deploy\" roles=\"manager-script\" /&gt;   GUI 기반이 아닌, CLI 기반으로 제어하기 위해, tomcat에 manager-script 롤을 선언한다. 배포에 사용할 계정 및 패스워드를 선언하고 (위의 예에서는 deploy/deploy), 이 계정에 앞서 선언한 manager-script 롤을 부여한다.      Maven 설정 Cargo 홈페이지상의 Downloads 탭으로부터 다운로드를 받을 수도 있으나, Maven 기반으로 설정할 경우는, 본문에 표기된 바와 같이   If you want to use Cargo from Maven 2 or Maven 3, you don’t need to install anything at all as Maven will automatically download the required jars when you first use the plugin.   저절로 복사가 된다고 하였으나, 저절로 복사가 되도록 설정을 해줘야 한다. http://repo1.maven.org/maven2/org/codehaus/cargo/cargo-maven2-plugin 에 접속하여 확인하면,   2014/7/28에 배포된 1.4.9가 이 글을 작성하는 시점 기준으로 가장 최신버전이다. 본 글은, 예전에 1.2.4로 테스트 환경을 구축한 것을 바탕으로 작성하였기에, 아래 설정에는 1.2.4로 기록하였다. (1.4.9로 적어도 동일하게 동작할 것이라 예상한다.)   pom.xml에 cargo-maven2-plugin 설정을 하자.  &lt;plugin&gt;     &lt;groupId&gt;org.codehaus.cargo&lt;/groupId&gt; \t&lt;artifactId&gt;cargo-maven2-plugin&lt;/artifactId&gt; \t&lt;version&gt;1.2.4&lt;/version&gt; \t&lt;configuration&gt; \t\t&lt;container&gt; \t\t\t&lt;containerId&gt;tomcat7x&lt;/containerId&gt; \t\t\t&lt;type&gt;remote&lt;/type&gt; \t\t&lt;/container&gt; \t\t&lt;configuration&gt; \t\t\t&lt;type&gt;runtime&lt;/type&gt; \t\t\t&lt;properties&gt; \t\t\t\t&lt;cargo.remote.uri&gt;http://tomcat_ip:tomcat_port/manager/text&lt;/cargo.remote.uri&gt; \t\t\t\t&lt;cargo.remote.username&gt;username&lt;/cargo.remote.username&gt; \t\t\t\t&lt;cargo.remote.password&gt;password&lt;/cargo.remote.password&gt; \t\t\t&lt;/properties&gt; \t\t&lt;/configuration&gt;  \t\t&lt;deployer&gt; \t\t\t&lt;type&gt;remote&lt;/type&gt; \t\t\t&lt;deployables&gt; \t\t\t\t&lt;deployable&gt; \t\t\t\t\t&lt;groupId&gt;com.luran&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;jsweb&lt;/artifactId&gt; \t\t\t\t\t&lt;type&gt;war&lt;/type&gt; \t\t\t\t\t&lt;properties&gt; \t\t\t\t\t\t&lt;context&gt;/jsweb&lt;/context&gt; \t\t\t\t\t&lt;/properties&gt; \t\t\t\t&lt;/deployable&gt; \t\t\t&lt;/deployables&gt; \t\t&lt;/deployer&gt; \t&lt;/configuration&gt; &lt;/plugin&gt;      cargo.remote.uri 에 Tomcat manager 주소를 http://ip:port/manager/text로 기록   cargo.remote.username에 배포를 위한 계정 기록   cargo.remote.password에 배포를 위한 계정의 비밀번호 기록   deployable이하 내용에는 어떤 war로 배포할 것인지 (maven 형식) 기록   properties~context에는, servlet context 명 기록 (본 예제에서는, http://ip:port/jsweb으로 배포함)      Maven을 통한, 웹 애플리케이션 관리            deploy         mvn cargo:deploy                           또는   mvn cargo:deployer-deploy      undeploy     mvn cargo:undeploy           또는  mvn cargo:deployer-undeploy      redeploy     mvn cargo:redeploy           또는  mvn cargo:deployer-redploy      자동 배포 응용 (빌드 서버 또는 스크립트 상)   mvn clean install package cargo:redploy   이와 같은 식으로 작성한다면, 빌드까지 모두 한 후 Cargo를 통해 Tomcat으로 웹 애플리케이션을 자동 배포할 수 있다.   ","categories": ["development"],
        "tags": ["deploy","java","maven","cargo","tomcat"],
        "url": "http://localhost:4000/development/deploy-tomcat-application-by-commandline/",
        "teaser":null},{
        "title": "iCloudDrive + Good Reader를 통한 지속적인 읽기",
        "excerpt":"   PDF 같은 문서를 즐겨 본다.   아이폰과 아이패드를 모두 사용한다.   보통 때는 아이패드로 문서를 보지만, 이동시에는 아이폰으로라도 먼저 보던 문서를 계속 이어서 보고 싶다.   문서를 보다가 문서에 무언가 기록을 하거나, 밑줄을 긋기도 한다.   아이폰  아이패드로 전환하더라도, 표기한 내용이 그대로 보존되었으면 좋겠다.   이와 같은 시나리오에서 사용할 수 있는 해결방법이라 생각하면 될 듯하다. 이 시나리오를 딱히 표현할 말이 떠오르지 않아, 지속적인 읽기(continuous reading)이라 멋대로 이름짓는다. 지속적인 읽기를 위해 필요한 것은 결국 ‘동기화’다.   Good Reader 사용 중, export to iCloud메뉴를 발견하였는데, 반대로 import from iCloud 메뉴가 보이지 않아 Good Reader에 메일로 문의했고, 그에 대한 답변을 받았다.   이 기능을 사용하려면 다음의 조건만 만족하면 된다.     iOS 8.1.3이상을 쓰고,   iCloud Drive 기능을 활성화   Good Reader 4 이상을 사용할 것   사용하려면, Good Reader &gt; Settings &gt; General &gt; Use iCloud 스위치를 켠다. 그러면, My documents 섹션 아래, iCloud Folder가 생긴다. 이후, 동기화하고 싶은 문서를 이곳에 복사하면, 그 컨텐츠는 자동으로 동기화 된다.  같은 계정을 사용하는 iOS디바이스에서도 동일하게 설정하면, 읽고 있는 pdf를 효율적으로 지속적인 읽기를 할 수 있다.   열심히 책 보자!   ","categories": ["reading"],
        "tags": [],
        "url": "http://localhost:4000/reading/iclouddrive-good-reader-continuous-reading/",
        "teaser":null},{
        "title": "Redis Study Note Single Redis Instance",
        "excerpt":"layout: single title: Redis Study Note - single 인스턴스 설치 date: 2016-09-17 14:53:32.000000000 +09:00 categories:          system tags: [redis] author: login: everydayminder email: 2jhyun@gmail.com display_name: everydayminder first_name: ‘’ last_name: ‘’ — Redis single instance를 다음과 같이 설치해 본다.       OS : CentOS 6.8 / VirtualBox   Redis 버전 : 3.2.3   공식사이트 Redis 공식 사이트 : http://redis.io   다운로드 2016/9/16현재 최신 stable버전인 3.2를 다운로드 한다.   설치 공식 사이트에 안내된 바와 같이, 단일 인스턴스 설치 과정 자체는 무척 간단하다.   http://download.redis.io/releases/redis-3.2.3.tar.gz   에서 다운로드하고,   $ tar xzf redis-3.2.3.tar.gz $ cd redis-3.2.3 $ make   이대로 설치를 진행하면, gcc부터 시작하여 여러 라이브러리가 없다고 줄줄이 에러가 발생한다. README.md파일을 읽어보면, 의존성 라이브러리들이 deps 디렉토리에 존재한다는 것을 알 수 있다. 일단 다음과 같이 실행한다. (추후 다른 라이브러리들도 필요할 것 같아, 일단 epel-release도 함께 설치하였다.)   $ yum install epel-release $ yum install gcc   설치시 의존성 문제가 발생하면, 아래와 같이 초기화하고 다시 설치해 본다.   $ make distclean   정상적으로 설치를 마쳤다면, 다음과 같은 메시지가 눈에 띌 것이다.  Hint: It's a good idea to run 'make test' ;)  테스트를 실행해보면, 이번에는 tcl을 설치하라고 한다.  $ yum install tcl  tcl을 설치 후, 이제 make test를 실행하면  $ make test  수많은 테스트를 알아서 수행한다.   Execution time of different units:   1 seconds - unit/type/incr   1 seconds - unit/printver   2 seconds - unit/auth   2 seconds - unit/keyspace   0 seconds - unit/quit   3 seconds - unit/scan   2 seconds - unit/multi   7 seconds - unit/protocol   11 seconds - unit/expire   23 seconds - unit/type/list   11 seconds - integration/aof   4 seconds - integration/rdb   3 seconds - integration/convert-zipmap-hash-on-load   2 seconds - integration/logging   1 seconds - unit/pubsub   2 seconds - unit/slowlog   46 seconds - unit/aofrw   51 seconds - unit/other   1 seconds - unit/introspection   57 seconds - unit/type/hash   2 seconds - unit/limits   58 seconds - integration/replication   7 seconds - unit/introspection-2   19 seconds - unit/scripting   10 seconds - unit/bitfield   72 seconds - unit/type/string   81 seconds - unit/type/set   38 seconds - unit/bitops   96 seconds - integration/replication-2   102 seconds - unit/type/zset   63 seconds - unit/maxmemory   117 seconds - unit/dump   127 seconds - unit/sort   58 seconds - unit/memefficiency   121 seconds - integration/replication-psync   86 seconds - unit/geo   156 seconds - unit/type/list-2   159 seconds - unit/type/list-3   155 seconds - integration/replication-4   160 seconds - integration/replication-3   92 seconds - unit/hyperloglog   114 seconds - unit/obuf-limits  \\o/ All tests passed without errors!  Cleanup: may take some time... OK   make까지 성공했으면 src/redis-server로 실행가능한 상태가 되었을 것이다. 아래와 같이 실행하면, /usr/local/bin에 redis-server, cli 파일 등이 복사된다.   $ make install   환경설정 파일 등의 절차를 쉽게 하도록, utils/install_server.sh를 실행한다. 포트, 로그 위치, config에 관련된 정보/ 위치에 대해 물으나, 기본 설정으로 모두 그대로 enter를 쳤고, executable의 위치는 다른 가이드에 나온대로 기본 인식이 되지 않아, /usr/local/bin/redis-server를 직접 입력하였다.   redis 기본 포트가 6379라, /etc/init.d/redis_6379 파일이 생성되었다. 아래의 명령어로 서비스를 시작/ 정지시킬 수 있다.   $ service redis_6379 start $ service redis_6379 stop   redis가 정상적으로 떠 있는 상태라면,  $ ps -ef | grep redis  root     22694     1  0 00:25 ?        00:00:05 /usr/local/bin/redis-server 127.0.0.1:6379  이제, redis client로 접속하면,  $ /usr/local/bin/redis-cli 127.0.0.1:6379&amp;gt;  와 같이 접속 상태가 안내되고, 다음과 같이 기본 동작 상태를 확인할 수 있다.   27.0.0.1:6379&amp;gt; set test 1234 OK  127.0.0.1:6379&amp;gt; set name &amp;quot;abc def&amp;quot; OK  127.0.0.1:6379&amp;gt; exists name (integer) 1  127.0.0.1:6379&amp;gt; get name &amp;quot;abc def&amp;quot;  127.0.0.1:6379&amp;gt; get test &amp;quot;1234&amp;quot;   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/redis-study-note-single-redis-instance/",
        "teaser":null},{
        "title": "OSX VirtualBox의 host에서 guest linux(CentOS)로 ssh 접속 설정하려면?",
        "excerpt":"윈도우즈 기반 VirtualBox 환경에서는,     게스트의 네트워크를 브릿지 네트워크로 설정하거나   NAT로 설정하고, 포트포워딩 설정을 VirtualBox에서 해줘서 ssh 접속하는 방식으로 사용했다.   OSX에서도 물론 방식 1로 하면 이상없이 사용할 수 있다. 브릿지 방식이 아닌, 호스트 네트워크로 설정해 보자.   어댑터2를 새로 추가하고 호스트 전용 어댑터를 선택하면, “잘못된 설정 감지됨” 에러가 뜬다. 이를 해결하기 위해,   VirtualBox &gt; 환경설정 &gt; 네트워크 &gt; 호스트 전용 네트워크 탭 선택 후,   (http://localhost:4000/images/201609/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2016-09-17-e1848be185a9e18492e185ae-6-54-09.png) 우측의 추가 버튼을 클릭한다.   (http://localhost:4000/images/201609/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2016-09-17-e1848be185a9e18492e185ae-6-54-45.png) 우측의 추가 버튼을 클릭한다. 그러면, vboxnet0이 추가되고, 다시 이전의 vm 인스턴스 설정 화면에서 네트워크 설정을 변경하면, 이제 호스트 전용 네트워크 (어댑터2)를 선택하더라도 잘못된 설정이라고 뜨지 않는다.   VM 인스턴스 시작 후, 네트워크 설정을 원하는대로 마친 후 호스트에서 게스트로 직접 ssh 연결을 시도하면 될 것이다.   ","categories": ["system"],
        "tags": ["virtualbox","ssh"],
        "url": "http://localhost:4000/system/osx-virtualbox-host-guest-linuxcentos-ssh/",
        "teaser":null},{
        "title": "docker + postgresql",
        "excerpt":"테스트를 위해 간단하게 써보고자 할 때, 굳이 번거롭게 설치하지 말고 docker를 써보자. 다음의 명령어만으로도, docker 기반으로 postgres를 쓸 수 있다.   docker run -p 5432:5432 --name postgres -e POSTGRES_PASSWORD=postgres -d postgres   그런데, 띄우고 나서도 직접 container에 들어가서 psql도 직접 접속해보고, \\dt 도 해보고 싶을 수도 있잖아?   그럴 때는, 직접 container에 다음과 같이 접속하자.   docker exec -it postgres bash   ","categories": ["system"],
        "tags": ["docker","postgres"],
        "url": "http://localhost:4000/system/docker-postgresql/",
        "teaser":null},{
        "title": "OSX에서 스크린캡처 경로 변경하려면",
        "excerpt":"터미널에서,   defaults write com.apple.screencapture location 변경경로 killall SystemUIServer  ","categories": ["system"],
        "tags": ["tips","osx"],
        "url": "http://localhost:4000/system/osx-screen-capture-path-change/",
        "teaser":null},{
        "title": "windows10에 bash 설정",
        "excerpt":"제어판 검색 창으로부터, “개발자 기능 사용”을 입력한다. 개발자용 &gt; 개발자 기능 사용 중, “개발자 모드”를 선택한다. 다시 검색 창에서, Linux용 Windows 하위 시스템을 선택한다.   cmd를 관리자 권한으로 실행 후,   lxrun /install   를 입력한다. 일정 시간 경과 후, 만들 계정 (username, password)을 설정하면 완료된다.   ","categories": ["system"],
        "tags": ["windows","bash"],
        "url": "http://localhost:4000/system/windows10-bash/",
        "teaser":null},{
        "title": "windows10에 docker 간편 설치하기",
        "excerpt":"Windows용 Docker를 설치하기 위해, https://www.docker.com/community-edition 를 방문하여, Docker Community Edition for Windows를 찾아 다운로드 설치한다.   재부팅 및 업데이트를 마치고 나서, 태스크 바를 보면, 고래 아이콘을 발견할 수 있을 것이다. 아이콘에 커서 올리고, 마우스 우클릭 하면 Kitematic이라는 버튼을 누르면, 최초 실행시 Kitematic을 다운로드 하라고 안내 창이 뜬다.   다운로드 받아서, 압축을 해제한 후 Docker가 설치된 곳으로 복사해준다. 다시 Docker &gt; Kitematic을 실행시키면 이미지를 쉽게 다운로드하고 설치할 수 있다.      이제 원하는 이미지를 쉽게 실행시킬 수 있다.   ","categories": ["system"],
        "tags": ["windows","docker"],
        "url": "http://localhost:4000/system/windows-docker-installation/",
        "teaser":null},{
        "title": "Docker용 Jenkins 설치/ 기본 환경설정",
        "excerpt":"Docker - Kitematic으로부터 Jenkins 이미지를 클릭하여 Jenkins를 쉽게 설치할 수 있다.    일단, 설치 후 실행 시키면, 다음과 같은 창이 뜬다.    이후, General, Hostname/Ports, Volumes, Network, Advanced 등의 탭으로부터 정보를 확인하거나, 원하는 값으로 설정을 변경하여 사용할 수 있다. Hostname/Ports 탭은 아래와 같이 나오는데,    로컬서버의 32769 포트로 접속하면, Jenkins에 접속가능하다는 뜻이다. 이 주소로 접속해 보면, 다음과 같은 초기화면이 뜬다.    화면에 뜬 바와 같이, unlock 정보를 확인해보자.   /var/jenkins_home/secrets/initialAdminPassword  내용을 확인해보자. 앞서 확인한 Kitematic의 관리 패널에서 EXEC를 클릭한다.      콘솔 창이 뜨면,  cat /var/jenkins_home/secrets/initialAdminPassword  를 실행하고, 결과값을 복사하여 Jenkins를 unlock 하는 창에 복사해 넣자. unlock이 성공하면 다음과 같이 창이 뜰 것이다.      우선 Install suggested plugins를 클릭하여, 권장 플러그인들을 설치하자.    계정 정보까지 모두 설정 완료하면, 기본 설치는 완료된다.    ","categories": ["system"],
        "tags": ["docker","jenkins","windows","kitematic"],
        "url": "http://localhost:4000/system/docker-jenkins-installation/",
        "teaser":null},{
        "title": "local tunneling을 통해 외부로부터 접속 허용하기",
        "excerpt":"SSH tunneling을 통한 port forwarding을 하고자, putty나 secure crt로 설정하고 썼다. 혹은 집의 내부 서버에 DNSEver 같은 DDNS 서비스랑 연결하고, 내부에서 포트 포워딩을 하거나. 그러다가, 손쉽게 local tunneling을 구축할 수 있는 서비스를 찾아보게 되었고, 간단하게 비교해 보았다.   ngrok : https://ngrok.com/   NAME: ngrok - tunnel local ports to public URLs and inspect traffic  DESCRIPTION: ngrok exposes local networked services behinds NATs and firewalls to the public internet over a secure tunnel. Share local websites, build/test webhook consumers and self-host personal services. Detailed help for each command is available with 'ngrok help &lt;command&gt;&lt;/command&gt;'. &lt;command&gt;&lt;/command&gt; Open http://localhost:4040 for ngrok's web interface to inspect traffic.  EXAMPLES: ngrok http 80 # secure public URL for port 80 web server ngrok http -subdomain=baz 8080 # port 8080 available at baz.ngrok.io ngrok http foo.dev:80 # tunnel to host:port instead of localhost ngrok tcp 22 # tunnel arbitrary TCP traffic to port 22 ngrok tls -hostname=foo.com 443 # TLS traffic for foo.com to port 443 ngrok start foo bar baz # start tunnels from the configuration file  VERSION: 2.2.8  AUTHOR: inconshreveable - &lt;alan@ngrok.com&gt;  COMMANDS: authtoken save authtoken to configuration file credits prints author and licensing information http start an HTTP tunnel start start tunnels by name from the configuration file tcp start a TCP tunnel tls start a TLS tunnel update update ngrok to the latest version version print the version string help Shows a list of commands or help for one command    위에서 보는 바와 같이, localhost:32769를 dd8b0a83.ngrok.io로 연결시켜준다. 문제는, 실행할 때마다 저 subdomain이 매번 바뀐다는 점.   -subdomain 옵션을 사용하면, subdomain을 원하는 고정 값으로 사용할 수 있으나, 유료.   장점)     사용하기 쉽다.     ngrok http 8080          이라고만 하면, 곧바로 설정된다.       실행하면, 곧바로 console 창에 사용중인 트래픽이 뜨기도 하고, 별도의 웹으로도 조회가 가능하다.   단점)     무료 버전의 경우, 분당 40 요청만 처리 가능하다.   저 주소가 실행할 때마다 바뀐다. 못 외움. 일시적으로 잠깐 쓰기는 좋음   LocalTunnel : https://localtunnel.github.io/www/  [num]  Options: -h, --host Upstream server providing forwarding [default: \"http://localtunnel.me\"] -s, --subdomain Request this subdomain -l, --local-host Tunnel traffic to this host instead of localhost, override Host header to this host -o, --open opens url in your browser -p, --port Internal http server port [required] --help Show this help and exit [boolean] --version Show version number [boolean]  위의 옵션을 사용하여, 앞서 ngrok를 설정한 것처럼 다음과 같이 띄울 수 있다.   lt -s mytest -p 32769   그러면, https://mytest.localtunnel.me로 해당 매핑이 설정된다! 무료로!   장점)     무료   별도 유료 정책 등 신경쓸게 없다.   서브도메인을 쉽게 설정할 수 있다.   단점)     다소 느린 듯?   npm을 깔아야 한다.   pagekite : https://pagekite.net/     트라이얼 기간동안 일정 사용량이 있다. 설치하면, 사용할 subdomain을 입력하라고 안내한다.   장점)     python 2.7 기준으로 동작한다.   소스까지 무려 공개한다.   localtunnel 보다 빠른 것 같다.   서브 도메인을 쉽게 지정할 수 있고, 옵션이 많다.   단점)     python 3.x에서는 안된다고 한다.   빠른데는 이유가 있다. quota 제도임  ","categories": ["system"],
        "tags": ["local","tunneling"],
        "url": "http://localhost:4000/system/local-tunneling/",
        "teaser":null},{
        "title": "OSX) virtualenvwrapper로 virtualenv 편하게 쓰기",
        "excerpt":"python 개발 환경 설정 중, virtualenv를 쓰다 매번 activate를 하기 위해 설정위치/bin/activate로 활성화하고, deactivate로 비활성화 하는 것이 번거로웠는데, 이를 좀더 쉽게 도와주는 virtualenvwrapper를 알게 되었다.      설치   sudo pip install virtualenv virtualenvwrapper    로 하면 될텐데, 내 경우는 패키지 설치도중 permission 에러가 발생하여,   sudo pip install --ignore-installed virtualenv virtualenvwrapper   로 설치완료했다.      환경 설정   mkdir ~/virtualenvs   .bash_profile 또는 .zshrc 등과 같은 본인 SHELL 설정 파일에   export WORKON_HOME=~/virtualenvs source /usr/local/bin/virtualenvwrappper.sh  를 추가한다.   설정 완료 후, iterm 등 shell을 열어서,   echo $WORKON_HOME lsvirtualenv   등과 같은 명령을 실행시켜서 정상 설정 여부를 확인하자.      주요 명령어   virtualenvwrapper   라고 실행시키면, 다음과 같이 주요 명령어 리스트가 출력된다.     add2virtualenv: add directory to the import path   allvirtualenv: run a command in all virtualenvs   cdproject: change directory to the active project   cdsitepackages: change to the site-packages directory   cdvirtualenv: change to the $VIRTUAL_ENV directory   cpvirtualenv: duplicate the named virtualenv to make a new one   lssitepackages: list contents of the site-packages directory   lsvirtualenv: list virtualenvs   mkproject: create a new project directory and its associated virtualenv   mktmpenv: create a temporary virtualenv   mkvirtualenv: Create a new virtualenv in $WORKON_HOME   rmvirtualenv: Remove a virtualenv   setvirtualenvproject: associate a project directory with a virtualenv   showvirtualenv: show details of a single virtualenv   toggleglobalsitepackages: turn access to global site-packages on/off   virtualenvwrapper: show this help message   wipeenv: remove all packages installed in the current virtualenv   workon: list or change working virtualenvs  이 help에 안내된 바와 같이, 설정을 만들 때는, mkvirtualenv를 사용하고, 설정한 프로파일로 변경할 때는 workon을 사용하고, 빠져나올 때는 기존처럼 deactivate를 하면 된다.   mkvirtualenv: Create a new virtualenv in $WORKON_HOME   라고 되어 있듯이,설정을 해당 디렉토리에 모아서 생성해 주는 일을 한다. 결국, $WORKON_HOME으로 이동해서 source $WORKON_HOME/설정프로파일 디렉토리/bin/activate를 직접 실행시켜도 동일하게 동작한다. (원래 virtualenv 사용법)   예를 들어 python3 디렉토리에 python3 설정을 해뒀다면,   source python3/bin/activate (개발) deactivate   라고 작업하는 것과 다음 작업이 동일하다.   workon python3 (개발) deactivate   ","categories": ["development"],
        "tags": ["python","virtualenv","virtualenvwrapper"],
        "url": "http://localhost:4000/development/osx-virtualenvwrapper-virtualenv/",
        "teaser":null},{
        "title": "virtualenvwrapper를 사용한 python3 virtualenv 설정",
        "excerpt":"보통 python 2.7.x가 설치가 되어 있고, python3은 별도 설치해야 하는데, 여전히 ver 2.x에 대한 의존도가 있어서, 별도 개발환경을 3.x로 맞추고 싶다.   brew install python3   로 osx에 python3을 설치하자. 설치를 완료하면,   $ python  Python 2.7.10 (default, Jul 30 2016, 18:31:42) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.   /usr/local/bin/python3에 symbolic link가 있는 것을 확인하고,  $ python3  Python 3.6.4 (default, Jan  6 2018, 11:51:15) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.   매번, python3라고 실행할 수는 없지 않나? 그리고, 라이브러리 의존성도 생각해야지.   virtualenvwrapper가 설치된 상태에서, 다음과 같이 실행하자.   mkvirtualenv -p /usr/local/bin/python3 python3   그러면, $WORKON_HOME내, python3 프로파일이 설정되고,   workon python3   를 실행하는 것으로, python3 환경으로 전환할 수 있다.   (python3) $ python  Python 3.6.4 (default, Jan  6 2018, 11:51:15) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin  Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.   물론, venv에서 벗어나려면 deactivate   ","categories": ["development"],
        "tags": ["python","vim","virtualenv","virtualenvwrapper"],
        "url": "http://localhost:4000/development/virtualenvwrapper-python3-virtualenv/",
        "teaser":null},{
        "title": "여러 버전의 python을 관리하는 또 다른 방법 : pyenv",
        "excerpt":"python3가 출시된지 오래되었으나, 아직 python2는 많이 사용되고 있다. 결국, python2와 python3의 버전 변경은 아직도 자주 겪는 일이 될 수 밖에 없을 것이다.   앞서, virtualenvwrapper를 통해 python2와 python3 switch를 하는 방법을 포스팅했으나, 다른 방법으로 여러 버전의 python을 컨트롤 하는 방법에 대해 기록으로 남긴다.   pyenv라는 툴을 사용해 보자.   pyenv 설치   brew update brew install pyenv   설치 후,   echo $PATH   를 해보면, 기존과 변화가 보일 것이다.   /Users/사용자계정/.pyenv/shims:   가 자동으로 추가되어있을 것이다. pyenv가 동작하면서, shims를 일종의 proxy 하기 때문에 자동으로 추가되는데,   항상 동작하도록 하기 위해, 본인의 shell profile(.bash_profile, .zshrc, .zshenv) 등에 다음과 같이 추가한다.   if which pyenv &gt; /dev/null; then eval \"$(pyenv init -)\"; fi   필요한 버전 python 설치   다음과 같이, 설치 가능한 버전의 python을 조회할 수 있다.   pyenv install -list   다음과 같이 여러 버전의 python을 설치해 보자.   pyenv install 2.7.14 pyenv install 3.6.4 pyenv install 3.7-dev   python version 확인   이제 시스템에 설치된 python들의 버전을 확인하자. 현재 python의 버전을 확인한다.   pyenv version   현재 python 및 설치되어 있는 (변경 가능한) 버전을 모두 확인한다.   pyenv versions   아래와 같이 출력된다.      system (set by /Users/사용자계정/.pyenv/version) 2.7.14 3.6.4 3.7-dev   불필요한 버전 삭제   불필요한 버전은   pyenv uninstall 3.7-dev   와 같이 버전을 직접 명시하면 삭제 가능하다. (지우는 건 순식간에 됨)   python version 변경   현재, 내 시스템의 python 버전은 2.7.10이다.   python version  을 실행하면,   Python 2.7.10 이 출력된다.   다음과 같이 python version을 변경해 보자.   pyenv global 3.6.4    pyenv versions를 실행해보면, * 마크가 표시된 곳이 변경된다.   system   2.7.14     3.6.4 (set by /Users/사용자계정/.pyenv/version)   python --version   하면, 변경된 버전을 확인할 수 있다. 이와 같이, 버전을 변경하는 모드에는 세 가지가 있다.   pyenv global 버전 pyenv local 버전 pyenv shell 버전   global은 전체 버전을 변경하는 command이고, local은 해당 디렉토리내 .python-version 파일을 기록하여 해당 디렉토리 한정 특정 버전으로 변경 유지한다. shell은 해당 shell의 세션내 버전 변경을 유지해준다. 필요한 것으로 사용하자.   local로 설정한 옵션은 또 local 커맨드로 overwrite 하거나, –unset 파라미터를 주거나, 직접 .python-version 파일을 지워주면 원복할 수 있다.   아래는, global은 3.6.4로 test 디렉토리에서는 2.7.14로 설정한 경우의 버전 출력 예이다.   ~ $ pyenv version 3.6.4 (set by /Users/사용자계정/.pyenv/version)   ~/test $ pyenv version 2.7.14 (set by /Users/사용자계정/test/.python-version)   ~/test $ cat .python-version 2.7.14   ","categories": ["python"],
        "tags": ["pyenv"],
        "url": "http://localhost:4000/python/handling-various-verions-of-pythons-with-pyenv/",
        "teaser":null},{
        "title": "Turning my VIM to an IDE",
        "excerpt":"There are many blog posts about setting up vim as an IDE tool. I’m posting this for myself because I will be googling the posts again in the future whenever I change or reset my PC or account.   1. VIM   brew install vim --with-override-system-vi   My OSX is Sierra, 10.12, and it didn’t go well without the option ‘–with-override-system-vi’. After installation, I was able to execute vim7 by hitting vi and vim8 by hitting vim.   2. Vundle  Vundle is a VIM plugin management tool. It saves time by eliminating processes. Without this, I should download packages, unzip them to a specific directory by myself. What I need to do is just giving an alias for the proper plugins now.   git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim   Then, I copied this snippet to ~/.vimrc  set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" The following are examples of different formats supported. \" Keep Plugin commands between vundle#begin/end. \" plugin on GitHub repo Plugin 'tpope/vim-fugitive' \" plugin from http://vim-scripts.org/vim/scripts.html \" Plugin 'L9' \" Git plugin not hosted on GitHub Plugin 'git://git.wincent.com/command-t.git' \" git repos on your local machine (i.e. when working on your own plugin) Plugin 'file:///home/gmarik/path/to/plugin' \" The sparkup vim script is in a subdirectory of this repo called vim. \" Pass the path to set the runtimepath properly. Plugin 'rstacruz/sparkup', {'rtp': 'vim/'} \" Install L9 and avoid a Naming conflict if you've already installed a \" different version somewhere else. \" Plugin 'ascenator/L9', {'name': 'newL9'} \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line   Now, I can install plugins with this command from vim extension mode.   :PluginInstall   This command does the details for me. I can update plugins easily.   :PluginUpdate   After removing plugin quotations from the vimrc file, just do this.   :PluginClean  For bundles, this command will work.  :BundleInstall   3. AirLine: Status Line   Let’s put a status line to my vim. I added this to my ~/.vimrc file.  Plugin 'vim-airline/vim-airline'   After installation, my vim has the status line now.    4. NERDTree: Explorer  I wanted to have project view window just like IntelliJ Idea or Eclipse. With NERDTree plugin, I can browse project directories better. Added this to ~/.vimrc, and installed the plugin.   Plugin 'scrooloose/nerdtree’   By default, :NERDTree summons NERDTreem, and it is not cool. So I added a shortcut for NERDTree to the vimrc file as follow.   map :NERDTreeToggle  Now I can use NERDTree with Ctrl + C.    5. ctrlp : File Search Helper  This plugin is really helpful when I know the filename, but I am not sure where it is. As its name implies, I can search nominees from the keyword by pressing Ctrl + p.    6. Jedi-Vim: Python Auto Completer  Even though I’ve quoted screenshots from some java files, I wanted to use vim for toy projects in python. This plugin might make my VIM more like a commercial one.      ","categories": [],
        "tags": ["python","vim"],
        "url": "http://localhost:4000/turning-my-vim-to-an-ide/",
        "teaser":null},{
        "title": "pyenv + virtualenv + autoenv",
        "excerpt":"앞서     virtualenv 기반 환경 설정 방법   virtualenvwrapper를 사용하여 환경 전환 편의성을 높이는 방법 에 대해 정리해 뒀다.   python2.x와 python3.x가 로컬에 이미 설치되어 있을 때는 위 방법만으로도 충분할 수도 있다.   그런데, 더 다양한 버전의 python을 설치하고 테스트해 봐야한다면 이 방법만으로도 괜찮을까?   pyenv를 써서, 다양한 버전의 python을 쉽게 설정할 수 있다는 것도 알아봤는데, 이와 virtualenv를 조합하면 어떨까?   본 포스트에서는, 다양한 버전의 python을 pyenv와 virutalenv를 조합 사용하는 방안에 대해 기록하고자 한다.   pyenv-virtualenv 설치   brew install pyenv-virtualenv   위와 같이 입력하고 실행하면 잠시 후 설치완료 후, 다음과 같은 메시지가 뜬다.   To enable auto-activation add to your profile:   if which pyenv-virtualenv-init &gt; /dev/null; then eval \"$(pyenv virtualenv-init -)\"; fi   pyenv를 활용한 virtualenv 생성   앞서 설정한 python 버전들은 다음과 같다.     system   2.7.14 * 3.6.4 (set by /Users/luran/.pyenv/version)   이제 3.6.4 버전의 virtualenv를 만들어 보자.   pyenv virtualenv 3.6.4 venv-3.6.4   이후, pyenv versions를 실행해 보면, 이제 새로 만든 virtual environment가 목록에 추가되었음을 확인할 수 있다.   * system (set by /Users/luran/.pyenv/version)   2.7.14   3.6.4   3.6.4/envs/venv-3.6.4   venv-3.6.4   이제 다음과 같이, 해당 virtual environment에 진입하고, 나올 수 있다.   # 명시적으로 activate pyenv activate venv-3.6.4 pyenv deactivate  # shell 기준 적용 pyenv shell venv-3.6.4  # local 기준 적용 : 해당 디렉토리에 .python-version 파일이 생기며, 이 디렉토리에 진입하면 항상 이 virtual env로 전환됨  pyenv local venv-3.6.4  # 원복 pyenv local system or rm .python-version   # global 기준 적용 : 무조건 전체 적용 pyenv global venv-3.6.4 pyenv global system   autoenv를 통한 가상 환경 적용 자동화   autoenv라는 툴을 먼저 설치하자. 이 녀석은 이름이 뜻하는 바와 같이, 자동으로 환경을 잡아주는 역할을 도와준다.   brew install autoenv   .bash_profile(bash) 혹은 .zshrc(zsh)에 다음과 같이 내용이 반영되도록 하자.   echo 'source /usr/local/opt/autoenv/activate.sh' &gt;&gt; .zhsrc   이제 원하는 디렉토리에 .env를 작성하고, 실행을 원하는 내용을 적으면 된다.   pyenv shell venv-3.6.4   예를 들어, 설정한 디렉토리가 test 디렉토리라 하면,   cd test   라고 치는 것만으로, pyenv shell 명령어가 실행될 것이다.   autoenv vs. pyenv local   pyenv shell을 하면, 그 shell의 세션이 유지되는 동안 (venv-3.6.4)가 유지될 것이다.   다른 디렉토리로 간다하더라도.   pyenv activate를 하면, 그 디렉토리에서 나갈 때도 어딘가에서는 autoenv를 통해 deactivate를 해줘야 자동으로 원상복구되지 않을까?   아니면, 수동으로 매번 pyenv deactivate 하던가.   autoenv를 쓰는 목적이, 그 디렉토리에 진입할 때마다 오로지 특정 virtualenv를 자동으로 설정하고 싶은 것이라면,   굳이 autoenv를 쓰지 않아도 되지 않나 생각한다. pyenv local을 쓰면 그 디렉토리에서 나가면 자동으로 원래대로 돌아간다.   ","categories": ["development"],
        "tags": ["python"],
        "url": "http://localhost:4000/development/pyenv-virtualenv-autoenv/",
        "teaser":null},{
        "title": "python에서의 unit test #2 - 디렉토리 구조 구성하기 + unittest 적용하기",
        "excerpt":"이제 프로젝트의 메인 모듈과 테스트 파일을 분리해서, 기존과 같이 동작하는지 확인해 보자. 구조는   project ㄴ lib   ㄴ func.py ㄴ tests   ㄴ test_func.py   의 구성을 따른다고 가정한다. 앞서 작성한 test_func.py의 내용의 구현부와 테스트부를 별도 파일로 나눠놓자.   # lib/func.py  def add_1(x):     return x + 1  # tests/test_func.py def test_add_1_1plus1():     assert add_1(1) == 2  def test_add_1_1plus2():     assert add_1(2) == 1   이렇게 파일을 분리하고, nosetests를 돌리면, add_1 function을 찾을 수 없다는 에러가 나온다.  test_func.test_add_1_1plus1 ... ERROR test_func.test_add_1_1plus2 ... ERROR  ====================================================================== ERROR: test_func.test_add_1_1plus1 ---------------------------------------------------------------------- Traceback (most recent call last):   File \"/Library/Python/2.7/site-packages/nose/case.py\", line 197, in runTest     self.test(*self.arg)   File \"/Users/luran/devwork/unit_test/tests/test_func.py\", line 2, in test_add_1_1plus1     assert add_1(1) == 2 NameError: global name 'add_1' is not defined  ====================================================================== ERROR: test_func.test_add_1_1plus2 ---------------------------------------------------------------------- Traceback (most recent call last):   File \"/Library/Python/2.7/site-packages/nose/case.py\", line 197, in runTest     self.test(*self.arg)   File \"/Users/luran/devwork/unit_test/tests/test_func.py\", line 5, in test_add_1_1plus2     assert add_1(2) == 1 NameError: global name 'add_1' is not defined  ---------------------------------------------------------------------- Ran 2 tests in 0.001s  파일을 분리해 놓고, import를 안 해줬으니 이제 테스트 코드에 import를 해준다.   # tests/test_func.py  from lib import func  def test_add_1_1plus1():     assert func.add_1(1) == 2  def test_add_1_1plus2():     assert func.add_1(2) == 1   다시 실행하면, 다른 에러가 나온다. 이번에는 lib를 찾을 수 없다는 에러이다.  Failure: ImportError (No module named lib) ... ERROR  ====================================================================== ERROR: Failure: ImportError (No module named lib) ---------------------------------------------------------------------- Traceback (most recent call last):   File \"/Library/Python/2.7/site-packages/nose/loader.py\", line 418, in loadTestsFromName     addr.filename, addr.module)   File \"/Library/Python/2.7/site-packages/nose/importer.py\", line 47, in importFromPath     return self.importFromDir(dir_path, fqname)   File \"/Library/Python/2.7/site-packages/nose/importer.py\", line 94, in importFromDir     mod = load_module(part_fqname, fh, filename, desc)   File \"/Users/luran/devwork/unit_test/tests/test_func.py\", line 1, in &lt;module&gt;     from lib import func ImportError: No module named lib  ---------------------------------------------------------------------- Ran 1 test in 0.001s  FAILED (errors=1)   project내부에서 디렉토리 구조를 서로 인식하게 해주기 위해, tests 디렉토리와, lib 디렉토리 내에서 init.py를 생성해 준다.   touch __init__.py   그러면, 디렉토리 구조는 다음과 같이 변경된다. project ㄴ lib   ㄴ init.py   ㄴ func.py ㄴ tests   ㄴ init.py   ㄴ test_func.py   이제 앞서 실행했던 테스트를 실행하면, 처음에 실행한 것과 같은 결과를 볼 수 있다.   $ nosetests -v test_func.py --with-coverage  tests.test_func.test_add_1_1plus1 ... ok tests.test_func.test_add_1_1plus2 ... FAIL  ====================================================================== FAIL: tests.test_func.test_add_1_1plus2 ---------------------------------------------------------------------- Traceback (most recent call last):   File \"/Library/Python/2.7/site-packages/nose/case.py\", line 197, in runTest     self.test(*self.arg)   File \"/Users/luran/devwork/unit_test/tests/test_func.py\", line 7, in test_add_1_1plus2     assert func.add_1(2) == 1 AssertionError  Name              Stmts   Miss  Cover ------------------------------------- lib/__init__.py       0      0   100% lib/func.py           2      0   100% ------------------------------------- TOTAL                 2      0   100% ----------------------------------------------------------------------  Ran 2 tests in 0.002s  FAILED (failures=1)   이제, unittest를 적용해 보자. 기존 테스트 케이스를 아래와 같이 변경한다.  import unittest from lib import func  class MyTest(unittest.TestCase):     def test_add_1_case1(self):         self.assertEqual(func.add_1(1), 2)      def test_add_1_case2(self):         self.assertEqual(func.add_1(1), 1)   기존보다 읽기는 더 어려워졌다. 그러나, assertion 실패시 좀더 구체적인 에러 메시지를 보여준다.       self.assertEqual(func.add_1(1), 1) AssertionError: 2 != 1   assert문의 종류에는 assertEqual외에도 assertNotEqual, assertTrue, assertFalse 등을 활용할 수 있다. 그런데, 부가적으로 작성해줘야 하는 코드가 너무 많은 것 또한 사실이다.   ","categories": ["development"],
        "tags": ["python","test","unittest"],
        "url": "http://localhost:4000/development/python-unit-test-2/",
        "teaser":null},{
        "title": "python에서의 unit test #3 - pytest (unittest + nose의 대안)",
        "excerpt":"unittest를 많이 쓰고 있는 것 같으나, 너무 많은 bolierplate를 작성해 줘야 한다는 느낌이 들어 대안을 찾아본다. py.test를 실험해 보자.   설치는 다음과 같다.  pip install pytest   앞서 unittest용으로 작성한 테스트를 변경한다. 원래 버전에 가깝게 되었다.  # test_by_pytest.py  from lib import func  def test_add_1_case1():     assert func.add_1(1) == 2  def test_add_1_case2():     assert func.add_1(1) == 1   이제 pytest로 해당 결과를 보면, 다음과 같이 더 가독성이 좋은 리포트가 나온다.   pytest -v test_by_pytest.py  ================================================================================================= test session starts ================================================================================================== platform darwin -- Python 3.6.4, pytest-3.4.0, py-1.5.2, pluggy-0.6.0 -- /Users/luran/.pyenv/versions/3.6.4/envs/venv-3.6.4/bin/python cachedir: .pytest_cache rootdir: /Users/luran/devwork/unit_test/tests, inifile: collected 2 items  test_by_pytest.py::test_add_1_case1 PASSED                                                                                                                                                                       [ 50%] test_by_pytest.py::test_add_1_case2 FAILED                                                                                                                                                                       [100%]  ======================================================================================================= FAILURES ======================================================================================================= ___________________________________________________________________________________________________ test_add_1_case2 ___________________________________________________________________________________________________      def test_add_1_case2(): &gt;       assert func.add_1(1) == 1 E       assert 2 == 1 E        +  where 2 = &lt;function add_1 at 0x10bb53840&gt;(1) E        +    where &lt;function add_1 at 0x10bb53840&gt; = func.add_1  test_by_pytest.py:7: AssertionError ========================================================================================== 1 failed, 1 passed in 0.04 seconds ==========================================================================================   nose에서 coverage를 측정했듯이, pytest로 coverage를 측정하기 위해, 툴을 하나 더 설치하자.   pip install pytest-cov  다음과 같이 실행하면, coverage를 포함한 결과가 나타난다.   pytest --cov=lib tests/test_by_pytest.py  ...  ---------- coverage: platform darwin, python 3.6.4-final-0 ----------- Name              Stmts   Miss  Cover ------------------------------------- lib/__init__.py       0      0   100% lib/func.py           2      0   100% ------------------------------------- TOTAL                 2      0   100%  ....  coverage report는 아래와 같이 옵션을 지정하여 추출할 수 있다.   --cov_report html --cov_report xml  동시에 지정할 수도 있는데, html의 경우는 디렉토리로, xml의 경우는 단독 파일로 결과가 export된다. pytest로 테스트 코드를 작성하는데 unittest에 비해 더 적은 노력이 들고, 똑같이 coverage도 측정된다. (물론, fixture나 mock 등도 더 살펴 봐야할 것이지만)   ","categories": ["development"],
        "tags": ["python","unittest","pytest"],
        "url": "http://localhost:4000/development/python-unit-test-3-pytest-unittest-nose/",
        "teaser":null},{
        "title": "Python project를 위한 Docker 기반 Jenkins 설정하기 #1",
        "excerpt":"앞서 Docker의 Kitematic으로 Jenkins 이미지를 받아서 띄워봤는데, Python 프로젝트를 위해 몇몇 설정을 변경하기 위해 직접 Docker 이미지를 만들어보자.   1. Dockerfile 작성  Dockerfile을 만들고 아래와 같이 내용을 넣자.   # Dockerfile FROM jenkins:latest USER root  RUN apt-get update  # pip 설치 RUN apt-get install -y python-pip ENV JAVA_ARGS -Xms512m -Xmx1024m  RUN apt-get update &amp;&amp; \\     apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget  # pyenv 설치/ 설정 RUN git clone https://github.com/pyenv/pyenv.git .pyenv ENV PYENV_ROOT $HOME/.pyenv ENV PATH $PYENV_ROOT/shims:$PYENV_ROOT/bin:$PATH  # python v2, v3 설치 RUN pyenv install 2.7.14 RUN pyenv install 3.6.4 RUN pyenv rehash   2. Docker 이미지 생성  이제 앞서 작성한 Dockerfile을 사용하여 Docker 이미지를 만들자.   docker build -t \"jenkins:python\" yourpath   3. Docker 실행  다음과 같이 실행하자.   docker run -p 8080:8080 -v host_directory:container_directory jenkins:python   -p로 포트를 매핑하고, -v로 마운트할 디렉토리를 지정하자. docker를 실행할 때마다, jenkins를 매번 초기화/ 설정하고 싶지 않다면, volume 마운트를 하자. 설정한 내용을 host에 기록하고, container를 띄울 때 마운트한다. jenkins의 기본 디렉토리가 var/jenkins_home이므로, 아래와 같이 실행한다.   docker run -p 8080:8080 -v mydirectory:/var/jenkins_home jenkins:python   이후, jenkins를 unlock하고, 필요한 기본 plugin들을 업데이트하면 된다. 나중에 다시 jenkins를 다시 시작시켜도 기존에 설정한 내용들이 유지된다.   ","categories": ["python","tools"],
        "tags": [],
        "url": "http://localhost:4000/python/tools/python-project-docker-jenkins/",
        "teaser":null},{
        "title": "Wordpress post를 Jekyll로 이관",
        "excerpt":"개요   기존에 wordpress에 올라가 있던 글들을 Jekyll로 옮기던 과정에서의 lessons를 정리해 둔다. 또 할지도 모르니까.   많은 사람들이, 한결같이 “쉽게 마이그레이션 했다”고 했다.   그러나, 나는 오래 걸렸다. (심지어, 아직도 진행 중이다.) 그래서 적어둬야 한다.   Exporting from Wordpress   기존의 댓글은 과감히 포기하기로 한다.   3rd party 플러그인과, 워드프레스 빌트인 플러그인으로 각각 export/ import를 비교해봤으나, 워드프레스 자체 플러그인이 더 깔끔해 보여서, 후자를 택하기로 한다.   워드프레스의 도구 &gt; 글을 선택하여, 글만 전체 export 한다. 한 개의 xml 파일이 생성될 것이다.   Importing to Jekyll   블로그의 작업 디렉토리로 이동하여, 앞서 export 및 다운로드한 xml 파일을 사용하여 포스트를 import 한다.   ruby -rubygems -e 'require \"jekyll-import\"; JekyllImport::Importers::WordpressDotCom.run({ \"source\" =&gt; \"블로그이름.wordpress.2018-06-09.xml\" })’    이제, 로컬에서 jekyll을 띄워서 글들이 제대로 이사왔는지 보자.   jekyll serve --port 8080   레이아웃이 깨진다.   이슈 #1 : 레이아웃 미적용   콘솔 창에서 다음과 같은 에러 메시지가 뜨는 것을 확인하였다.   Build Warning: Layout 'post' requested in _posts/2014-07-03-%eb%a7%a5%ec%97%90%ec%84%9c-%eb%a7%88%ec%9a%b0%ec%8a%a4-%ec%8a%a4%ed%81%ac%eb%a1%a4-%eb%b0%a9%ed%96%a5-%eb%b0%98%eb%8c%80%eb%a1%9c-%ed%95%98%eb%a0%a4%eb%a9%b4.html does not exist.   기본 옵션으로 이전해 온 블로그 포스트들이, 기본 레이아웃으로 ‘post’로 export되어 있어, 위와 같은 warning 메시지가 발생하였다. minimal mistakes 테마를 선택하였기에, 기본 레이아웃을 single로 변경하였다.   sed -i \"\" 's/layout: post/layout: single/g' *.html   이슈 #2 : Syntax Highlighter 미동작   이렇게 변경하고 보니, warning 메시지는 사라졌는데 기존 글들의 syntax highlighter가 제대로 동작하지 않았다.   highlight 구문으로 묶으면 syntax highlighter가 동작하나, ```로 묶으면 변환되지 않고 그대로 노출되는 현상을 발견하였다.   확장자가 html이 아니라, md여야 제대로 변환해 주는 것을 확인하였다. 기존에 import한 상태는 모두 확장자가 html로 되어 있던 상태였다. 이에, 다음과 같이 _post/*.html의 확장자를 md로 변경해 주었다.   rename 's/.html/.md/' *.html   이렇게 변경하고 나니, 레이아웃도 잡히고, syntax highlighter도 일단 동작하는 것을 확인하였다.  ","categories": ["life"],
        "tags": ["wordpress","jekyll"],
        "url": "http://localhost:4000/life/migrating-posts-to-jekyll/",
        "teaser":null},{
        "title": "Jekyll로 이전 - Jekyll 설치/ 설정",
        "excerpt":"이전 배경   Tistory를 거쳐, 호스팅형 Wordpess 및 설치형 Wordpress를 거쳐 Jekyll로 옮겨본다.   다른 사람들과 마찬가지로 트렌드도 그렇지만,      워드프레스가 해킹 대상이 많이 되는 점   GitHub로 옮기면 호스팅 무료   워드프레스보다 깔끔함   호기심   글쓰기의 편의성   등의 이류로 옮겨 보게 되었다.   설치 절차   다음에 혹시 이 작업을 반복할까 싶어, 겪은 내용을 기록으로 남긴다.   GitHub      GitHub을 방문하여 계정을 생성한다.   홈페이지를 만들기 위해 GitHub IO 설정을 한다.            앞서 만든 계정.github.io 프로젝트를 생성한다.       가이드에 따라, index.html을 생성하여 push하고, Hello World (Hello World 출력 예제)가 뜨는지 확인한다.           Jekyll   개발 환경 설정   올리기 전에, 제대로 포스팅 되는지 로컬에서 동작 확인을 해야하므로, 로컬 개발 환경 설정도 한다.   # 루비 설정 여부 확인 ruby --version   gem install jekyll gem install bundler gem install github-pages  # syntax highlighter gem install rouge   테마 설정   이름만큼 고르기 힘든, 테마 고르기. 사람들이 많이 쓰는 것 같은 minimal-mistakes로 일단 정착하기로 한다.   gem install minimal-mistakes-jekyll   혹은, github에서 clone 또는 다운로드 한다.   기본 디렉토리/ 파일 설정   작업 디렉토리에, _posts, _drafts 디렉토리가 없으면 생성한다.   버전 관리 대상이 되지 않는 파일들은 .gitignore에 등록한다.   기본 동작 확인   jekyll serve   를 실행시키면, 기본 설정 포트 4000으로 뜨는 것을 확인할 수 있다. 내 경우는, 4000 포트가 어떤 프로그램이 이미 쓰고 있는 모양이다.   jekyll serve --port 8080   과 같이 다른 포트를 지정하면, 다른 포트로 띄울 수 있다.   $jekyll --help  jekyll 3.8.3 -- Jekyll is a blog-aware, static site generator in Ruby  Usage:    jekyll &lt;subcommand&gt; [options]  Options:         -s, --source [DIR]  Source directory (defaults to ./)         -d, --destination [DIR]  Destination directory (defaults to ./_site)             --safe         Safe mode (defaults to false)         -p, --plugins PLUGINS_DIR1[,PLUGINS_DIR2[,...]]  Plugins directory (defaults to ./_plugins)             --layouts DIR  Layouts directory (defaults to ./_layouts)             --profile      Generate a Liquid rendering profile         -h, --help         Show this message         -v, --version      Print the name and version         -t, --trace        Show the full backtrace when an error occurs  Subcommands:   docs   import   build, b              Build your site   clean                 Clean the site (removes site output and metadata file) without building.   doctor, hyde          Search site and print specific deprecation warnings   help                  Show the help message, optionally for a given subcommand.   new                   Creates a new Jekyll site scaffold in PATH   new-theme             Creates a new Jekyll theme scaffold   serve, server, s      Serve your site locally   serve에 대한 설명을, 좀더 확인해 보면,   $ jekyll serve --help  jekyll serve -- Serve your site locally  Usage:    jekyll serve [options]  Options:             --config CONFIG_FILE[,CONFIG_FILE2,...]  Custom configuration file         -d, --destination DESTINATION  The current folder will be generated into DESTINATION         -s, --source SOURCE  Custom source directory             --future       Publishes posts with a future date             --limit_posts MAX_POSTS  Limits the number of posts to parse and publish         -w, --[no-]watch   Watch for changes and rebuild         -b, --baseurl URL  Serve the website from the given base URL             --force_polling  Force watch to use polling             --lsi          Use LSI for improved related posts         -D, --drafts       Render posts in the _drafts folder             --unpublished  Render posts that were marked as unpublished         -q, --quiet        Silence output.         -V, --verbose      Print verbose output.         -I, --incremental  Enable incremental rebuild.             --strict_front_matter  Fail if errors are present in front matter             --ssl-cert [CERT]  X.509 (SSL) certificate.         -H, --host [HOST]  Host to bind to         -o, --open-url     Launch your site in a browser         -B, --detach       Run the server in the background             --ssl-key [KEY]  X.509 (SSL) Private Key.         -P, --port [PORT]  Port to listen on             --show-dir-listing  Show a directory listing instead of loading your index file.             --skip-initial-build  Skips the initial site build which occurs before the server is started.         -l, --livereload   Use LiveReload to automatically refresh browsers             --livereload-ignore ignore GLOB1[,GLOB2[,...]]  Files for LiveReload to ignore. Remember to quote the values so your shell won't expand them             --livereload-min-delay [SECONDS]  Minimum reload delay             --livereload-max-delay [SECONDS]  Maximum reload delay             --livereload-port [PORT]  Port for LiveReload to listen on         -h, --help         Show this message         -v, --version      Print the name and version         -t, --trace        Show the full backtrace when an error occurs         -s, --source [DIR]  Source directory (defaults to ./)         -d, --destination [DIR]  Destination directory (defaults to ./_site)             --safe         Safe mode (defaults to false)         -p, --plugins PLUGINS_DIR1[,PLUGINS_DIR2[,...]]  Plugins directory (defaults to ./_plugins)             --layouts DIR  Layouts directory (defaults to ./_layouts)             --profile      Generate a Liquid rendering profile         -h, --help         Show this message         -v, --version      Print the name and version         -t, --trace        Show the full backtrace when an error occurs   port뿐만 아니라 host까지도 지정할 수 있다. 결국, 내 로컬에서도 serve 옵션으로 띄워놓고 운영해도 된다는 뜻이다. (당연히)   그러면, 외부에서도 접속할 수 있도록 공인 IP를 써 주거나, DDNS하거나, 터널링 등을 해주면 될 것이다. 그러나, github에서 서비스 해주는데, 내 로컬 repository가 노출되는게 싫은 이상, 굳이 이렇게까지 하는 사람은 별로 없을 것 같다.   어쨌든, 나는   jekyll serve --port 8080   으로 정착.   ","categories": ["life"],
        "tags": ["jekyll"],
        "url": "http://localhost:4000/life/migrating-to-jekyll-setup/",
        "teaser":null},{
        "title": "VI에서 개행 문자(^M) 삭제하기",
        "excerpt":"윈도우즈에서 작성된 파일을 linux/ OSX로 옮겨오다 보니, 가끔 원치 않는 개행 문자가 그대로 살아있는 경우가 있다.   VI에서 해당 파일을 열어보면, ^M과 같이 표시되는데, 이를 치환/ 삭제 하려면 다음과 같이 입력한다.   :%s/^M//g   그런데, 단순히 shift + 6 M을 입력하면 치환대상이 되지 않고,   ^M을 입력할 때, ctrl + v, ctrl + M을 눌러서 입력하면 된다.  ","categories": [],
        "tags": ["vi"],
        "url": "http://localhost:4000/replace-carrage-return/",
        "teaser":null},{
        "title": "Jekyll로 블로그 이전시 _drafts에서 _posts로 옮기기",
        "excerpt":"배경   앞서, export -&gt; import를 하면서 신경쓰게 된 부분은,      이미지를 모두 assets에 통째로 넣고 관리할 것인가?            분명 같은 파일명을 주게 될 일도 생길테니, images/YYYYMM으로 디렉토리를 만들고 파일을 넣자           기존에 export한 포스트명과 이미지 이름들이 모두 인코딩 되어 있다.            나중에 수정하거나, 변경하려면 찾아내기 힘들 것 같다.       그대로 두면, 유지보수 하기 힘들 것 같다.           물론, Git에 올리면 읽을 수도 있겠지만 개발환경에서는 분간해 내기 어렵다.   결국, _drafts에 있는 포스트들을 _posts로 옮기는 작업을 하되, 이름을 변경해가면서 옮겨줘야 한다. (불편)   활용   vi에서 본문 편집 -&gt; mv _drafts/원본파일(한글).md _posts/영문파일.md   이런 식으로 작업하려니 번거롭기 그지 없다.   이 때, NERDTreeFind를 사용해 보자. (사전 설치한 플러그인)   NERDTreeFind 실행   파일 이동   extension mode로부터, m을 누르면 다음과 같이 하단에 interactive menu가 뜬다.   NERDTree Menu. Use j/k/enter and the shortcuts indicated ========================================================== &gt; (a)dd a childnode   (m)ove the current node   (d)elete the current node   (c)opy the current node   (l)ist the current node   여기에서 m을 한 번 더, 누르면 말 그대로 move를 하겠다는 것이고, 그에 따른 또다른 문구가 출력된다.   Rename the current node ========================================================== Enter the new path for the node: /Users/luran/devwork/blog/_drafts/2013-07-11-spring3-1-hibernate3-jasypt1-8-%ec%97%b0%eb%8f%99%ed%85%8c%ec%8a%a4%ed%8a%b8.md   보다시피, 내 경우는 예전에 워드프레스에서 작성했던 포스트들이 그냥 한글이름으로 export되면서, 향후 유지보수를 하려면 본문을 열어봐야 하는 불편함이 생겼다. 이에 _drafts 디렉토리에서 대략 편집한 후, _posts 디렉토리로 옮겨야 하는 수요가 발생한다. 이 때, NERDTreeFind에서 m - m을 눌러서, 파일 이름이나 디렉토리를 편집하고 엔터를 치면, 원하는 디렉토리로 글을 옮길 수 있다.  ","categories": ["life"],
        "tags": ["jekyll","wordpress"],
        "url": "http://localhost:4000/life/filename-mangement-when-migrating-to-jekyll/",
        "teaser":null},{
        "title": "Integrating Gradle Test Local Sonar",
        "excerpt":"Docker로 sonarqube 설치   docker pull sonarqube   sonarqube 이미지를 다운로드하고 실행시킨다.   docker run -d --name sonarqube -p 9000:9000 -p 9092:9092 sonarqube:latest   이제 로컬 sonar를 확인할 수 있다.   http://localhost:9000   Spring Boot Gradle Sample Project 생성   spring init --build gradle --package-name=mytest --name=mytest -g=mytest -a=mytest -x   Gradle Property 선언   buildscript { ... dependencies { ... \t\tclasspath \"org.sonarsource.scanner.gradle:sonarqube-gradle-plugin:2.6.2\" \t} }  apply plugin: \"org.sonarqube\" apply plugin: 'groovy' apply plugin: 'jacoco'  ... sonarqube { \tproperties { \t\tproperty \"sonar.host.url\", \"http://localhost:9090\" \t\tproperty \"sonar.sources\", \"src\" \t\tproperty \"sonar.language\", \"java\"  \t\tproperty \"sonar.sourceEncoding\", \"UTF-8\" \t\tproperty \"sonar.exclusions\", \"**/*Test*.*, **/Q*.java\" \t\tproperty \"sonar.test.inclusions\", \"**/*Spec.groovy, **/*Test.java\" \t\tproperty \"sonar.coverage.exclusions\", \"**/*Test*.*, **/Q*.java\" \t\tproperty \"sonar.java.junit.reportPaths\", \"${buildDir}/test-results\" \t\tproperty \"sonar.jacoco.reportPaths\", \"${buildDir}/jacoco/jacoco.exec\" \t} }  dependencies { \ttestCompile 'org.spockframework:spock-core:1.1-groovy-2.4' \ttestCompile 'org.spockframework:spock-spring:1.1-groovy-2.4' \ttestRuntime 'cglib:cglib-nodep:3.2.4' }    기존 build.gradle에 위 내용을 추가한다.   Gradle task로 sonarqube 확인   ./gradlew test sonarqube -Dsonar.host.url=http://localhost:9000   로컬에서 테스트한 결과를 sonarqube로 연동하여 곧바로 확인할 수 있다.   ","categories": ["development"],
        "tags": ["docker","sonarqube"],
        "url": "http://localhost:4000/development/Integrating-Gradle-Test-Local-Sonar/",
        "teaser":null},{
        "title": "Local Zeppelin 설치하기 (OSX)",
        "excerpt":"brew가 실행가능하도록 설정되어 있다는 전제하에  brew install apache-zeppelin  위와 같이 간편하게 zeppelin을 설치한다.   alias 설정  실행 편의를 위해, 다음과 같이 alias를 선언한다.   which zeppelin.sh  로 zeppelin 설치 위치를 확인하고,   alias zeppelin-start=\"/usr/local/bin/zeppelin.sh start\" alias zeppelin-stop=\"/usr/local/bin/zeppelin.sh stop\"  과 같이 본인이 사용하는 shell에 등록한다.   Zeppelin 실행  zeppelin을 실행하려면, java runtime이 설치되어 있어야 한다.   zeppelin-start     Zeppelin 종료  zeppelin-stop   환경설정  설치 디렉토리 하위로 탐색해 보면, conf 디렉토리를 발견할 수 있을 것이다.   예) /usr/local/Cellar/apache-zeppelin/0.8.0/libexec/conf   기본 설치를 완료하면, 위 디렉토리 내 설정파일들은 모두 .template의 확장자로 되어있다. (일종의 주석처리) 이 .template 부분만 이름에서 지워주면, 원래 파일에 기록되어 있는 설정 내용들이 적용된다. 바꿔말하면, 원하는 설정을 변경 후 파일이름으로부터 .template을 없애주고 서비스를 재시작하면 변경한 설정이 반영된다.   실행 포트 변경  conf/zeppelin-site.xml.template내 아래 설정을 찾아서,  원하는 포트로 변경 후, 저장하고 zeppelin-site.xml.template를 zeppelin-site.xml로 변경 저장한다.   &lt;property&gt;   &lt;name&gt;zeppelin.server.port&lt;/name&gt;   &lt;value&gt;8080&lt;/value&gt;   &lt;description&gt;Server port.&lt;/description&gt; &lt;/property&gt;   계정 권한 설정 (anonymous -&gt; 계정 지정)  기본 설정은 익명계정으로 모든 노트북이 공유되도록 되어 있다. 만약, 계정별로 관리하거나 보호하고자 한다면, 계정과 관련된 설정을 별도 진행해야 한다. 계정 및 보안에 대한 설정은 conf/shiro.ini.template에 기록이 되어 있다.   [users] # List of users with their password allowed to access Zeppelin. # To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections # To enable admin user, uncomment the following line and set an appropriate password. #admin = password1, admin user1 = password2, role1, role2 user2 = password3, role3 user3 = password4, role2  위의 내용을 참조하여 계정 및 계정의 role을 부여하고 shiro.ini.template을 shiro.ini로 변경하고 서비스를 재시작하면 적용된다.   ","categories": ["development"],
        "tags": ["zeppelin"],
        "url": "http://localhost:4000/development/installing-zeppelin/",
        "teaser":null},{
        "title": "Docker 기반 Local Spark Cluster 설치하기",
        "excerpt":"Baremetal 서버나 cloud, VM 등에 설치하기에 앞서, docker기반으로 spark cluster 환경을 빨리 쉽게 구축하고 사용할 수 있다. 어떻게 동작하는지 살펴보거나, 간단한 프로젝트를 하고자 한다면 나쁘지 않은 선택이 될 것이다.   Spark Cluster Project 받기  인터넷에서 다음의 프로젝트를 받아서 설치한다.  git clone https://github.com/big-data-europe/docker-hadoop-spark-workbench.git   Spark Cluster 시작하기  위 프로젝트를 clone한 디렉토리에서, 다음과 같이 실행하면 프로젝트내 기술된 docker-compose.yml을 바탕으로 spark cluster가 기동된다. (OSX 기준, docker-for-mac 등이 설치되어 있어야 함)   docker-compose up   모든 이미지들을 최초 다운로드 받는데 다소 시간이 걸리지만, 이후에는 곧바로 실행된다.   실행중인 프로세스 확인  위와 같이 클러스터를 시작하면, 원본 github 페이지에 안내된 바와 같이 아래 프로세스들이 동작하는 것을 확인할 수 있다.   docker ps  실행하면, 아래와 같이 동작 중인 container들이 나타난다.  CONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS                    PORTS                                                      NAMES bfbe4579b5e8        bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8   \"entrypoint.sh /bin/…\"   36 minutes ago      Up 36 minutes (healthy)   0.0.0.0:8081-&gt;8081/tcp                                     docker-hadoop-spark-workbench_spark-worker_1 47e1225e695c        bde2020/hadoop-datanode:1.1.0-hadoop2.8-java8     \"/entrypoint.sh /run…\"   36 minutes ago      Up 36 minutes (healthy)   0.0.0.0:50075-&gt;50075/tcp                                   docker-hadoop-spark-workbench_datanode_1 00effc501b21        bde2020/spark-master:2.1.0-hadoop2.8-hive-java8   \"entrypoint.sh /bin/…\"   36 minutes ago      Up 36 minutes (healthy)   0.0.0.0:7077-&gt;7077/tcp, 6066/tcp, 0.0.0.0:8080-&gt;8080/tcp   spark-master 2ad2864b8ef2        bde2020/hadoop-namenode:1.1.0-hadoop2.8-java8     \"/entrypoint.sh /run…\"   36 minutes ago      Up 36 minutes (healthy)   0.0.0.0:50070-&gt;50070/tcp                                   namenode 19f6e280b334        bde2020/spark-notebook:2.1.0-hadoop2.8-hive       \"/entrypoint.sh /run…\"   36 minutes ago      Up 36 minutes             0.0.0.0:9001-&gt;9001/tcp                                     spark-notebook 5284894c2597        bde2020/hdfs-filebrowser:3.11                     \"/entrypoint.sh buil…\"   36 minutes ago      Up 36 minutes             0.0.0.0:8088-&gt;8088/tcp                                     docker-hadoop-spark-workbench_hue_1   이는 프로젝트 github에 안내된 바와 같이   Namenode: http://localhost:50070 Datanode: http://localhost:50075 Spark-master: http://localhost:8080 Spark-notebook: http://localhost:9001 Hue (HDFS Filebrowser): http://localhost:8088/home   의 내용을 포함하고 있다. 즉, 브라우저에서 위의 주소를 입력하면 동작여부를 가시적으로 확인할 수 있다.   Spark Application 실행하기  다음과 같이 spark-submit 명령어로 애플리케이션을 실행할 수 있다.   #!/bin/bash  spark-submit \\     --deploy-mode client \\     --num-executors 10 \\     --driver-memory 40g \\     --driver-cores 8 \\     --executor-memory 80g \\     --executor-cores 10 \\     --conf spark.yarn.executor.memoryOverhead=4096 \\     --jars {참조할 jar 경로 포함한 이름} \\     --class {패키지명 포함한 실행 대상 클래스 이름} \\       {클래스 파일이 들어있는 jar 파일이름}   Spark Worker Node Scale Out  혹시, 이미 동작중인 클러스터내 worker의 수를 더 늘리고 싶다면? 다음의 명령어로 worker의 수를 늘릴 수 있다.   docker-compose scale spark-worker=3   단, 위와 같이 프로젝트 파일을 클론한 기본 상태에서는, scale out 시도시 port 매핑 충돌이 발생하여 다음과 같은 에러가 발생한다.  WARNING: The scale command is deprecated. Use the up command with the --scale flag instead. WARNING: The \"spark-worker\" service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash. Starting docker-hadoop-spark-workbench_spark-worker_1 ... done Creating docker-hadoop-spark-workbench_spark-worker_2 ... error Creating docker-hadoop-spark-workbench_spark-worker_3 ... error  ERROR: for docker-hadoop-spark-workbench_spark-worker_3  Cannot start service spark-worker: b'driver failed programming external connectivity on endpoint docker-hadoop-spark-workbench_spark-worker_3 (e6579099d4cff6a3d7986dae5ebdd0ab594584d2ed2e2bd3974174016ffe9d57): Bind for 0.0.0.0:8081 failed: port is already allocated'  ERROR: for docker-hadoop-spark-workbench_spark-worker_2  Cannot start service spark-worker: b'driver failed programming external connectivity on endpoint docker-hadoop-spark-workbench_spark-worker_2 (8bc751510f81453fb3ec2225d6ee0f4e1900569412644b64eafdec213a80e995): Bind for 0.0.0.0:8081 failed: port is already allocated' ERROR: Cannot start service spark-worker: b'driver failed programming external connectivity on endpoint docker-hadoop-spark-workbench_spark-worker_3 (e6579099d4cff6a3d7986dae5ebdd0ab594584d2ed2e2bd3974174016ffe9d57): Bind for 0.0.0.0:8081 failed: port is already allocated'  따라서, docker-compose.yml 파일 중, worker의 포트 매핑 정보를 주석처리 또는 삭제하고 다시 실행한다.     spark-worker:     image: bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8     depends_on:       - spark-master     environment:       - SPARK_MASTER=spark://spark-master:7077 #    ports: #      - 8081:8081     env_file:       - ./hadoop.env   위의 커맨드를 실행하면,  Starting docker-hadoop-spark-workbench_spark-worker_1 ... done Creating docker-hadoop-spark-workbench_spark-worker_2 ... done Creating docker-hadoop-spark-workbench_spark-worker_3 ... done  와 같이 메시지가 뜨고, docker container 리스트에도 반영된다.   CONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS                    PORTS                                                      NAMES dcc35f0449ce        bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8   \"entrypoint.sh /bin/…\"   47 seconds ago      Up 46 seconds (healthy)   8081/tcp                                                   docker-hadoop-spark-workbench_spark-worker_2 3915f1e48222        bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8   \"entrypoint.sh /bin/…\"   47 seconds ago      Up 46 seconds (healthy)   8081/tcp                                                   docker-hadoop-spark-workbench_spark-worker_3 bfbe4579b5e8        bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8   \"entrypoint.sh /bin/…\"   3 hours ago         Up 3 hours (healthy)      0.0.0.0:8081-&gt;8081/tcp                                     docker-hadoop-spark-workbench_spark-worker_1 47e1225e695c        bde2020/hadoop-datanode:1.1.0-hadoop2.8-java8     \"/entrypoint.sh /run…\"   3 hours   앞서 표기한, master 노드의 웹에 들어가서 확인해도 이 내용이 반영되어 있음을 확인 가능하다.    Spark Cluster 종료하기  실행 중인 클러스터는 다음과 같이 종료한다.   docker-compose down   ","categories": ["development"],
        "tags": ["spark","docker"],
        "url": "http://localhost:4000/development/installing-spark-cluster/",
        "teaser":null},{
        "title": "Local Airflow 설치하기",
        "excerpt":"예전에는 실행하고자 하는 job들을 crontab으로만 구성하였으나, 이제는 rundeck, oozie, airflow 등 많은 툴들이 사용되고 있다.  이와 같은 툴들은 각 job들의 실행 순서를 정의하고, 언제 실행될지, 실행시간이 얼마나 걸리는지 등에 대한 정보도 제공하기 때문에 유지보수나 관리, 성능개선 트래킹 관점에서도 crontab과는 비교할 수 없을만큼 유용하다.   Airflow Local 설치 (OSX)  AirFlow의 공식 사이트는 https://airflow.apache.org/ 이다. 다른 어떤 문서보다 공식사이트내 설치, 설정 활용 등에 대한 가이드를 먼저 참고하는 것이 좋다.   pip로 airflow를 설치할 것이므로, 다음과 같이 pip를 설치한다.   예전 방식)  sudo easy_install pip   설치시 문제가 발생하여, pip를 하위 버전으로 다운그레이드하고,   pip install --pip==9.0.3 pip install airflow  와 같이 설치를 진행하였다. (당시 별 문제 없었음)   최근 방식)  그런데, 2018/12/04 기준으로 위와 같이 진행하니 다른 에러가 발생하여, 아래와 같이 변경 설치하였다. 설치 환경에 따라 다를 수 있으므로, 에러가 나는 경우에만 관련 내용을 참고하면 될 듯 하다.   sudo pip install apache-airflow   만약, 다음과 같은 에러가 뜬다면   File \"/private/tmp/pip-build-pq6f_5/apache-airflow/setup.py\", line 49, in verify_gpl_dependency   raise RuntimeError(\"By default one of Airflow's dependencies installs a GPL \"   RuntimeError: By default one of Airflow's dependencies installs a GPL dependency (unidecode). To avoid this dependency set SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you install or upgrade Airflow. To force installing the GPL version set AIRFLOW_GPL_UNIDECODE  맨 마지막 줄에 뜨는 메시지를 반영하여, 다음과 같이 실행한다.   sudo SLUGIFY_USES_TEXT_UNIDECODE=yes pip install apache-airflow   또, 만약 dateutil과 관련하여 다음과 같은 에러가 뜬다면, (예전엔 못봤던 에러)   Found existing installation: python-dateutil 1.5     Uninstalling python-dateutil-1.5: Could not install packages due to an EnvironmentError:   최종적으로 다음과 같이 실행하여 airflow 설치를 완료하였다.   sudo SLUGIFY_USES_TEXT_UNIDECODE=yes pip install apache-airflow --ignore-installed python-dateutil   Airflow 설정 DB 초기화  설치 후, 1회만 해주면 되는 job으로 airflow가 내부 참조하는 sqlite db 초기화 작업이 있다.   airflow initdb   Airflow Web Server 실행  airflow webserver -p {포트}      웹 서버를 띄우면, 위와 같이 여러 샘플 workflow들이 포함되어 있다.   Example  제일 위의 example_bash_operator를 살펴보면, 세부 내역을 확인할 수 있다.      각 job의 실행 순서가 다소 헷갈릴 수 있는데, leaf 부터 root로 올라가는 실행 구조이다.  이는 graph view에서 보면 좀더 직관적으로 표현된다. (화살표가 보여서)      DAG(Directed Acyclic Graph)는 말 그대로, 방향성이 있으면서 사이클이 없는 그래프 즉, 일방통행 그래프라 할 수 있다. 임의의 노드를 클릭하면, context menu가 뜨는데 해당 노드별 세부 내용 조회 및 지시를 할 수 있다.      상단의 코드 탭을 클릭하면, 이 DAG의 소스를 볼 수 있다.   # -*- coding: utf-8 -*- # # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements.  See the NOTICE file # distributed with this work for additional information # regarding copyright ownership.  The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \"License\"); you may not use this file except in compliance # with the License.  You may obtain a copy of the License at # #   http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied.  See the License for the # specific language governing permissions and limitations # under the License.  from builtins import range from datetime import timedelta  import airflow from airflow.models import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.dummy_operator import DummyOperator  args = {     'owner': 'airflow',     'start_date': airflow.utils.dates.days_ago(2), }  dag = DAG(     dag_id='example_bash_operator',     default_args=args,     schedule_interval='0 0 * * *',     dagrun_timeout=timedelta(minutes=60), )  run_this_last = DummyOperator(     task_id='run_this_last',     dag=dag, )  # [START howto_operator_bash] run_this = BashOperator(     task_id='run_after_loop',     bash_command='echo 1',     dag=dag, ) # [END howto_operator_bash]  run_this &gt;&gt; run_this_last  for i in range(3):     task = BashOperator(         task_id='runme_' + str(i),         bash_command='echo \"\" &amp;&amp; sleep 1',         dag=dag,     )     task &gt;&gt; run_this  # [START howto_operator_bash_template] also_run_this = BashOperator(     task_id='also_run_this',     bash_command='echo \"run_id= | dag_run=\"',     dag=dag, ) # [END howto_operator_bash_template] also_run_this &gt;&gt; run_this_last  if __name__ == \"__main__\":     dag.cli()   Workflow 실행순서 지정  위의 tree view 또는 graph view에 표현된 바와 같이, job의 실행 순서를 지정하는 부분이 task » run_this와 같이 표기된 부분이다.   Airflow 공식 가이드에도 소개된 바와 같이 두 가지의 표현방식을 사용할 수 있다.   1. using set_upstream(), set_downstream() methods  op1.set_downstream(op2) op2.set_downstream(op3) op3.set_upstream(op4)  2. using bitwise operators  op1 &gt;&gt; op2 &gt;&gt; op3 &lt;&lt; op4   두 표현식 모두 실행순서는 동일하다. op1을 먼저 실행하고, op2, op3의 순서대로 실행된다.   Scheduler 실행  DAG의 실행주기를 설정한 후, scheduler daemon을 띄워서 주기적으로 실행되도록 한다.  airflow scheduler   ","categories": ["development"],
        "tags": ["airflow"],
        "url": "http://localhost:4000/development/installing-local-airflow/",
        "teaser":null},{
        "title": "jenv 설치하기",
        "excerpt":"여러 버전 java   사실 개발하면서 Java의 버전을 계속 바꿀 일은 별로 없는 것 같다.   프로젝트마다 바꿀 일도 없고, 회사 정책이 바뀌지 않는한 기존의 버전을 사용할 가능성이 높다.   또한, 기존에 java 8을 사용한 프로젝트라면 특별한 이유가 없는 한 Java8을 유지보수할 가능성이 높다.   Java의 버전 릴리즈 주기가 6개월로 짧아지면서, 여러 버전을 설치하고 사용하고자 할 때 jenv를 사용하면 좋을 것이다. (Python처럼) 3년 주기의 Java LTS 버전을 감안하면, production에서는 현재로서는 Java11이 유용한 선택지가 될 것이다.      https://en.wikipedia.org/wiki/Java_version_history   jenv: Java용 Pyenv   Python 개발 환경에 익숙한 사람들은 아마 이미 pyenv에 익숙할 것이다.   그리고, java 환경에도 동일한 tool이 있다. 이름도 사용법도 유사하다. pyenv를 써온 사람들이라면 그대로 쓰면 된다. (약간의 명령어 차이와 사용법 차이는 있겠지만)   Homebrew, Cask 설치   brew update &amp;&amp; brew upgrade brew-cask &amp;&amp; brew cleanup &amp;&amp; brew cask cleanup   Java 설치   brew cask info java brew cask install java   2020/5 기준, java14가 설치된다.   jenv 설치/ 환경 설정   brew install jenv  if which jenv &gt; /dev/null; then eval \"$(jenv init -)\"; fi  jenv add /Library/Java/JavaVirtualMachines/openjdk-14.0.1.jdk/Contents/Home   설치/ 설정 후 jenv versions를 실행해 보면,   $ jenv versions   system   1.8   1.8.0.192 * 14.0 (set by /Library/Java/JavaVirtualMachines/openjdk-14.0.1.jdk/Contents/Home/.java-version)   14.0.1   openjdk64-14.0.1   oracle64-1.8.0.192   만약, LTS 버전인 Java 11을 설치하고자 한다면, 다음과 같이 해보자.   AdaptOpenJdk를 사용하기 위해   brew tap AdoptOpenJDK/openjdk brew cask install adoptopenjdk11   설치 후, /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home 를 jenv에 추가해 준다.   $ jenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home   $ jenv versions   system * 1.8 (set by /Users/luran/.jenv/version)   1.8.0.192   11.0   11.0.7   14.0   14.0.1   openjdk64-11.0.7   openjdk64-14.0.1   oracle64-1.8.0.192   jenv 실행   jenv 0.5.2 Usage: jenv &lt;command&gt; [&lt;args&gt;]  Some useful jenv commands are:    commands    List all available jenv commands    local       Set or show the local application-specific Java version    global      Set or show the global Java version    shell       Set or show the shell-specific Java version    rehash      Rehash jenv shims (run this after installing executables)    version     Show the current Java version and its origin    versions    List all Java versions available to jenv    which       Display the full path to an executable    whence      List all Java versions that contain the given executable  See `jenv help &lt;command&gt;' for information on a specific command. For full documentation, see: https://github.com/hikage/jenv#readme   jenv만 실행하면, pyenv와 마찬가지로 가능 옵션을 알 수 있다.      versions: 사용 가능한 옵션들 보기   version: 현재 동작하는 옵션 보기   local: 특정 디렉토리 이하 버전 적용   global: 전체 버전 적용   shell: 현재 shell에만 적용   jenv global 1.8 jenv local 11.0 jenv shell 14.0   등과 같이 설정하여 사용하면 된다.   특히 local로 설정하게 되면, .java-version에 버전 정보가 저장된다.   만약, global 설정과 local 설정을 조합하여 버전을 제어하려고 하는데, 너무 상위 디렉토리에서 local로 선언한다면 원하는 제어를 하기 어려워 질 수 있다.   global/ local 제어가 뜻대로 제어가 되지 않는다면, local을 너무 상위 디렉토리에서 선언하지는 않았는지 확인해 볼 필요가 있다. .java-version 파일을 삭제하면, 기존에 선언한 버전 정보가 삭제된다.  ","categories": ["development"],
        "tags": ["jenv","java"],
        "url": "http://localhost:4000/development/install-jenv/",
        "teaser":null}]
